 How to Install Kubernetes Cluster on Ubuntu 22.04
https://www.linuxtechi.com/install-kubernetes-on-ubuntu-22-04/

root@ssd:~# sudo swapoff -a
root@ssd:~#  sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab

root@ssd:~# systemctl stop firewalld 

root@ssd:~# sudo tee /etc/modules-load.d/containerd.conf <<EOF
overlay
br_netfilter
EOF

root@ssd:~# sudo modprobe overlay
root@ssd:~# sudo modprobe br_netfilter

root@dev:~# sudo tee /etc/sysctl.d/kubernetes.conf <<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF

[]{{{{{{{][\]][[]][][][]][]]][}}{}{{}{{}}}]}}}}}}}   carefully   results 

root@ssd:~#  sudo tee /etc/modules-load.d/containerd.conf <<EOF
overlay
br_netfilter
EOF
overlay
br_netfilter
root@ssd:~# sudo modprobe overlay
root@ssd:~# sudo modprobe br_netfilter
root@ssd:~#  sudo tee /etc/sysctl.d/kubernetes.conf <<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
root@ssd:~# 
====-----
root@ssd:~# sudo sysctl --system

root@ssd:~#  sudo apt install -y curl gnupg2 software-properties-common apt-transport-https ca-certificates

root@ssd:~#  sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmour -o /etc/apt/trusted.gpg.d/docker.gpg

root@ssd:~# sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"

root@ssd:~#  sudo apt update

root@ssd:~# sudo apt install -y containerd.io

root@ssd:~# containerd config default | sudo tee /etc/containerd/config.toml >/dev/null 2>&1
root@ssd:~#  sudo sed -i 's/SystemdCgroup \= false/SystemdCgroup \= true/g' /etc/containerd/config.toml

root@ssd:~# sudo systemctl restart containerd
root@ssd:~#  sudo systemctl enable containerd 

root@ssd:~# curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -

root@ssd:~# sudo apt-add-repository "deb http://apt.kubernetes.io/ kubernetes-xenial main"

root@ssd:~# sudo apt update

root@ssd:~# sudo apt install -y kubelet kubeadm kubectl
root@ssd:~# sudo apt-mark hold kubelet kubeadm kubectl

  
        systemctl stop firewalld 
root@ssd:~#  sudo kubeadm init --pod-network-cidr=10.244.0.0/16   
    


Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.122.103:6443 --token etlbem.un3nsyo072l91yot \
	--discovery-token-ca-cert-hash sha256:0d213c6eef7dc1d7f83c9eb82e2ccd8b24182d0caf436e8d04a046450e823800 
dev@master:~$ 




kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml   

==========+++++++++++++++++++++++++==================================================
ssh client 

curl -1sLf 'https://dl.cloudsmith.io/public/asbru-cm/release/cfg/setup/bash.deb.sh' | sudo -E bash
sudo apt-get install asbru-cm

=================+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Install and Set Up kubectl on Linux - Kubernetes
https://kubernetes.io/docs/tasks/tools/
https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/

curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"

curl -LO https://dl.k8s.io/release/v1.26.0/bin/linux/amd64/kubectl

echo "$(cat kubectl.sha256)  kubectl" | sha256sum --check
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl

root@anji:~# mkdir -p .kube
root@anji:~# cd .kube/
dev@dev:~/.kube$ scp dev@192.168.122.103:/home/dev/.kube/config  .





/\/\/\/\/\/\/\/\/\\\\\/\\/\/\/\\\\/\/=====+++++++++++=====================[[[[[[[[[]]]]]]]]]\\\\[\/////////////////////////\]
https://github.com/justmeandopensource/kubernetes/blob/master/docs/install-cluster-ubuntu-20.md
https://github.com/rritsoft/kubernetes-install-in-ubuntu-20-Install-Kubernetes-Cluster-on-Ubuntu-20.04-

kubernetes install in ubuntu 20 =kubeadm install in ubuntu 20 =Kubernetes cluster on Ubuntu 20.04 , 
=========================-----------------                  ----------------===================================   


Login as root user

sudo su -
--------------------
Perform all the commands as root user unless otherwise specified
Disable Firewall

ufw disable
systemctl stop firewalld 
----------------
Disable swap

swapoff -a; sed -i '/swap/d' /etc/fstab
--------------------------------------
Update sysctl settings for Kubernetes networking

cat >>/etc/sysctl.d/kubernetes.conf<<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl --system
----------------------------------------
Install docker engine

{
  apt install -y apt-transport-https ca-certificates curl gnupg-agent software-properties-common
  curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -
  add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
  apt update
  apt install -y docker-ce=5:19.03.10~3-0~ubuntu-focal containerd.io
}
---------------------------------------
Kubernetes Setup
Add Apt repository

{
  curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
  echo "deb https://apt.kubernetes.io/ kubernetes-xenial main" > /etc/apt/sources.list.d/kubernetes.list
}
-----------------------------------------------
Install Kubernetes components

apt update && apt install -y kubeadm kubelet kubectl
-----------------------------------------------------------
In case you are using LXC containers for Kubernetes nodes

Hack required to provision K8s v1.15+ in LXC containers

{
  mknod /dev/kmsg c 1 11
  echo '#!/bin/sh -e' >> /etc/rc.local
  echo 'mknod /dev/kmsg c 1 11' >> /etc/rc.local
  chmod +x /etc/rc.local
}
----------------------------------------------------------
On kmaster
Initialize Kubernetes Cluster

Update the below command with the ip address of kmaster

kubeadm init --apiserver-advertise-address=172.16.16.100 --pod-network-cidr=192.168.0.0/16  --ignore-preflight-errors=all
OR OR 
sudo kubeadm init --pod-network-cidr=10.244.0.0/16


  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

  export KUBECONFIG=/etc/kubernetes/admin.conf


kubeadm join 192.168.122.103:6443 --token etlbem.un3nsyo072l91yot \
	--discovery-token-ca-cert-hash sha256:0d213c6eef7dc1d7f83c9eb82e2ccd8b24182d0caf436e8d04a046450e823800 

-----------------------------------------------------
Deploy WEAVE network
kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml   
-------------------------------------------
kubectl get nodes
hai@i7laptop:~$ kubectl get nodes 
NAME     STATUS   ROLES           AGE   VERSION
master   Ready    control-plane   15d   v1.25.4
==============================================   bellow steps in worker nodes===================
sudo systemctl stop firewalld 
sudo rm  -rf /etc/containerd/config.toml
sudo  systemctl restart containerd
hai@i7laptop:~$ sudo kubeadm join 192.168.122.103:6443 --token etlbem.un3nsyo072l91yot \
	--discovery-token-ca-cert-hash sha256:0d213c6eef7dc1d7f83c9eb82e2ccd8b24182d0caf436e8d04a046450e823800 
================================================================= completed ========
hai@i7laptop:~$ kubectl get nodes 
NAME     STATUS   ROLES           AGE   VERSION
master   Ready    control-plane   15d   v1.25.4
worker   Ready    <none>          15d   v1.25.4
===============================================================  if any  errors  ====
//////////////////////////////////////////////////////////////////////////////////////////
hai@master:~$ sudo kubeadm init --pod-network-cidr=10.244.0.0/16
I1218 06:45:37.145708  633010 version.go:256] remote version is much newer: v1.26.0; falling back to: stable-1.25
[init] Using Kubernetes version: v1.25.5
[preflight] Running pre-flight checks
error execution phase preflight: [preflight] Some fatal errors occurred:
	[ERROR Port-6443]: Port 6443 is in use
	[ERROR Port-10259]: Port 10259 is in use
	[ERROR Port-10257]: Port 10257 is in use
	[ERROR FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml]: /etc/kubernetes/manifests/kube-apiserver.yaml already exists
	[ERROR FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml]: /etc/kubernetes/manifests/kube-controller-manager.yaml already exists
	[ERROR FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml]: /etc/kubernetes/manifests/kube-scheduler.yaml already exists
	[ERROR FileAvailable--etc-kubernetes-manifests-etcd.yaml]: /etc/kubernetes/manifests/etcd.yaml already exists
	[ERROR Port-2379]: Port 2379 is in use
	[ERROR Port-2380]: Port 2380 is in use
	[ERROR DirAvailable--var-lib-etcd]: /var/lib/etcd is not empty
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher

SOLUTIONS:= 
systemctl stop firewalld 

hai@master:~$ sudo kubeadm reset 

hai@master:~$ sudo rm -rf /etc/kubernetes/manifests/kube-apiserver.yaml 

hai@master:~$ sudo rm -rf /etc/kubernetes/manifests/kube-controller-manager.yaml 

hai@master:~$ sudo rm  -rf  /etc/kubernetes/manifests/kube-scheduler.yaml

hai@master:~$ sudo rm -rf /etc/kubernetes/manifests/etcd.yaml
 hai@master:~$ sudo rm  -rf /etc/containerd/config.toml
hai@master:~$ sudo  systemctl restart containerd

================= THANK YOU ==========================
======================$$$$$$$$$$$$$$$$5%%%%%%%%%%%%%%%%%%%%%%5&&&&&&&&&&&888888888888*****]]]]]][[[[]][]]][][]][][][[[]]]

https://github.com/kubernetesway/kubernetes/wiki/Kubernetes-Bare-metal-cluster-on-Ubuntu-20.04.2
https://www.youtube.com/watch?v=zrb0daoSEEo&t=951s

https://computingforgeeks.com/deploy-kubernetes-cluster-on-ubuntu-with-kubeadm/   2023 document 
https://www.knowledgehut.com/blog/devops/install-kubernetes-on-ubuntu   2023 document 



sudo apt-get install docker.io

sudo systemctl enable docker

sudo systemctl status docker

sudo apt-get install curl

curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add

sudo apt-add-repository "deb http://apt.kubernetes.io/ kubernetes-xenial main"

sudo apt-get install kubeadm kubelet kubectl

sudo apt-mark hold kubeadm kubelet kubectl

sudo kubeadm version

sudo swapoff -a
sudo systemctl stop firewalld

###########Below step apply only on Master node

sudo hostnamectl set-hostname master

######Below step apply onlyon Worker node

sudo hostnamectl set-hostname worker

#** Below step apply only on Master node**

sudo kubeadm init --pod-network-cidr=10.244.0.0/16   







kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml         // "wavenetworks "



kubectl get nodes

sudo wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

kubectl apply -f kube-flannel.yml

or  //  or  // OR   OR  
git clone  https://github.com/rritsoft/kubernetes-10.git

git clone https://github.com/rritsoft/jenkins-cicd-docker-push-image-project.git



kubectl get nodes

kubeadm token create --print-join-command

hai@ubuntu:~$ kubeadm token create --print-join-command
kubeadm join 192.168.68.137:6443 --token 1bkysu.dydhuqmbuxa63546 --discovery-token-ca-cert-hash sha256:1ea0e848089c60445897d2d0ad1d9493b95a7f0dd9f96c57848052bbd73dac8f 

kubeadm join 192.168.122.29:6443 --token e0heub.1ezekb8t0imcm2fc --discovery-token-ca-cert-hash sha256:c38ee3fc3b5267990b27a222508dff7bdc41f1fb13a9322a5c5dcc0bffc8746f
sudo rm /etc/containerd/config.toml
sudo systemctl restart containerd
kkubeadm init

root@ubuntu:~# kubeadm init
[init] Using Kubernetes version: v1.25.4
[preflight] Running pre-flight checks
error execution phase preflight: [preflight] Some fatal errors occurred:
  [ERROR CRI]: container runtime is not running: output: E1125 08:08:38.456633    2764 remote_runtime.go:948] "Status from runtime service failed" err="rpc error: code = Unimplemented desc = unknown service runtime.v1alpha2.RuntimeService"
time="2022-11-25T08:08:38-08:00" level=fatal msg="getting status of runtime: rpc error: code = Unimplemented desc = unknown service runtime.v1alpha2.RuntimeService"
, error: exit status 1
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher
root@ubuntu:~# rm /etc/containerd/config.toml
root@ubuntu:~# systemctl restart containerd

--- == = = = = = = = 
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:
 apply the below steps up to step 10 on both master and worker nodes**
https://github.com/kubernetesway/kubernetes/wiki/Kubernetes-Bare-metal-cluster-on-Ubuntu-20.04.2
https://www.youtube.com/watch?v=zrb0daoSEEo&t=951s


sudo apt-get install docker.io

sudo systemctl enable docker

sudo systemctl status docker

sudo apt-get install curl

curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add

sudo apt-add-repository "deb http://apt.kubernetes.io/ kubernetes-xenial main"

sudo apt-get install kubeadm kubelet kubectl

sudo apt-mark hold kubeadm kubelet kubectl

sudo kubeadm version

sudo swapoff -a

###########Below step apply only on Master node

sudo hostnamectl set-hostname master

######Below step apply onlyon Worker node

sudo hostnamectl set-hostname worker

#** Below step apply only on Master node**

sudo kubeadm init --pod-network-cidr=10.244.0.0/16




Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.122.73:6443 --token kwy6zf.k0lfip6tqzt3urzf \
  --discovery-token-ca-cert-hash sha256:66efbe5d56fa82dac91c81eaf260170bf6b2306881c9c6dba11e2f789d83705c 







kubectl get nodes

sudo wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

kubectl apply -f kube-flannel.yml

kubectl get nodes

kubeadm token create --print-join-command

hai@ubuntu:~$ kubeadm token create --print-join-command
kubeadm join 192.168.68.137:6443 --token 1bkysu.dydhuqmbuxa63546 --discovery-token-ca-cert-hash sha256:1ea0e848089c60445897d2d0ad1d9493b95a7f0dd9f96c57848052bbd73dac8f 


#############Join to cluster from Worker nodes

kubectl get nodes

Metallb Loadbalancer deployment

kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.5/manifests/namespace.yaml

kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.5/manifests/metallb.yaml

kubectl create secret generic -n metallb-system memberlist --from-literal=secretkey="$(openssl rand -base64 128)"

kubectl -n metallb-system get all

Create a YAML file for configuring metallb IP pool

vi /tmp/metallb.yaml
**copy the contents of metallb.yaml that is attached below , save and exit **

metallb.yaml

kubectl create -f /tmp/metallb.yaml

kubectl -n metallb-system get all

Deploying a sample Nginx web application

kubectl create deploy nginx --image nginx

kubectl get deployments

kubectl expose deploy nginx --port 80 --type LoadBalancer

kubectl get svc

hai  ALL=(ALL) NOPASSWD:ALL

}}{{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}    ERROR  "ERROR "
hai@master:~$ sudo kubeadm init --pod-network-cidr=10.244.0.0/16
I1218 06:45:37.145708  633010 version.go:256] remote version is much newer: v1.26.0; falling back to: stable-1.25
[init] Using Kubernetes version: v1.25.5
[preflight] Running pre-flight checks
error execution phase preflight: [preflight] Some fatal errors occurred:
	[ERROR Port-6443]: Port 6443 is in use
	[ERROR Port-10259]: Port 10259 is in use
	[ERROR Port-10257]: Port 10257 is in use
	[ERROR FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml]: /etc/kubernetes/manifests/kube-apiserver.yaml already exists
	[ERROR FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml]: /etc/kubernetes/manifests/kube-controller-manager.yaml already exists
	[ERROR FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml]: /etc/kubernetes/manifests/kube-scheduler.yaml already exists
	[ERROR FileAvailable--etc-kubernetes-manifests-etcd.yaml]: /etc/kubernetes/manifests/etcd.yaml already exists
	[ERROR Port-2379]: Port 2379 is in use
	[ERROR Port-2380]: Port 2380 is in use
	[ERROR DirAvailable--var-lib-etcd]: /var/lib/etcd is not empty
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher

hai@master:~$ sudo rm -rf /etc/kubernetes/manifests/kube-apiserver.yaml 

hai@master:~$ sudo rm -rf /etc/kubernetes/manifests/kube-controller-manager.yaml 

hai@master:~$ sudo rm  -rf  /etc/kubernetes/manifests/kube-scheduler.yaml

hai@master:~$ sudo rm -rf /etc/kubernetes/manifests/etcd.yaml

   only MASTER   =:==  ERROR ERROR  ERROR  
https://stackoverflow.com/questions/72504257/i-encountered-when-executing-kubeadm-init-error-issue
ANSWER 
sudo rm  -rf /etc/containerd/config.toml
sudo  systemctl restart containerd


=====================================================================================================================
####################################################################################################################################
## commands real time 
https://www.youtube.com/channel/UCTIESbYFCPhiDZReih-j9kA
rajesh 
https://www.youtube.com/watch?v=-suYI4pFNQo&list=PL8SR-mMnL9f6KKYDnrG3saVnyLqNDz1GP
------------------------------------
https://stackoverflow.com/questions/53559291/kubernetes-coredns-in-crashloopbackoff
hai@master:~$ sudo systemctl  restart  kubelet
[sudo] password for hai: 
hai@master:~$ sudo systemctl  restart  kubelet 
hai@master:~$ sudo systemctl daemon-reload
hai@master:~$ kubectl get pods --all-namespaces
NAMESPACE        NAME                             READY   STATUS                       RESTARTS        AGE  
default          lg-54b6dc5746-8v27q              1/1     Running                      0               3h25m
default          nginx                            1/1     Running                      0               47h  
default          nginx-76d6c9b8c-h9vns            1/1     Running                      0               14d  
kube-flannel     kube-flannel-ds-7v8ds            1/1     Running                      0               15d
kube-flannel     kube-flannel-ds-hbg8d            1/1     Running                      0               15d
kube-system      coredns-565d847f94-8drrl         0/1     Running                      378 (65s ago)   15d
kube-system      coredns-565d847f94-nnthx         0/1     Running                      378 (69s ago)   15d
kube-system      etcd-master                      1/1     Running                      0               15d
kube-system      kube-apiserver-master            1/1     Running                      0               15d
kube-system      kube-controller-manager-master   1/1     Running                      0               15d
kube-system      kube-proxy-hsm4r                 1/1     Running                      0               15d
kube-system      kube-proxy-r2x99                 1/1     Running                      0               15d
kube-system      kube-scheduler-master            1/1     Running                      0               15d
metallb-system   controller-6fdc8d5477-stg47      1/1     Running                      0               14d
metallb-system   speaker-4x92g                    0/1     CreateContainerConfigError   0               15d
#=================================
hai@master:~$ kubectl get pods -n kube-system
NAME                             READY   STATUS    RESTARTS        AGE
coredns-565d847f94-8drrl         0/1     Running   381 (63s ago)   15d
coredns-565d847f94-nnthx         0/1     Running   381 (67s ago)   15d
etcd-master                      1/1     Running   0               15d
kube-apiserver-master            1/1     Running   0               15d
kube-controller-manager-master   1/1     Running   0               15d
kube-proxy-hsm4r                 1/1     Running   0               15d
kube-proxy-r2x99                 1/1     Running   0               15d
kube-scheduler-master            1/1     Running   0               15d
## or  or  or  or 
hai@master:~$ kubectl -n kube-system get pods 
NAME                             READY   STATUS    RESTARTS        AGE
coredns-565d847f94-8drrl         0/1     Running   382 (12s ago)   15d
coredns-565d847f94-nnthx         0/1     Running   382 (16s ago)   15d
etcd-master                      1/1     Running   0               15d
kube-apiserver-master            1/1     Running   0               15d
kube-controller-manager-master   1/1     Running   0               15d
kube-proxy-hsm4r                 1/1     Running   0               15d
kube-proxy-r2x99                 1/1     Running   0               15d
kube-scheduler-master            1/1     Running   0               15d
##===============================
https://github.com/kubernetes/kubeadm/issues/1162
hai@master:~$ kubectl logs -n kube-system coredns-565d847f94-8drrl 
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 591cf328cccc12bc490481273e738df59329c62c0b729d94e8b61db9961c2fa5f046dd37f1cf888b953814040d180f52594972691cd6ff41be96639138a43908
CoreDNS-1.9.3
linux/amd64, go1.18.2, 45b0a11
[ERROR] plugin/errors: 2 7252264926370649471.8801835257175202880. HINFO: read udp 10.244.0.5:40554->192.168.68.2:53: i/o timeout
[WARNING] plugin/kubernetes: Kubernetes API connection failure: Get "https://10.96.0.1:443/version": dial tcp 10.96.0.1:443: connect: no route to host
[ERROR] plugin/errors: 2 7252264926370649471.8801835257175202880. HINFO: read udp 10.244.0.5:35893->192.168.68.2:53: i/o timeout
[ERROR] plugin/errors: 2 7252264926370649471.8801835257175202880. HINFO: read udp 10.244.0.5:42322->192.168.68.2:53: i/o timeout
[ERROR] plugin/errors: 2 7252264926370649471.8801835257175202880. HINFO: read udp 10.244.0.5:33926->192.168.68.2:53: i/o timeout
[ERROR] plugin/errors: 2 7252264926370649471.8801835257175202880. HINFO: read udp 10.244.0.5:43235->192.168.68.2:53: i/o timeout
[WARNING] plugin/kubernetes: Kubernetes API connection failure: Get "https://10.96.0.1:443/version": dial tcp 10.96.0.1:443: connect: no route to host
[ERROR] plugin/errors: 2 7252264926370649471.8801835257175202880. HINFO: read udp 10.244.0.5:59094->192.168.68.2:53: i/o timeout
[ERROR] plugin/errors: 2 7252264926370649471.8801835257175202880. HINFO: read udp 10.244.0.5:44565->192.168.68.2:53: i/o timeout
[ERROR] plugin/errors: 2 7252264926370649471.8801835257175202880. HINFO: read udp 10.244.0.5:51991->192.168.68.2:53: i/o timeout
[ERROR] plugin/errors: 2 7252264926370649471.8801835257175202880. HINFO: read udp 10.244.0.5:58672->192.168.68.2:53: i/o timeout
[WARNING] plugin/kubernetes: Kubernetes API connection failure: Get "https://10.96.0.1:443/version": dial tcp 10.96.0.1:443: connect: no route to host
[ERROR] plugin/errors: 2 7252264926370649471.8801835257175202880. HINFO: read udp 10.244.0.5:37747->192.168.68.2:53: i/o timeout
[WARNING] plugin/kubernetes: Kubernetes API connection failure: Get "https://10.96.0.1:443/version": dial tcp 10.96.0.1:443: connect: no route to host
[WARNING] plugin/kubernetes: Kubernetes API connection failure: Get "https://10.96.0.1:44
### =========================================
hai@master:~$ kubectl logs  -n  kube-system coredns-565d847f94-nnthx
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 591cf328cccc12bc490481273e738df59329c62c0b729d94e8b61db9961c2fa5f046dd37f1cf888b953814040d180f52594972691cd6ff41be96639138a43908     
CoreDNS-1.9.3
linux/amd64, go1.18.2, 45b0a11
[ERROR] plugin/errors: 2 4964736449842461070.1693789343325947944. HINFO: read udp 10.244.0.4:46391->192.168.68.2:53: i/o timeout
[WARNING] plugin/kubernetes: Kubernetes API connection failure: Get "https://10.96.0.1:443/version": dial tcp 10.96.0.1:443: connect: no route to host
[ERROR] plugin/errors: 2 4964736449842461070.1693789343325947944. HINFO: read udp 10.244.0.4:35687->192.168.68.2:53: i/o timeout
[WARNING] plugin/kubernetes: Kubernetes API connection failure: Get "https://10.96.0.1:443/version": dial tcp 10.96.0.1:443: connect: no route to host
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s
[WARNING] plugin/kubernetes: Kubernetes API connection failure: Get "https://10.96.0.1:443/version": dial tcp 10.96.0.1:443: connect: no route to host
##======================---------------------------------======
 sudo systemctl daemon-reload
 sudo systemctl restart kubelet
 $ kubectl get pods -n kube-system -oname |grep coredns |xargs kubectl delete -n kube-system
 hai@master:~$ kubectl  get pods --all-namespaces 
NAMESPACE        NAME                             READY   STATUS                       RESTARTS        AGE
default          lg-54b6dc5746-8v27q              1/1     Running                      0               3h47m
default          nginx                            1/1     Running                      0               47h  
default          nginx-76d6c9b8c-h9vns            1/1     Running                      0               14d  
kube-flannel     kube-flannel-ds-7v8ds            1/1     Running                      0               15d  
kube-flannel     kube-flannel-ds-hbg8d            1/1     Running                      0               15d  
kube-system      coredns-565d847f94-8drrl         0/1     Running                      385 (43s ago)   15d  
kube-system      coredns-565d847f94-nnthx         0/1     Running                      385 (42s ago)   15d  
kube-system      etcd-master                      1/1     Running                      0               15d  
kube-system      kube-apiserver-master            1/1     Running                      0               15d  
kube-system      kube-controller-manager-master   1/1     Running                      0               15d  
kube-system      kube-proxy-hsm4r                 1/1     Running                      0               15d  
kube-system      kube-proxy-r2x99                 1/1     Running                      0               15d  
kube-system      kube-scheduler-master            1/1     Running                      0               15d  
metallb-system   controller-6fdc8d5477-stg47      1/1     Running                      0               14d  
metallb-system   speaker-4x92g                    0/1     CreateContainerConfigError   0               15d  
####=================================
kubectl -n kube-system delete pod -l k8s-app=kube-dns

root@mylaptop:~# hostnamectl  set-hostname  i7laptop
root@mylaptop:~# bash
root@i7laptop:~# 

kubectl install in linux
https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/

root@i7laptop:~# curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   138  100   138    0     0    388      0 --:--:-- --:--:-- --:--:--   388
100 45.7M  100 45.7M    0     0  3555k      0  0:00:13  0:00:13 --:--:-- 3798k
root@i7laptop:~# ll
-rw-r--r--  1 root root 48021504 Dec 13 01:06 kubectl

root@i7laptop:~# chmod +x kubectl 
root@i7laptop:~# mv  kubectl  /usr/local/bin/
hai@master:~$ cd /etc/kubernetes/
hai@master:/etc/kubernetes$ ll

-rwxrwxrwx   1 root root  5642 Nov 27 07:35 admin.conf*
-rw-------   1 root root  5674 Nov 27 07:35 controller-manager.conf
-rw-------   1 root root  1962 Nov 27 07:36 kubelet.conf
drwxr-xr-x   2 root root  4096 Nov 27 07:35 manifests/
drwxr-xr-x   3 root root  4096 Nov 27 07:35 pki/
-rw-------   1 root root  5622 Nov 27 07:35 scheduler.conf
##==== ---------
hai@master:/etc/kubernetes$ cd pki 
hai@master:/etc/kubernetes/pki$ ll
total 68
drwxr-xr-x 3 root root 4096 Nov 27 07:35 ./
drwxr-xr-x 4 root root 4096 Nov 27 07:35 ../
-rw-r--r-- 1 root root 1281 Nov 27 07:35 apiserver.crt
-rw-r--r-- 1 root root 1155 Nov 27 07:35 apiserver-etcd-client.crt   
-rw------- 1 root root 1675 Nov 27 07:35 apiserver-etcd-client.key   
-rw------- 1 root root 1679 Nov 27 07:35 apiserver.key
-rw-r--r-- 1 root root 1164 Nov 27 07:35 apiserver-kubelet-client.crt
-rw------- 1 root root 1679 Nov 27 07:35 apiserver-kubelet-client.key
-rw-r--r-- 1 root root 1099 Nov 27 07:35 ca.crt
-rw------- 1 root root 1679 Nov 27 07:35 ca.key
drwxr-xr-x 2 root root 4096 Nov 27 07:35 etcd/
-rw-r--r-- 1 root root 1115 Nov 27 07:35 front-proxy-ca.crt
-rw------- 1 root root 1675 Nov 27 07:35 front-proxy-ca.key
-rw-r--r-- 1 root root 1119 Nov 27 07:35 front-proxy-client.crt      
-rw------- 1 root root 1679 Nov 27 07:35 front-proxy-client.key      
-rw------- 1 root root 1675 Nov 27 07:35 sa.key
-rw------- 1 root root  451 Nov 27 07:35 sa.pub
# == ---  = -----
hai@master:/etc/kubernetes/manifests$ ll
total 24
drwxr-xr-x 2 root root 4096 Nov 27 07:35 ./
drwxr-xr-x 4 root root 4096 Nov 27 07:35 ../
-rw------- 1 root root 2382 Nov 27 07:35 etcd.yaml
-rw------- 1 root root 4019 Nov 27 07:35 kube-apiserver.yaml
-rw------- 1 root root 3520 Nov 27 07:35 kube-controller-manager.yaml
-rw------- 1 root root 1440 Nov 27 07:35 kube-scheduler.yaml
#== -- ==--=-----
nano /etc/ssh/sshd_config

    permitRootLogin  yes
    passwordAuthenticated  yes

root@ubuntu:~# passwd
New password: 
Retype new password: 
passwd: password updated successfully

root@master:/etc/kubernetes# scp admin.conf  root@192.168.68.134:/root
The authenticity of host '192.168.68.134 (192.168.68.134)' can't be established.
ECDSA key fingerprint is SHA256:WERuPb5G60GuhPYHZmaedTWpEp+1SPbNgsWfKVmlXBU.
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
Warning: Permanently added '192.168.68.134' (ECDSA) to the list of known hosts.
root@192.168.68.134's password: 
admin.conf                                                                  100% 5642   422.5KB/s   00:00    
root@master:/etc/kubernetes# 
#=== ---=====================
root@ubuntu:~# mkdir  .kube
root@ubuntu:~# mv  admin.conf  .kube/config
root@ubuntu:~# ll
drwxr-xr-x  2 root root 4096 Dec 13 04:15 .kube/
root@ubuntu:~# cd .kube/
root@ubuntu:~/.kube# ll
-rwxr-xr-x 1 root root 5642 Dec 13 04:10 config*

hai@i7laptop:~$ kubectl get nodes 
NAME     STATUS   ROLES           AGE   VERSION
master   Ready    control-plane   15d   v1.25.4
worker   Ready    <none>          15d   v1.25.4

https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-reset/

preflight           Run reset pre-flight checks
remove-etcd-member  Remove a local etcd member.
cleanup-node        Run cleanup node.

##=====================
kubeadm token list
https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-token/
https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-join/

hai@master:~$ openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'
8f3c5c8c74a13b2d34856b6775c8aceb46c7d2410517671d9deb80672a1657ab

hai@master:~$ kubeadm token list 
TOKEN                     TTL         EXPIRES                USAGES                   DESCRIPTION                     EXTRA GROUPS
ypfouv.gvxkq8gb7sqqpazh   23h         2022-12-15T05:22:11Z   authentication,signing   <none>                          system:bootstrappers:kubeadm:default-node-token

hai@master:~$ kubeadm token create  --ttl 10m
wnl2to.xqjr8wrl16a5g6o1
hai@master:~$ kubeadm token list 
TOKEN                     TTL         EXPIRES                USAGES                   DESCRIPTION                                                EXTRA GROUPS
wnl2to.xqjr8wrl16a5g6o1   9m          2022-12-14T05:53:44Z   authentication,signing   <none>                                                     system:bootstrappers:kubeadm:default-node-token
ypfouv.gvxkq8gb7sqqpazh   23h         2022-12-15T05:22:11Z   authentication,signing   <none>                                                     system:bootstrappers:kubeadm:default-node-token

---- 
hai@master:~$ kubeadm token create --ttl 0
wnmi2n.3mrwpd0hghlqdqa5
hai@master:~$ kubeadm token list 
TOKEN                     TTL         EXPIRES                USAGES                   DESCRIPTION                                                EXTRA GROUPS
wnl2to.xqjr8wrl16a5g6o1   1m          2022-12-14T05:53:44Z   authentication,signing   <none>                                                     system:bootstrappers:kubeadm:default-node-token
wnmi2n.3mrwpd0hghlqdqa5   <forever>   <never>   authentication,signing   <none>                                                     system:bootstrappers:kubeadm:default-node-token
ypfouv.gvxkq8gb7sqqpazh   23h         2022-12-15T05:22:11Z   authentication,signing   <none>                                                     system:bootstrappers:kubeadm:default-node-token
####==----
hai@master:~$ kubeadm token create --print-join-command
kubeadm join 192.168.68.138:6443 --token jnd0hu.5r5sctc0g4cktp4o --discovery-token-ca-cert-hash sha256:8f3c5c8c74a13b2d34856b6775c8aceb46c7d2410517671d9deb80672a1657ab 

###  if  suppose  we  add  thee new  token  first genereate the token and  apply   kubeadm  reset on   worker node , then  again add with new token join --
and  delete  worker node in  master cluster 
hai@worker:~$ kubeadm reset 
hai@master:~$ kubectl delete node worker
hai@worker:~$ sudo rm -rf /etc/kubernetes/pki/ca.crt
hai@worker:~$ sudo rm -rf  /etc/kubernetes/kubelet.conf

sudo kubeadm join 192.168.68.138:6443 --token  wnmi2n.3mrwpd0hghlqdqa5  --discovery-token-ca-cert-hash sha256:8f3c5c8c74a13b2d34856b6775c8aceb46c7d2410517671d9deb80672a1657ab 
#---------================
Token-based discovery without CA pinning 
https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-join/

CA public key, using   --discovery-token-unsafe-skip-ca-verification  

kubeadm join 192.168.68.138:6443 --token jnd0hu.5r5sctc0g4cktp4o --discovery-token-ca-cert-hash sha256:8f3c5c8c74a13b2d34856b6775c8aceb46c7d2410517671d9deb80672a1657ab 
kubeadm join 192.168.68.138:6443 --token jnd0hu.5r5sctc0g4cktp4o   --discovery-token-unsafe-skip-ca-verification 

##############################################
Turning off auto-approval of node client certificates
https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-join/

hai@master:~$ kubectl delete clusterrolebinding kubeadm:node-autoapprove-bootstrap
clusterrolebinding.rbac.authorization.k8s.io "kubeadm:node-autoapprove-bootstrap" deleted


hai@master:~/test$ kubectl get csr

hai@worker:~$ sudo sudo kubeadm join 192.168.68.138:6443 --token  wnmi2n.3mrwpd0hghlqdqa5  --discovery-token-ca-cert-hash sha256:8f3c5c8c74a13b2d34856b6775c8aceb46c7d2410517671d9deb80672a1657ab 
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...-------...............................................???

hai@master:~/test$ kubectl get csr
NAME        AGE   SIGNERNAME                                    REQUESTOR                 REQUESTEDDURATION   CONDITION
csr-ksd9v   10m   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:sgn48v   <none>              Approved,Issued
csr-wffc8   16m   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:sgn48v   <none>              Pending
csr-ztxrb   21m   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:sgn48v   <none>              Pending
hai@master:~/test$ kubectl certificate approve csr-ksd9v
certificatesigningrequest.certificates.k8s.io/csr-ksd9v approved

hai@master:~/test$ kubectl certificate approve node-csr-c69HXe7aY

kubectl get csr


------
https://github.com/postfinance/kubelet-csr-approver


#================================================================================
https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-join/

https://qinlj.github.io/docs/reference/setup-tools/kubeadm/kubeadm-join/

Example kubeadm join commands:

kubeadm join --discovery-file path/to/file.conf (local file)

kubeadm join --discovery-file https://url/file.conf (remote HTTPS URL)

Advantages:
Allows bootstrapping nodes to securely discover a root of trust for the control-plane node even if the network or other worker nodes are compromised.

Disadvantages:
Requires that you have some way to carry the discovery information from the control-plane node to the bootstrapping nodes. If the discovery file contains credentials you must keep it secret and transfer it over a secure channel. This might be possible with your cloud provider or provisioning tool

https://github.com/kubernetes/website/blob/main/content/en/docs/reference/setup-tools/kubeadm/kubeadm-join.md

===========##########################################################################
Working With Kubernetes Objects

https://www.containiq.com/post/kubernetes-objects
https://www.htown-tech.com/blogs/kubernetes-its-8-types-of-objects

Kubernetes & Its 8 Types of Objects

Kubernetes Objects:
The Kubernetes Platform contains control over the resources related to Storage and Compute. These resources are regarded as Objects, and it contains 8 Key objects.

1. Pods:

Being a higher-level abstraction grouping containerized component, it consists of one or more containers that can co-exist on the host system and share resources. With each Pod having a unique IP Address in a particular cluster, it allows the usage of ports without any conflicts.

2. Replica Sets:

At any time, it is needed to maintain a stable set of running replica Pods. This is maintained by Replica sets. Also, its purpose is to manage the availability of the required number of identical Pods.

3. Services:

A Kubernetes service is defined by a set of pods that work together. These sets of pods are defined with a label selector. The service discovery can happen in two different modes, using environmental variables or Kubernetes DNS.

4. Volumes:

By default, ephemeral storage will be provided by the File Systems of Kubernetes. This form of storage will remove all the data stored in such containers when the Pod is restarted. The Kubernetes Volume will provide persistent storage such that the data exists for the whole lifetime of the Pod.

5. Name Spaces:

The large number of resources managed by Kubernetes are separated into multiple non-overlapping sets. These sets are referred to as Namespaces. These are generally used when a large number of users exist in the form of multiple teams or projects.

6. ConfigMaps and Secrets:

Some of the Configuration Data may contain confidential information which makes the storage and maintenance of configuration information a challenge. ConfigMaps and Secrets are two mechanisms provided by Kubernetes that can deal with this problem. Both methods permit changes in the configuration without application-build.

7. Stateful Sets:

Stateless applications are easier to handle in terms of scaling. Because all it needs is to add up the number of Pods. But for Stateful workloads, you need to maintain the states when the Pod is restarted, and the state may need to be redistributed for scaling. The stateful sets provided by Kubernetes are used to run stateful Applications ensuring uniqueness and ordering of the instances of a Pod.

8. Daemon Sets:

Generally, Kubernetes Scheduler is responsible for deciding the location where Pods are run. It is done by the algorithm. This mode of scheduling the Pods is implemented by the feature called Daemon Sets.
 

â€‹For your business, container management is as much needed as container creation. This is best handled by Kubernetes. And all the objects provided by the Kubernetes act as resources to fulfil its purpose.

##
hai@master:~/test$ kubectl api-resources
NAME                              SHORTNAMES   APIVERSION                             NAMESPACED   KIND   #  kind models = 54 models 
bindings                                       v1                                     true         Binding        
componentstatuses                 cs           v1                                     false        ComponentStatus
configmaps                        cm           v1                                     true         ConfigMap
endpoints                         ep           v1                                     true         Endpoints
events                            ev           v1                                     true         Event
limitranges                       limits       v1                                     true         LimitRange
namespaces                        ns           v1                                     false        Namespace
nodes                             no           v1                                     false        Node
persistentvolumeclaims            pvc          v1                                     true         PersistentVolumeClaim
persistentvolumes                 pv           v1                                     false        PersistentVolume     
pods                              po           v1                                     true         Pod
podtemplates                                   v1                                     true         PodTemplate
replicationcontrollers            rc           v1                                     true         ReplicationController
resourcequotas                    quota        v1                                     true         ResourceQuota        
secrets                                        v1                                     true         Secret
serviceaccounts                   sa           v1                                     true         ServiceAccount       
services                          svc          v1                                     true         Service
mutatingwebhookconfigurations                  admissionregistration.k8s.io/v1        false        MutatingWebhookConfiguration
validatingwebhookconfigurations                admissionregistration.k8s.io/v1        false        ValidatingWebhookConfiguration
customresourcedefinitions         crd,crds     apiextensions.k8s.io/v1                false        CustomResourceDefinition
apiservices                                    apiregistration.k8s.io/v1              false        APIService
controllerrevisions                            apps/v1                                true         ControllerRevision
daemonsets                        ds           apps/v1                                true         DaemonSet
deployments                       deploy       apps/v1                                true         Deployment
replicasets                       rs           apps/v1                                true         ReplicaSet
statefulsets                      sts          apps/v1                                true         StatefulSet
tokenreviews                                   authentication.k8s.io/v1               false        TokenReview
localsubjectaccessreviews                      authorization.k8s.io/v1                true         LocalSubjectAccessReview
selfsubjectaccessreviews                       authorization.k8s.io/v1                false        SelfSubjectAccessReview
selfsubjectrulesreviews                        authorization.k8s.io/v1                false        SelfSubjectRulesReview
subjectaccessreviews                           authorization.k8s.io/v1                false        SubjectAccessReview
horizontalpodautoscalers          hpa          autoscaling/v2                         true         HorizontalPodAutoscaler
cronjobs                          cj           batch/v1                               true         CronJob
jobs                                           batch/v1                               true         Job
certificatesigningrequests        csr          certificates.k8s.io/v1                 false        CertificateSigningRequest
leases                                         coordination.k8s.io/v1                 true         Lease
endpointslices                                 discovery.k8s.io/v1                    true         EndpointSlice
events                            ev           events.k8s.io/v1                       true         Event
flowschemas                                    flowcontrol.apiserver.k8s.io/v1beta2   false        FlowSchema
prioritylevelconfigurations                    flowcontrol.apiserver.k8s.io/v1beta2   false        PriorityLevelConfiguration
ingressclasses                                 networking.k8s.io/v1                   false        IngressClass
ingresses                         ing          networking.k8s.io/v1                   true         Ingress
networkpolicies                   netpol       networking.k8s.io/v1                   true         NetworkPolicy
runtimeclasses                                 node.k8s.io/v1                         false        RuntimeClass
poddisruptionbudgets              pdb          policy/v1                              true         PodDisruptionBudget
clusterrolebindings                            rbac.authorization.k8s.io/v1           false        ClusterRoleBinding
clusterroles                                   rbac.authorization.k8s.io/v1           false        ClusterRole
rolebindings                                   rbac.authorization.k8s.io/v1           true         RoleBinding
roles                                          rbac.authorization.k8s.io/v1           true         Role
priorityclasses                   pc           scheduling.k8s.io/v1                   false        PriorityClass
csidrivers                                     storage.k8s.io/v1                      false        CSIDriver
csinodes                                       storage.k8s.io/v1                      false        CSINode
csistoragecapacities                           storage.k8s.io/v1                      true         CSIStorageCapacity
storageclasses                    sc           storage.k8s.io/v1                      false        StorageClass
volumeattachments                              storage.k8s.io/v1                      false        VolumeAttachment
#==============================================
pod
container
ephermel containers
replication  controllers
replica sets
deployments
statefullset
daemon sets
job
cronjob
horizontal pod auto scaler

----##
https://kubernetes.io/docs/reference/

https://kubernetes.io/docs/reference/kubernetes-api/

Workload Resources
Service Resources
Config and Storage Resources
Authentication Resources
Authorization Resources
Policy Resources
Extend Resources
Cluster Resources
Common Definitions
Other Resources
Common Parameters
########################################
Workload Resources  : -- 
https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/
Pod
Pod is a collection of containers that can run on a host.

PodTemplate
PodTemplate describes a template for creating copies of a predefined pod.

ReplicationController
ReplicationController represents the configuration of a replication controller.

ReplicaSet
ReplicaSet ensures that a specified number of pod replicas are running at any given time.

Deployment
Deployment enables declarative updates for Pods and ReplicaSets.

StatefulSet
StatefulSet represents a set of pods with consistent identities.

ControllerRevision
ControllerRevision implements an immutable snapshot of state data.

DaemonSet
DaemonSet represents the configuration of a daemon set.

Job
Job represents the configuration of a single job.

CronJob
CronJob represents the configuration of a single cron job.

HorizontalPodAutoscaler
configuration of a horizontal pod autoscaler.

HorizontalPodAutoscaler
HorizontalPodAutoscaler is the configuration for a horizontal pod autoscaler, which automatically manages the replica count of any resource implementing the scale subresource based on the metrics specified.

PriorityClass
PriorityClass defines mapping from a priority class name to the priority integer value.

PodScheduling v1alpha1
PodScheduling objects hold information that is needed to schedule a Pod with ResourceClaims that use "WaitForFirstConsumer" allocation mode.

ResourceClaim v1alpha1
ResourceClaim describes which resources are needed by a resource consumer.

ResourceClaimTemplate v1alpha1
ResourceClaimTemplate is used to produce ResourceClaim objects.

ResourceClass v1alpha1
ResourceClass is used by administrators to influence how resources are allocated.
#=----////////////////////////////////////////////////////////////////////////////////
Service Resources

Service
Service is a named abstraction of software service (for example, mysql) consisting of local port (for example 3306) that the proxy listens on, and the selector that determines which pods will answer requests sent through the proxy.

Endpoints
Endpoints is a collection of endpoints that implement the actual service.

EndpointSlice
EndpointSlice represents a subset of the endpoints that implement a service.

Ingress
Ingress is a collection of rules that allow inbound connections to reach the endpoints defined by a backend.

IngressClass
IngressClass represents the class of the Ingress, referenced by the Ingress Spec.
#+++++++================================#//////////
Config and Storage Resources

ConfigMap
ConfigMap holds configuration data for pods to consume.

Secret
Secret holds secret data of a certain type.

Volume
Volume represents a named volume in a pod that may be accessed by any container in the pod.

PersistentVolumeClaim
PersistentVolumeClaim is a user's request for and claim to a persistent volume.

PersistentVolume
PersistentVolume (PV) is a storage resource provisioned by an administrator.

StorageClass
StorageClass describes the parameters for a class of storage for which PersistentVolumes can be dynamically provisioned.

VolumeAttachment
VolumeAttachment captures the intent to attach or detach the specified volume to/from the specified node.

CSIDriver
CSIDriver captures information about a Container Storage Interface (CSI) volume driver deployed on the cluster.

CSINode
CSINode holds information about all CSI drivers installed on a node.

CSIStorageCapacity
CSIStorageCapacity stores the result of one CSI GetCapacity call.
-####################################################################################
Authentication Resources

ServiceAccount
ServiceAccount binds together: * a name, understood by users, and perhaps by peripheral systems, for an identity * a principal that can be authenticated and authorized * a set of secrets.

TokenRequest
TokenRequest requests a token for a given service account.

TokenReview
TokenReview attempts to authenticate a token to a known user.

CertificateSigningRequest
CertificateSigningRequest objects provide a mechanism to obtain x509 certificates by submitting a certificate signing request, and having it asynchronously approved and issued.
-#############################################################################
Authorization Resources

LocalSubjectAccessReview
LocalSubjectAccessReview checks whether or not a user or group can perform an action in a given namespace.

SelfSubjectAccessReview
SelfSubjectAccessReview checks whether or the current user can perform an action.

SelfSubjectRulesReview
SelfSubjectRulesReview enumerates the set of actions the current user can perform within a namespace.

SubjectAccessReview
SubjectAccessReview checks whether or not a user or group can perform an action.

SelfSubjectReview v1alpha1
SelfSubjectReview contains the user information that the kube-apiserver has about the user making this request.

ClusterRole
ClusterRole is a cluster level, logical grouping of PolicyRules that can be referenced as a unit by a RoleBinding or ClusterRoleBinding.

ClusterRoleBinding
ClusterRoleBinding references a ClusterRole, but not contain it.

Role
Role is a namespaced, logical grouping of PolicyRules that can be referenced as a unit by a RoleBinding.

RoleBinding
RoleBinding references a role, but does not contain it.
-###################################################################
Policy Resources

LimitRange
LimitRange sets resource usage limits for each kind of resource in a Namespace.

ResourceQuota
ResourceQuota sets aggregate quota restrictions enforced per namespace.

NetworkPolicy
NetworkPolicy describes what network traffic is allowed for a set of Pods.

PodDisruptionBudget
PodDisruptionBudget is an object to define the max disruption that can be caused to a collection of pods.
-######################################################################
Extend Resources
CustomResourceDefinition
CustomResourceDefinition represents a resource that should be exposed on the API server.

MutatingWebhookConfiguration
MutatingWebhookConfiguration describes the configuration of and admission webhook that accept or reject and may change the object.

ValidatingWebhookConfiguration
ValidatingWebhookConfiguration describes the configuration of and admission webhook that accept or reject and object without changing it.

ValidatingAdmissionPolicy v1alpha1
ValidatingAdmissionPolicy describes the definition of an admission validation policy that accepts or rejects an object without changing it.
-###########################################################
Cluster Resources
Node
Node is a worker node in Kubernetes.

Namespace
Namespace provides a scope for Names.

Event
Event is a report of an event somewhere in the cluster.

APIService
APIService represents a server for a particular GroupVersion.

Lease
Lease defines a lease concept.

RuntimeClass
RuntimeClass defines a class of container runtime supported in the cluster.

FlowSchema v1beta3
FlowSchema defines the schema of a group of flows.

PriorityLevelConfiguration v1beta3
PriorityLevelConfiguration represents the configuration of a priority level.

Binding
Binding ties one object to another; for example, a pod is bound to a node by a scheduler.

ComponentStatus
ComponentStatus (and ComponentStatusList) holds the cluster validation info.

ClusterCIDR v1alpha1
ClusterCIDR represents a single configuration for per-Node Pod CIDR allocations when the MultiCIDRRangeAllocator is enabled (see the config for kube-controller-manager).
-############################################################################
Common Definitions
DeleteOptions
DeleteOptions may be provided when deleting an API object.

LabelSelector
A label selector is a label query over a set of resources.

ListMeta
ListMeta describes metadata that synthetic resources must have, including lists and various status objects.

LocalObjectReference
LocalObjectReference contains enough information to let you locate the referenced object inside the same namespace.

NodeSelectorRequirement
A node selector requirement is a selector that contains values, a key, and an operator that relates the key and values.

ObjectFieldSelector
ObjectFieldSelector selects an APIVersioned field of an object.

ObjectMeta
ObjectMeta is metadata that all persisted resources must have, which includes all objects users must create.

ObjectReference
ObjectReference contains enough information to let you inspect or modify the referred object.

Patch
Patch is provided to give a concrete name and type to the Kubernetes PATCH request body.

Quantity
Quantity is a fixed-point representation of a number.

ResourceFieldSelector
ResourceFieldSelector represents container resources (cpu, memory) and their output format.

Status
Status is a return value for calls that don't return other objects.

TypedLocalObjectReference
TypedLocalObjectReference contains enough information to let you locate the typed referenced object inside the same namespace.
-######################################################################
Other Resources

ValidatingAdmissionPolicyBindingList v1alpha1
-#################################################
Common Parameters
allowWatchBookmarks
allowWatchBookmarks requests watch events with type "BOOKMARK". Servers that do not implement bookmarks may ignore this flag and bookmarks are sent at the server's discretion. Clients should not assume bookmarks are returned at any specific interval, nor may they assume the server will send any BOOKMARK event during a session. If this is not a watch, this field is ignored.

continue
The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the "next key".

This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.

dryRun
When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed

fieldManager
fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.

fieldSelector
A selector to restrict the list of returned objects by their fields. Defaults to everything.

fieldValidation
fieldValidation instructs the server on how to handle objects in the request (POST/PUT/PATCH) containing unknown or duplicate fields, provided that the ServerSideFieldValidation feature gate is also enabled. Valid values are: - Ignore: This will ignore any unknown fields that are silently dropped from the object, and will ignore all but the last duplicate field that the decoder encounters. This is the default behavior prior to v1.23 and is the default behavior when the ServerSideFieldValidation feature gate is disabled. - Warn: This will send a warning via the standard warning response header for each unknown field that is dropped from the object, and for each duplicate field that is encountered. The request will still succeed if there are no other errors, and will only persist the last of any duplicate fields. This is the default when the ServerSideFieldValidation feature gate is enabled. - Strict: This will fail the request with a BadRequest error if any unknown fields would be dropped from the object, or if any duplicate fields are present. The error returned from the server will contain all unknown and duplicate fields encountered.

force
Force is going to "force" Apply requests. It means user will re-acquire conflicting fields owned by other people. Force flag must be unset for non-apply patch requests.

gracePeriodSeconds
The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately.

labelSelector
A selector to restrict the list of returned objects by their labels. Defaults to everything.

limit
limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.

The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.

namespace
object name and auth scope, such as for teams and projects

pretty
If 'true', then the output is pretty printed.

propagationPolicy
Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground.

resourceVersion
resourceVersion sets a constraint on what resource versions a request may be served from. See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.

Defaults to unset

resourceVersionMatch
resourceVersionMatch determines how resourceVersion is applied to list calls. It is highly recommended that resourceVersionMatch be set for list calls where resourceVersion is set See https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions for details.

Defaults to unset

timeoutSeconds
Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity.

watch
Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion.
-#######################################################################
https://kubernetes.io/docs/reference/
One-page API Reference for Kubernetes v1.26
https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/

API Groups
The API Groups and their versions are summarized in the following table.

Group	Version
admissionregistration.k8s.io	v1, v1alpha1
apiextensions.k8s.io	v1
apiregistration.k8s.io	v1
apps	v1
authentication.k8s.io	v1, v1alpha1
authorization.k8s.io	v1
autoscaling	v2, v1
batch	v1
certificates.k8s.io	v1
coordination.k8s.io	v1
core	v1
discovery.k8s.io	v1
events.k8s.io	v1
flowcontrol.apiserver.k8s.io	v1beta3, v1beta2
internal.apiserver.k8s.io	v1alpha1
networking.k8s.io	v1, v1alpha1
node.k8s.io	v1
policy	v1
rbac.authorization.k8s.io	v1
resource.k8s.io	v1alpha1
scheduling.k8s.io	v1
storage.k8s.io	v1, v1beta1
#####3###################################
Imperative vs. Declarative â€” a Kubernetes Tutorial

There are two basic ways to deploy to Kubernetes: imperatively, with the many kubectl commands, or declaratively, by writing manifests and using kubectl apply.

---#=========================================
hai@master:~/test$ kubectl config view 
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://192.168.68.138:6443     
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: {}
users:
- name: kubernetes-admin
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
##########################################################
hai@master:~/.kube$ cat config 
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMvakNDQWVhZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdw
    cmRXSmwKY201bGRHVnpNQjRYRFRJeU1URXlOekUxTXpVek1sb1hEVE15TVRFeU5ERTFNelV6TWxvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BU
    UVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHdTCjNHM3Y4Z0NXeWZSUFhXZGY4VGR4czFmTW9BWjYvdDZWd3I3OE1RWDRxU0RJajlvQ3lPdVFuRldoQzNQMlZUL2IKWVZGTjl5NmE1ZHQyUUR4Unp
    LZ3NEVktCeExaeFZ1T1V4R2pVcTJJYzNBOEN1a0ZNeUdGNjRmSG5heDlTWHRBOApORFJoQ21IUlVxcEhlc3d0MUV5YU5SeXUwZ3dialVxRXVvZFlDV0w0NU1qWTBhOFhMMWlIUGtqTDNyV3FXNm
    5yCjd3aEZoOVVZV2hnTTNkRWlQY0I3OFBRVHVhcmF1NTBzTVpMTFhQVEhpc2ZxaVVzZEtRajluWkRHakoyajB5ZkkKN1ZzZjNXMWlmdTdvWUlCWW05dWZFMUF4NVE2QmEwSzF0UHVYRGE2Nllza
    0c0TkxSOVhOZVIwT1RKMzUzVFZFNAp3ZFJoeGJLR240eXZMamVnODJzQ0F3RUFBYU5aTUZjd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZ
    RUZKZkgwcG13b2lLMXhhVVRVdWZ1OWxNczRKRDJNQlVHQTFVZEVRUU8KTUF5Q0NtdDFZbVZ5Ym1WMFpYTXdEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUJBRm03RmRyRGpzT1E4aitSTmtwQgp6R1l
    4U3NKL0dkOHl1dkhCYzI4ckFmVlRMTzk2c1NjRC82QVpXaXl6NFdSTlFCTVhpZG1QdENTVFlqQnV5T1kwCjdybXFMVW4xVVVMcTE4RG9LYU96ZVMxUjNnRVlXSmtWWmhobkRUc3RIYXBZVWVrYVd
    qZmQ4WVBQU29YdWo0TmMKeHFaMFloS1lvQ01LQXRic0xrUENCcDVYR09qV0k1bzEwUjZ2YWN0cU01M21sQlpINnZhb1UwNzZZd1c3WklDWAovMjAvb1J6Qi91YS9zL2hBZmJvR2ZDaE5IUFFmOE5l
    V2ZWdTR6WnZsVTNQYTFoeHQzRnlzRnk0TkF5MjhnSEVsCnBzZlN4Ums5N0NWTnY5Y0hyOTNtSWNOTnR3SFBJTHhNaGx1SEJ1WTFRSWpRbGRkU24yOEd5QlNsQlhTSmFFbTIKd21rPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    server: https://192.168.68.138:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: {}
users:
- name: kubernetes-admin
  user:
    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURJVENDQWdtZ0F3SUJBZ0lJZlBPNEhVa21NWWN3RFFZSktvWklodmNOQVFFT
    JRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TWpFeE1qY3hOVE0xTXpKYUZ3MHlNekV4TWpjeE5UTTFNelJhTURReApGekFWQmdOVkJBb1REb
    k41YzNSbGJUcHRZWE4wWlhKek1Sa3dGd1lEVlFRREV4QnJkV0psY201bGRHVnpMV0ZrCmJXbHVNSUlCSWpBTkJna3Foa2lHOXcwQkFRRUZBQU9DQVE4QU1JSUJDZ0tD
    QVFFQTVEUlQ1TC9TR01wTGVRaGQKV0ExN2JpSEhFWSsxWFN3eUQ1RTdnclp1b3dHSDlpMnloUDdrSTZZcHpWRHpyZ3hRV20rV0xucjJFK0s1YzNONwo1cCt2ZnEwQkV
    DSDFQcklVOU5OSEFDWXRRZkpaMCtCV2pQT2VBVXB6dXN3d3poT1hwS2t3WThOdFVNbTBMRVVsCnVha2NiOTYvSDBtc0VjV2s5akZnbjlQVytXeVZYdWN2eHVBVTIvR0
    ZKa1kwdFBmaTBnNGF5WkdZWHVncExmVngKMUcxd2djTy81U1lBZ0xscklETnBzSUtjOFlSOWVMT2Ewc1FuSFJld0ExYlZEbWR4Z04vUGRSNXgxOUVxWnUzNgpyNmpPS
    m1lRFN5VVZDR3NDMU84dDJqL0YxUjVQMmluQVQ5cjQvd3VyOEw3M3M3Qk5valdBVWZKUFA3dG1KK29DCnU5ek5vUUlEQVFBQm8xWXdWREFPQmdOVkhROEJBZjhFQkFN
    Q0JhQXdFd1lEVlIwbEJBd3dDZ1lJS3dZQkJRVUgKQXdJd0RBWURWUjBUQVFIL0JBSXdBREFmQmdOVkhTTUVHREFXZ0JTWHg5S1pzS0lpdGNXbEUxTG43dlpUTE9DUQo
    5akFOQmdrcWhraUc5dzBCQVFzRkFBT0NBUUVBa2k5SWVIRVZHeWpma3o4OHY4SzRvMDkyNHpJYjhVTEYybC9VCnZkb3Yzby9NYmJCR3BxbWV0MzlPZTdaT0VqYzlDeH
    Z6MjUxZmo4aW81cVBFdEFBZjl1RzBMeldwaHBhaUJtSVYKemIzWUhwQXB4RVNvcDhMa1FGL2RYSEZmdkFpOFBKQUV4Y2V4cGx1aCt1Q3NPUHlQdm5Xc0dOb2xXNkNOL1
    lXNgovWmFDVFJ4OERzMENLb0E4OVUxRVhkM3BQSncrcHQwc0NMT1p4L0w0U2h5Nnh3SnM2cnRBdGhPVW94dE0wa242CnBZOXc4bjhSMDhOVWNMNGhYQ2JGcXQ5cWQrU
    0tjQlFhZFZYMklWSXdQNCtEUmFsdmF1NEN0UHdxTWQrZlZxclMKVnp0alJKNW9vVWh4amhKQTlIOUNuWFQ2WVpJNnZhelVTUTI3SmpscXd3WWU5c3Nhb2c9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==


    client-key-data: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFcGdJQkFBS0NBUUVBNURSVDVML1NHTXBMZVFoZFdBMTdiaUhIRVkrMVhTd3lENUU3Z3JadW93R
    0g5aTJ5CmhQN2tJNllwelZEenJneFFXbStXTG5yMkUrSzVjM043NXArdmZxMEJFQ0gxUHJJVTlOTkhBQ1l0UWZKWjArQlcKalBPZUFVcHp1c3d3emhPWHBLa3dZOE50VU1tMExFVWx1
    YWtjYjk2L0gwbXNFY1drOWpGZ245UFcrV3lWWHVjdgp4dUFVMi9HRkprWTB0UGZpMGc0YXlaR1lYdWdwTGZWeDFHMXdnY08vNVNZQWdMbHJJRE5wc0lLYzhZUjllTE9hCjBzUW5IUmV3QT
    FiVkRtZHhnTi9QZFI1eDE5RXFadTM2cjZqT0ptZURTeVVWQ0dzQzFPOHQyai9GMVI1UDJpbkEKVDlyNC93dXI4TDczczdCTm9qV0FVZkpQUDd0bUorb0N1OXpOb1FJREFRQUJBb0lCQVFERG
    1nNm1yYng1dnp1OQp2R3UyQTZoZWw5azN0a1F3WHFkeUxId2o2QmNnNVRrU3k4eU9ycVFCNG5WR0pUTVpaamlocTYvQW0yaTlWc0s2CnNLdnMyOGJpLzRzL3RydXRSNXNxMXNXdTNRcEIwbW
    NvY3N2OCtQdmdBMTc3Si9aVlRQbllDNlVNZmYvL0ZVUzQKQlhUMmxIdHBjRFhGcmJJdGNZbVo5R3pyMGZvYncxWVdyTVlhZUNXRnE0Z284RDdpM3psY1JuVGRaODYyT2piaApWUHlrQXBjcnl
    ZVWFmOURmckVKR1lQMWZKVUxsMDRQMXZxb2dxUWMzK0ZqNTlDWGtCbXBYMWxtbTZDQ0ZtSTU3CldnRDl2NkU2QkNLYjA3eUtiaHdnb1EvYVZZUXJ6bFIxbjJ1ZHFSZ3JCUyt1bFc4bm5UTXZwY
    3l1ZzdNVFQ4MUsKRHowYm84Q2hBb0dCQVBvRnN2dHBZLzQrOUtpVzEvbEoxSzl5VmlFenRLYzhrWkxIdGEvbUNPMXJreGl5Q0lINApUSysvNkpFaFVxMjdyNTJUM1k1M282eUhiVThvS20rRzNW
    U3RneC9rRGc0QVFvZUJCZWd0b3dIaWtZKy9wbHVqClJZTXdCcm05MXVLaERXaXQ5YUV3RU5sZndvcUVmRmVVZy8yYkNKNGYrU3B4anczZFdaWEtPVFYxQW9HQkFPbXAKRnNMVUxxUmtCRjlxWU02
    aithb29KZTF5Q2pSRml5QzBtU1RZeEcvcXhqeWg1ZkdyaWJLN2owQnRqNUt6cnVVeApXK0wvVFVBQVBtbVBKTm00aFFabkdLOUFROERHYlJFdk5CT3MrZ1I1MTZBbHBjbVU3bGdRdzVlWXZDRFd4
    c1gxCm9xcUpBMzIvWE5lVDlVV0hFVzRoSmJNblZiWi9JeGRFejN2VlF2WDlBb0dCQUxyNDNCcHJmQjI4RXRhb3g2WmoKY2l3cVF3TGRXZkhldDdhZXB0NStGcHNHWWFDTU14U1BEVG81TjhDZ3ZP
    V3pmK2RGbHVCZDFBYkgyQlRrSXFmNApvVGdiOGYwOVhNMVhvR2taTWNPcHJVZFJtaTEzMHY2Z29QRTBUek5FSnBpZ2ZCaVdUeVJWZjZRdm1wcGY3V1RKCmFkT1R4dVFKWVJvK2hnNkdRK0ppc2xt
    cEFvR0JBTE5wR3NuMEEvQkR2N3ZId1pkSS9yMnhZQlhtdDRFVldOTisKK0F2N3lURXA4cGJCdFA2UU9RV2MwRXluRTFPUVZoMHpmaHRZN21iVENSa2lTU2hIYnhUUWVucXQvSmY3ZytscQo5akNa
    WTh1bUJuTzRGSWtvcXEzQ0NYelFVTHRpVG5QWHZOUDJxbENXYStJM2dGK25hekhGajkzMHVQS013bFB1CmlPWExoVU85QW9HQkFJL1RhQ1c5VTBSTWxCQUJKQjBVVFk4dXdTU2pnNVptTnNjOVFy
    VlIzU0tjRVIrdTQ0NTEKUXBpL2JJbEJWVzlLalMwUWZjaEFQMXY5NHpmdmF5U0hqR2hYZlI0VDRJWTNJSGVDNk1SL0hVT2NNUTFtOS8zNwppamJiSWR4Mk5WZ0lmUzRVZ0M2UnBqU04wck5TTSt6
    eEk2ZGtRdUNFaDIzYkowSHR0TXJEOVQvUwotLS0tLUVORCBSU0EgUFJJVkFURSBLRVktLS0tLQo=
#################################################################################################
hai@master:~/.kube$ kubectl explain pod
KIND:     Pod
VERSION:  v1

DESCRIPTION:
     Pod is a collection of containers that can run on a host. This resource is
     created by clients and scheduled onto hosts.

FIELDS:
   apiVersion   <string>
     APIVersion defines the versioned schema of this representation of an      
     object. Servers should convert recognized schemas to the latest internal  
     value, and may reject unrecognized values. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources

   kind <string>
     Kind is a string value representing the REST resource this object
     represents. Servers may infer this from the endpoint the client submits
     requests to. Cannot be updated. In CamelCase. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds

   metadata     <Object>
     Standard object's metadata. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata

   spec <Object>
     Specification of the desired behavior of the pod. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

   status       <Object>
     Most recently observed status of the pod. This data may not be up to date.
     Populated by the system. Read-only. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

#====================
https://www.youtube.com/watch?v=Fg9J8qp36V0&t=398s

What are Kubernetes Pods?
A pod is the smallest execution unit in Kubernetes. A pod encapsulates one or more applications. Pods are ephemeral by nature, if a pod (or the node it executes on) fails, Kubernetes can automatically create a new replica of that pod to continue operations. Pods include one or more containers (such as Docker containers).

https://kubernetes.io/docs/tasks/debug/debug-application/get-shell-running-container/
https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#exec
kubectl exec --stdin --tty nginx-76d6c9b8c-4zhgz  -- /bin/bash
kubectl exec -i -t my-pod --container main-app -- /bin/bash

==################
hai@master:~$ kubectl explain deployment
KIND:     Deployment
VERSION:  apps/v1

DESCRIPTION:
     Deployment enables declarative updates for Pods and ReplicaSets.

FIELDS:
   apiVersion   <string>
     APIVersion defines the versioned schema of this representation of an
     object. Servers should convert recognized schemas to the latest internal
     value, and may reject unrecognized values. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources

   kind <string>
     Kind is a string value representing the REST resource this object
     represents. Servers may infer this from the endpoint the client submits
     requests to. Cannot be updated. In CamelCase. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds

   metadata     <Object>
     Standard object's metadata. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata

   spec <Object>
     Specification of the desired behavior of the Deployment.

   status       <Object>
     Most recently observed status of the Deployment.
-#----------------------------------------------

apiVersion: apps/v1
kind: Deployment
metadata:
   name: myredis
   labels: 
     app: myredis
spec: 
  selector:
     matchLabels:
        app: myredis
  replicas: 
  template:
     metadata: 
        labels:
           app:  myredis
     spec: 
        containers:
        -   name: myredis
            image: redis
            ports:
            -   containerPort: 6379
                name: myredis
----------------
hai@master:~/test$ kubectl create -f redis.yaml 

hai@master:~$ kubectl get deployments  -o wide 
NAME      READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS   IMAGES   SELECTOR   
myredis   1/1     1            1           2m47s   myredis      redis    app=myredis

hai@master:~$ kubectl get pod -o wide 
NAME                       READY   STATUS    RESTARTS   AGE     IP             NODE     NOMINATED NODE   READINESS GATES
myredis-5bd679c8ff-2j4xx   1/1     Running   0          3m23s   10.244.3.135   worker   <none>           <none>
#######
apiVersion: apps/v1
kind: Deployment
metadata:
   name: myredis
   labels: 
     app: myredis
spec: 
  selector:
     matchLabels:
        app: myredis
  replicas: 3    # patch work
  template:
     metadata: 
        labels:
           app:  myredis
     spec: 
        containers:
        -   name: myredis
            image: redis
            ports:
            -   containerPort: 6379
                name: myredis
hai@master:~/test$ kubectl create -f redis.yaml 
Error from server (AlreadyExists): error when creating "redis.yaml": deployments.apps "myredis" already exists
hai@master:~/test$ kubectl apply -f redis.yaml 
Warning: resource deployments/myredis is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
deployment.apps/myredis configured
------
hai@master:~/test$ kubectl get deployments  -o wide 
NAME      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES   SELECTOR
myredis   3/3     3            3           10m   myredis      redis    app=myredis

hai@master:~/test$ kubectl get pods -o wide 
NAME                       READY   STATUS    RESTARTS   AGE     IP             NODE     NOMINATED NODE   READINESS GATES
myredis-5bd679c8ff-2j4xx   1/1     Running   0          9m16s   10.244.3.135   worker   <none>           <none>
myredis-5bd679c8ff-4p95p   1/1     Running   0          2m14s   10.244.3.136   worker   <none>           <none>
myredis-5bd679c8ff-s8wsl   1/1     Running   0          2m14s   10.244.3.137   worker   <none>           <none>
####

apiVersion: apps/v1
kind: Deployment
metadata:
   name: myredis
   labels: 
     app: myredis
spec: 
  selector:
     matchLabels:
        app: myredis
  replicas: 5
  template:
     metadata: 
        labels:
           app:  myredis
     spec: 
        containers:
        -   name: myredis
            image: redis
            ports:
            -   containerPort: 6379
                name: myredis
###
hai@master:~/test$ kubectl get pods  -o wide 
NAME                       READY   STATUS    RESTARTS   AGE   IP             NODE     NOMINATED NODE   READINESS GATES
myredis-5bd679c8ff-2j4xx   1/1     Running   0          23m   10.244.3.135   worker   <none>           <none>
myredis-5bd679c8ff-4p95p   1/1     Running   0          16m   10.244.3.136   worker   <none>           <none>
myredis-5bd679c8ff-pn8gf   1/1     Running   0          17s   10.244.3.139   worker   <none>           <none>
myredis-5bd679c8ff-s2t59   1/1     Running   0          17s   10.244.3.138   worker   <none>           <none>
myredis-5bd679c8ff-s8wsl   1/1     Running   0          16m   10.244.3.137   worker   <none>           <none>
######
hai@master:~/test$ kubectl get pods -o wide
NAME                       READY   STATUS        RESTARTS   AGE     IP             NODE     NOMINATED NODE   READINESS GATES
myredis-5bd679c8ff-2j4xx   1/1     Terminating   0          26m     10.244.3.135   worker   <none>           <none>
myredis-5bd679c8ff-4p95p   1/1     Terminating   0          19m     10.244.3.136   worker   <none>           <none>
myredis-5bd679c8ff-pn8gf   1/1     Terminating   0          3m23s   10.244.3.139   worker   <none>           <none>
myredis-5bd679c8ff-s2t59   1/1     Terminating   0          3m23s   10.244.3.138   worker   <none>           <none>
myredis-5bd679c8ff-s8wsl   1/1     Running       0          19m     10.244.3.137   worker   <none>           <none>
hai@master:~/test$ kubectl get pods -o wide 
NAME                       READY   STATUS    RESTARTS   AGE   IP             NODE     NOMINATED NODE   READINESS GATES
myredis-5bd679c8ff-s8wsl   1/1     Running   0          19m   10.244.3.137   worker   <none>           <none>
###########################################################
hai@master:~/test$ kubectl get replicaset  -o wide 
NAME                 DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES   SELECTOR
myredis-5bd679c8ff   3         3         3       32m   myredis      redis    app=myredis,pod-template-hash=5bd679c8ff
#### 
apiVersion: apps/v1
kind: Deployment
metadata:
   name: myredis
   labels: 
     app: myredis
spec: 
  selector:
     matchLabels:
        app: myredis
  replicas: 3
  template:
     metadata: 
        labels:
           app:  myredis
     spec: 
        containers:
        -   name: myredis
            image: redis:alpine
            ports:
            -   containerPort: 6379
                name: myredis
------
            
hai@master:~/test$ kubectl apply -f redis.yaml

hai@master:~/test$ kubectl get replicaset   -o wide 
NAME                 DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES         SELECTOR
myredis-56f6bbf856   3         3         3       80s   myredis      redis:alpine   app=myredis,pod-template-hash=56f6bbf856
myredis-5bd679c8ff   0         0         0       40m   myredis      redis          app=myredis,pod-template-hash=5bd679c8ff
##
hai@master:~/test$ kubectl get deployment 
NAME      READY   UP-TO-DATE   AVAILABLE   AGE
myredis   3/3     3            3           73m

hai@master:~/test$ kubectl set image deployment/myredis   myredis=redis:latest  
deployment.apps/myredis image updated            # just watch modification 

hai@master:~/test$ kubectl get replicaset  -o  wide
NAME                 DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES         SELECTOR
myredis-56f6bbf856   0         0         0       38m   myredis      redis:alpine   app=myredis,pod-template-hash=56f6bbf856
myredis-5bd679c8ff   0         0         0       77m   myredis      redis          app=myredis,pod-template-hash=5bd679c8ff
myredis-6d77c6b664   3         3         3       84s   myredis      redis:latest   app=myredis,pod-template-hash=6d77c6b664   ### 3 3 3 is in id "" myredis-6d77c6b664"
hai@master:~/test$

hai@master:~/test$ kubectl rollout undo deployment/myredis  
deployment.apps/myredis rolled back

hai@master:~/test$ kubectl get replicaset -o wide 
NAME                 DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES         SELECTOR
myredis-56f6bbf856   3         3         3       45m     myredis      redis:alpine   app=myredis,pod-template-hash=56f6bbf856   ### 3 3 3  id  old  " myredis-56f6bbf856"
myredis-5bd679c8ff   0         0         0       84m     myredis      redis          app=myredis,pod-template-hash=5bd679c8ff
myredis-6d77c6b664   0         0         0       7m43s   myredis      redis:latest   app=myredis,pod-template-hash=6d77c6b664
#####################################################
hai@master:~/test$ kubectl get all
NAME                           READY   STATUS    RESTARTS   AGE
pod/myredis-56f6bbf856-lnpk5   1/1     Running   0          7m7s
pod/myredis-56f6bbf856-rnzxl   1/1     Running   0          7m12s
pod/myredis-56f6bbf856-s79qr   1/1     Running   0          7m17s

NAME                 TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
service/kubernetes   ClusterIP      10.96.0.1       <none>        443/TCP        17d
service/nginx        LoadBalancer   10.99.132.124   <pending>     80:30429/TCP   17d

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/myredis   3/3     3            3           90m      

NAME                                 DESIRED   CURRENT   READY   AGE
replicaset.apps/myredis-56f6bbf856   3         3         3       51m
replicaset.apps/myredis-5bd679c8ff   0         0         0       90m
replicaset.apps/myredis-6d77c6b664   0         0         0       14m
--##---
hai@master:~/test$ kubectl describe deployment.apps/myredis 
Name:                   myredis
Namespace:              default
CreationTimestamp:      Thu, 15 Dec 2022 00:28:57 -0800
Labels:                 app=myredis
Annotations:            deployment.kubernetes.io/revision: 4
Selector:               app=myredis
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=myredis
  Containers:
   myredis:
    Image:        redis:alpine
    Port:         6379/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   myredis-56f6bbf856 (3/3 replicas created)
Events:
  Type    Reason             Age                   From                   Message
  ----    ------             ----                  ----                   -------
  Normal  ScalingReplicaSet  59m (x2 over 84m)     deployment-controller  Scaled up   replica set myredis-5bd679c8ff to 3 from 1
  Normal  ScalingReplicaSet  52m                   deployment-controller  Scaled up   replica set myredis-56f6bbf856 to 1
  Normal  ScalingReplicaSet  52m                   deployment-controller  Scaled down replica set myredis-5bd679c8ff to 2 from 3
  Normal  ScalingReplicaSet  52m                   deployment-controller  Scaled down replica set myredis-5bd679c8ff to 1 from 2
  Normal  ScalingReplicaSet  52m                   deployment-controller  Scaled up   replica set myredis-56f6bbf856 to 3 from 2
  Normal  ScalingReplicaSet  52m                   deployment-controller  Scaled down replica set myredis-5bd679c8ff to 0 from 1
  Normal  ScalingReplicaSet  15m                   deployment-controller  Scaled up   replica set myredis-6d77c6b664 to 1
  Normal  ScalingReplicaSet  15m                   deployment-controller  Scaled down replica set myredis-56f6bbf856 to 2 from 3
  Normal  ScalingReplicaSet  15m                   deployment-controller  Scaled up   replica set myredis-6d77c6b664 to 2 from 1
  Normal  ScalingReplicaSet  15m                   deployment-controller  Scaled down replica set myredis-56f6bbf856 to 1 from 2
  Normal  ScalingReplicaSet  15m                   deployment-controller  Scaled up   replica set myredis-6d77c6b664 to 3 from 2
  Normal  ScalingReplicaSet  15m                   deployment-controller  Scaled down replica set myredis-56f6bbf856 to 0 from 1
  Normal  ScalingReplicaSet  8m21s                 deployment-controller  Scaled up   replica set myredis-56f6bbf856 to 1 from 0
  Normal  ScalingReplicaSet  8m16s (x2 over 52m)   deployment-controller  Scaled up   replica set myredis-56f6bbf856 to 2 from 1
  Normal  ScalingReplicaSet  8m16s                 deployment-controller  Scaled down replica set myredis-6d77c6b664 to 2 from 3
  Normal  ScalingReplicaSet  8m5s (x3 over 8m11s)  deployment-controller  (combined from similar events): Scaled down replica set myredis-6d77c6b664 to 0 from 1

hai@master:~/test$ kubectl rollout status deployment/myredis
deployment "myredis" successfully rolled out
hai@master:~/test$ kubectl rollout history deployment/myredis 
deployment.apps/myredis 
REVISION  CHANGE-CAUSE  
1         <none>        
3         <none>        
4         <none> 
##------
    
hai@master:~$ kubectl get ns  -o  wide      # or kubectl get namespaces    -o   wide
NAME              STATUS   AGE
default           Active   17d
kube-flannel      Active   17d
kube-node-lease   Active   17d
kube-public       Active   17d
kube-system       Active   17d
metallb-system    Active   17d
hai@master:~$  kubectl get pods -n default  -o  wide 
NAME                       READY   STATUS    RESTARTS   AGE     IP             NODE     NOMINATED NODE   READINESS GATES
myredis-56f6bbf856-lnpk5   1/1     Running   0          4h56m   10.244.3.150   worker   <none>           <none>
myredis-56f6bbf856-rnzxl   1/1     Running   0          4h56m   10.244.3.149   worker   <none>           <none>
myredis-56f6bbf856-s79qr   1/1     Running   0          4h56m   10.244.3.148   worker   <none>           <none>
hai@master:~$   kubectl get pods -n kube-flannel   -o  wide
NAME                    READY   STATUS    RESTARTS      AGE   IP               NODE     NOMINATED NODE   READINESS GATES
kube-flannel-ds-4phsm   1/1     Running   0             30h   192.168.68.145   worker   <none>           <none>
kube-flannel-ds-hbg8d   1/1     Running   2 (34h ago)   17d   192.168.68.138   master   <none>           <none>
hai@master:~$  kubectl get pods -n      kube-public   -o  wide 
No resources found in kube-public namespace.
hai@master:~$  kubectl get pods -n      kube-system  -o  wide 
NAME                             READY   STATUS             RESTARTS          AGE   IP               NODE     NOMINATED NODE   READINESS GATES
coredns-565d847f94-8drrl         0/1     CrashLoopBackOff   634 (3m28s ago)   17d   10.244.0.9       master   <none>           <none>
coredns-565d847f94-nnthx         0/1     CrashLoopBackOff   634 (3m28s ago)   17d   10.244.0.8       master   <none>           <none>
etcd-master                      1/1     Running            2 (34h ago)       17d   192.168.68.138   master   <none>           <none>
kube-apiserver-master            1/1     Running            2 (34h ago)       17d   192.168.68.138   master   <none>           <none>
kube-controller-manager-master   1/1     Running            2 (34h ago)       17d   192.168.68.138   master   <none>           <none>
kube-proxy-2sslb                 1/1     Running            0                 30h   192.168.68.145   worker   <none>           <none>
kube-proxy-r2x99                 1/1     Running            2 (34h ago)       17d   192.168.68.138   master   <none>           <none>
kube-scheduler-master            1/1     Running            2 (34h ago)       17d   192.168.68.138   master   <none>           <none>
hai@master:~$  kubectl get pods -n      kube-node-lease 
No resources found in kube-node-lease namespace.
hai@master:~$  kubectl get pods -n   metallb-system  
NAME                          READY   STATUS                       RESTARTS   AGE
controller-6fdc8d5477-t75nw   1/1     Running                      0          30h
speaker-6wqxk                 0/1     CreateContainerConfigError   0          30h
#------------------------------
kubectl create namespace venkata
hai@master:~$ kubectl get  ns
NAME              STATUS   AGE  
anjireddy         Active   2m48s
default           Active   17d  
kube-flannel      Active   17d  
kube-node-lease   Active   17d  
kube-public       Active   17d  
kube-system       Active   17d  
metallb-system    Active   17d  
velpula           Active   113s 
venkata           Active   2m24s
----
hai@master:~$ kubectl describe ns  anjireddy
Name:         anjireddy
Labels:       kubernetes.io/metadata.name=anjireddy
Annotations:  <none>
Status:       Active
No resource quota.
No LimitRange resource.

hai@master:~$ kubectl describe ns  venkata
Name:         venkata
Labels:       kubernetes.io/metadata.name=venkata
Annotations:  <none>
Status:       Active
No resource quota.
No LimitRange resource.
-#-========
kubectl run appserver  --image=nginx  -n  anjireddy
kubectl run appserver --image=nginx  -n  venkata
kubectl run appserver --image=nginx  -n velpula

hai@master:~$ kubectl get pods  -o  wide  -n  anjireddy
NAME         READY   STATUS    RESTARTS   AGE     IP             NODE     NOMINATED NODE   READINESS GATES
appserver    1/1     Running   0          3m2s    10.244.3.153   worker   <none>           <none>
dataserver   1/1     Running   0          3m21s   10.244.3.152   worker   <none>           <none>
webserver    1/1     Running   0          4m8s    10.244.3.151   worker   <none>           <none>
hai@master:~$ kubectl get pods  -o  wide  -n  velpula
NAME        READY   STATUS    RESTARTS   AGE     IP             NODE     NOMINATED NODE   READINESS GATES
appserver   1/1     Running   0          2m1s    10.244.3.155   worker   <none>           <none>
dbserver    1/1     Running   0          110s    10.244.3.156   worker   <none>           <none>
webserver   1/1     Running   0          2m11s   10.244.3.154   worker   <none>           <none>
hai@master:~$ kubectl get pods  -o  wide  -n  venkata
NAME        READY   STATUS    RESTARTS   AGE    IP             NODE     NOMINATED NODE   READINESS GATES
appserver   1/1     Running   0          93s    10.244.3.158   worker   <none>           <none>
dbserver    1/1     Running   0          104s   10.244.3.157   worker   <none>           <none>
webserver   1/1     Running   0          71s    10.244.3.159   worker   <none>           <none>
####################################################

apiVersion: v1
kind: Pod
metadata:
   name: appserver
   namespace: anjireddy
spec: 
   containers:
    - name: nginx
      image: nginx
      ports: 
       - name: nginx
         containerPort: 80
         protocol : TCP

hai@master:~/test$ kubectl apply  -f  ns.yaml 
pod/appserver created

hai@master:~$ kubectl get pods -n  anjireddy  -o  wide
NAME        READY   STATUS    RESTARTS   AGE    IP             NODE     NOMINATED NODE   READINESS GATES
appserver   1/1     Running   0          2m6s   10.244.3.160   worker   <none>           <none>

#############################################################
apiVersion: v1
kind: Pod
metadata: 
    name: appserver
    namespace: velpula
spec:
  containers:
     - name: nginx
       image: nginx
       ports: 
         - name: nginx
           containerPort: 80
           protocol: TCP

hai@master:~/test$ kubectl apply -f  nsve.yaml   -n  velpula
pod/appserver created

hai@master:~/test$ kubectl get pods -n  velpula
NAME        READY   STATUS    RESTARTS   AGE
appserver   1/1     Running   0          7s
############################################################
apiVersion: v1
kind: Pod
metadata: 
   name: appserver
   namespace: venkata
spec:
  containers:
    - name: nginx
      image: nginx
      ports: 
      - containerPort: 80
        name: nginx
        protocol: TCP

hai@master:~/test$ kubectl apply -f venka.yaml  
pod/appserver created

hai@master:~/test$ kubectl get pods  -n  venkata  -o wide
NAME        READY   STATUS    RESTARTS   AGE   IP             NODE     NOMINATED NODE   READINESS GATES
appserver   1/1     Running   0          41s   10.244.3.162   worker   <none>           <none>

hai@master:~/test$ kubectl delete -f venka.yaml  -n  venkata 
pod "appserver" deleted

hai@master:~/test$ kubectl get pods  -n  venkata  -o wide
No resources found in venkata namespace.
-############################################
hai@master:~$ kubectl exec -it  nginx   -- /bin/bash
root@nginx:/# ls
bin  boot  dev  docker-entrypoint.d  docker-entrypoint.sh  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var
root@nginx:/# 

###############################################################
hai@master:~$ kubectl get ns
NAME              STATUS   AGE
anjireddy         Active   13h
default           Active   18d
kube-flannel      Active   18d
kube-node-lease   Active   18d
kube-public       Active   18d
kube-system       Active   18d
metallb-system    Active   18d
velpula           Active   12h
venkata           Active   12h
-###
hai@master:~$ kubectl  config get-contexts 
CURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE
*         kubernetes-admin@kubernetes   kubernetes   kubernetes-admin   

hai@master:~$ kubectl  config set-context anjireddy1  --namespace=anjireddy  --user=kubernetes-admin --cluster=kubernetes
Context "anjireddy1" created.

hai@master:~$ kubectl config  get-contexts
CURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE
          anjireddy1                    kubernetes   kubernetes-admin   anjireddy
*         kubernetes-admin@kubernetes   kubernetes   kubernetes-admin

hai@master:~$ kubectl  config set-context velpula   --namespace=velpula  --user=kubernetes-admin --cluster=kubernetes
Context "velpula" created.

hai@master:~$ kubectl config set-context  venkata1  --namespace=venkata  --user=kubernetes-admin  --cluster=kubernetes
Context "venkata1" created.

hai@master:~$ kubectl config get-contexts
CURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE
          anjireddy1                    kubernetes   kubernetes-admin   anjireddy
***       kubernetes-admin@kubernetes   kubernetes   kubernetes-admin
          velpula                       kubernetes   kubernetes-admin   velpula
          venkata1                      kubernetes   kubernetes-admin   venkata

hai@master:~$ kubectl config use-context anjireddy1
Switched to context "anjireddy1".
hai@master:~$ kubectl config  get-contexts
CURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE
* **      anjireddy1                    kubernetes   kubernetes-admin   anjireddy
          kubernetes-admin@kubernetes   kubernetes   kubernetes-admin   
          velpula                       kubernetes   kubernetes-admin   velpula  
          venkata1                      kubernetes   kubernetes-admin   venkata  
hai@master:~$ kubectl config use-context anjireddy1
Switched to context "anjireddy1".
hai@master:~$ kubectl config  get-contexts
CURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE
*         anjireddy1                    kubernetes   kubernetes-admin   anjireddy
          kubernetes-admin@kubernetes   kubernetes   kubernetes-admin   
          velpula                       kubernetes   kubernetes-admin   velpula  
          venkata1                      kubernetes   kubernetes-admin   venkata  

hai@master:~$ kubectl get pods 
NAME        READY   STATUS    RESTARTS   AGE
appserver   1/1     Running   0          12h

hai@master:~$ kubectl run  redis --image=redis
pod/redis created

hai@master:~$ kubectl get pods  -o  wide 
NAME        READY   STATUS    RESTARTS   AGE   IP             NODE     NOMINATED NODE   READINESS GATES
appserver   1/1     Running   0          12h   10.244.3.160   worker   <none>           <none>
redis       1/1     Running   0          15s   10.244.3.166   worker   <none>           <none>

hai@master:~$ kubectl describe ns  anjireddy 
Name:         anjireddy
Labels:       kubernetes.io/metadata.name=anjireddy
Annotations:  <none>
Status:       Active
No resource quota.
-########################################################
hai@master:~$ kubectl get pods 
NAME        READY   STATUS    RESTARTS   AGE
appserver   1/1     Running   0          14h
redis       1/1     Running   0          83m
hai@master:~$ kubectl describe pod  redis
Name:             redis
Namespace:        anjireddy
Node:             worker/192.168.68.145
IP:               10.244.3.166
Containers:
  redis:
    Container ID:   containerd://89a3b1986891dfd975300b89d03a43458467cfe1003584d0af4b003ad8ff5602
    Image:          redis
##############################################################################
hai@master:~$ kubectl   config  get-contexts
CURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE
****      anjireddy1                    kubernetes   kubernetes-admin   anjireddy
          default
          kubernetes-admin@kubernetes   kubernetes   kubernetes-admin
          velpula                       kubernetes   kubernetes-admin   velpula  
          venkata1                      kubernetes   kubernetes-admin   venkata  

hai@master:~$ kubectl config use-context   kubernetes-admin@kubernetes
Switched to context "kubernetes-admin@kubernetes".

hai@master:~$ kubectl config get-contexts 
CURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE
          anjireddy1                    kubernetes   kubernetes-admin   anjireddy
          default
****      kubernetes-admin@kubernetes   kubernetes   kubernetes-admin
          velpula                       kubernetes   kubernetes-admin   velpula
          venkata1                      kubernetes   kubernetes-admin   venkata
###################
hai@master:~$ kubectl config get-contexts
CURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE
          anjireddy1                    kubernetes   kubernetes-admin   anjireddy
          default
***       kubernetes-admin@kubernetes   kubernetes   kubernetes-admin
          velpula                       kubernetes   kubernetes-admin   velpula
          venkata1                      kubernetes   kubernetes-admin   venkata

hai@master:~$ kubectl config delete-context anjireddy1
deleted context anjireddy1 from /home/hai/.kube/config

hai@master:~$ kubectl config get-contexts
CURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE
          default
***       kubernetes-admin@kubernetes   kubernetes   kubernetes-admin
          velpula                       kubernetes   kubernetes-admin   velpula
          venkata1                      kubernetes   kubernetes-admin   venkata

hai@master:~$ kubectl config delete-context  velpula 
deleted context velpula from /home/hai/.kube/config

hai@master:~$ kubectl config delete-context  venkata1
deleted context venkata1 from /home/hai/.kube/config

hai@master:~$ kubectl config get-contexts
CURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE
****         kubernetes-admin@kubernetes   kubernetes   kubernetes-admin
#######################################################################################
hai@master:~$ kubectl get pods 
NAME    READY   STATUS    RESTARTS   AGE 
nginx   1/1     Running   0          145m

hai@master:~$ kubectl   exec  -it  nginx  --  /bin/bash
root@nginx:/# ls
bin   dev                  docker-entrypoint.sh  home  lib64  mnt  proc  run   srv  tmp  var
boot  docker-entrypoint.d  etc                   lib   media  opt  root  sbin  sys  usr     

root@nginx:/#
hai@master:~$ kubectl describe pod nginx
Name:             nginx  
Namespace:        default

IP:               10.244.3.165
IPs:
  IP:  10.244.3.165
Containers:
  nginx:
    Container ID:   containerd://b1415f5e20cc0edebe9ef8226d8965189d8eed297fdd28367dc82d2a99e97bfa
    Image:          nginx
 
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
hai@master:~$ kubectl exec -it  <POD NAME>  -c  <container NAME> --  /bin/bash
#########################################################################################################
hai@master:~$ kubectl get pods -o wide
NAME    READY   STATUS    RESTARTS   AGE    IP             NODE     NOMINATED NODE   READINESS GATES
nginx   1/1     Running   0          3h4m   10.244.3.165   worker   <none>           <none>

hai@master:~$ kubectl port-forward  nginx 8089:80  --address  0.0.0.0              ###################### best ############################
Forwarding from 0.0.0.0:8089 -> 80                   ## see look port forword  port 
 Handling connection for 8089
Handling connection for 8089
^Z
[1]+  Stopped                 kubectl port-forward nginx 8089:80 --address 0.0.0.0
hai@master:~$  
========##############=================--------------------------------------
taints  taint 

hai@master:~$ kubectl describe node master
Name:               master
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=master
                    kubernetes.io/os=linux
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        flannel.alpha.coreos.com/backend-data: {"VNI":1,"VtepMAC":"02:c7:ec:a6:aa:31"}    
                    flannel.alpha.coreos.com/backend-type: vxlan
                    flannel.alpha.coreos.com/kube-subnet-manager: true
                    flannel.alpha.coreos.com/public-ip: 192.168.68.138
                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 27 Nov 2022 07:36:46 -0800
Taints:             node-role.kubernetes.io/control-plane:NoSchedule      ## best importent - taint  is off mode
Unschedulable:      false
Lease:
  HolderIdentity:  master
  AcquireTime:     <unset>
  RenewTime:       Thu, 15 Dec 2022 23:47:55 -0800
Conditions:
  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----                 ------  -----------------                 ------------------                ------                       -------
  NetworkUnavailable   False   Tue, 13 Dec 2022 20:44:10 -0800   Tue, 13 Dec 2022 20:44:10 -0800   FlannelIsUp                  Flannel is running on this node
  MemoryPressure       False   Thu, 15 Dec 2022 23:43:37 -0800   Sun, 27 Nov 2022 07:36:46 -0800   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure         False   Thu, 15 Dec 2022 23:43:37 -0800   Sun, 27 Nov 2022 07:36:46 -0800   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure          False   Thu, 15 Dec 2022 23:43:37 -0800   Sun, 27 Nov 2022 07:36:46 -0800   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready                True    Thu, 15 Dec 2022 23:43:37 -0800   Sun, 27 Nov 2022 07:51:50 -0800   KubeletReady                 kubelet is posting ready status. AppArmor enabled
Addresses:
  InternalIP:  192.168.68.138
  Hostname:    master
Capacity:
  cpu:                4
  ephemeral-storage:  71151768Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3983208Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  65573469281
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3880808Ki
  pods:               110
System Info:
  Machine ID:                 65b65a2b19884bb294b23d024674f4e1
  System UUID:                a0764d56-7953-c519-4b13-d8ece9e9158b
  Boot ID:                    a8407919-5a25-47b5-b450-9b0484afe50d
  Kernel Version:             5.15.0-56-generic
  OS Image:                   Ubuntu 20.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  containerd://1.6.12
  Kubelet Version:            v1.25.4
  Kube-Proxy Version:         v1.25.4
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (8 in total)
  Namespace                   Name                              CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                              ------------  ----------  ---------------  -------------  ---
  kube-flannel                kube-flannel-ds-hbg8d             100m (2%)     100m (2%)   50Mi (1%)        50Mi (1%)      18d
  kube-system                 coredns-565d847f94-8drrl          100m (2%)     0 (0%)      70Mi (1%)        170Mi (4%)     18d
  kube-system                 coredns-565d847f94-nnthx          100m (2%)     0 (0%)      70Mi (1%)        170Mi (4%)     18d
  kube-system                 etcd-master                       100m (2%)     0 (0%)      100Mi (2%)       0 (0%)         18d
  kube-system                 kube-apiserver-master             250m (6%)     0 (0%)      0 (0%)           0 (0%)         18d
  kube-system                 kube-controller-manager-master    200m (5%)     0 (0%)      0 (0%)           0 (0%)         18d
  kube-system                 kube-proxy-r2x99                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         18d
  kube-system                 kube-scheduler-master             100m (2%)     0 (0%)      0 (0%)           0 (0%)         18d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                950m (23%)  100m (2%)
  memory             290Mi (7%)  390Mi (10%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:              <none>
=================================================================================
hai@master:~$ kubectl describe node master   | grep -i  taints        ###  best  importent ###
Taints:             node-role.kubernetes.io/control-plane:NoSchedule

hai@master:~$ kubectl get nodes 
NAME     STATUS   ROLES           AGE   VERSION
master   Ready    control-plane   18d   v1.25.4
worker   Ready    <none>          47h   v1.25.4

hai@master:~$ kubectl get pods  -o  wide 
NAME    READY   STATUS    RESTARTS   AGE    IP             NODE     NOMINATED NODE   READINESS GATES
nginx   1/1     Running   0          4h3m   10.244.3.165   worker   <none>           <none>

hai@master:~$ kubectl run redis  --image=redis
pod/redis created

hai@master:~$ kubectl get pods  -o  wide
NAME    READY   STATUS              RESTARTS   AGE    IP             NODE     NOMINATED NODE   READINESS GATES
nginx   1/1     Running             0          4h4m   10.244.3.165   worker   <none>           <none>
redis   0/1     ContainerCreating   0          3s     <none>         worker   <none>           <none>

hai@master:~$ kubectl get pods  -o  wide 
NAME    READY   STATUS    RESTARTS   AGE    IP             NODE     NOMINATED NODE   READINESS GATES
nginx   1/1     Running   0          4h4m   10.244.3.165   worker   <none>           <none>
redis   1/1     Running   0          6s     10.244.3.168   worker   <none>           <none>

hai@master:~$ kubectl describe  node master |  grep -i taint              ###  best  importent ###
Taints:             node-role.kubernetes.io/control-plane:NoSchedule

hai@master:~$ kubectl taint node  master node-role.kubernetes.io/control-plane-       ###  best  importent ###
node/master untainted

hai@master:~$ kubectl describe node master | grep -i taint      ###  best  importent ###
Taints:             <none>

hai@master:~$ kubectl run nginx1  --image=nginx
pod/nginx1 created

hai@master:~$ kubectl run redis1  --image=redis
pod/redis1 created

hai@master:~$ kubectl get pods  -o wide 
NAME     READY   STATUS    RESTARTS   AGE     IP             NODE     NOMINATED NODE   READINESS GATES
nginx    1/1     Running   0          4h13m   10.244.3.165   worker   <none>           <none>
nginx1   1/1     Running   0          34s     10.244.3.169   master   <none>           <none>       ###  best  "importent" ###
redis    1/1     Running   0          9m8s    10.244.3.168   worker   <none>           <none>
redis1   1/1     Running   0          16s     10.244.3.170   master   <none>           <none>
---#===============================================================
hai@master:~$ kubectl describe  node master |  grep -i taint              ###  best  "importent-" ###
Taints:             <none>
hai@master:~$ kubectl  taint node  master node-role.kubernetes.io/control-plane:NoSchedule                ###  'best  importent" ###
node/master tainted
hai@master:~$ kubectl describe  node master |  grep -i taint              ###  best  importent ###
Taints:             node-role.kubernetes.io/control-plane:NoSchedule
======================================================================
hai@master:~$ kubectl delete pods --all
pod "app-server" deleted
pod "nginx" deleted
pod "nginx1" deleted
pod "redis" deleted
pod "redis1" deleted
-------====
hai@master:~$ kubectl run nginx --image=nginx
pod/nginx created

hai@master:~$ kubectl get pods  -o wide
NAME    READY   STATUS    RESTARTS   AGE   IP             NODE     NOMINATED NODE   READINESS GATES
nginx   1/1     Running   0          6s    10.244.3.172   worker   <none>           <none>

hai@master:~$ kubectl describe  node worker | grep -i  taint             ###  'best  importent" ###
Taints:             <none>

hai@master:~$ kubectl run app-server  --image=nginx
pod/app-server created

hai@master:~$ kubectl describe  node worker | grep -i  taint      ###  'best  importent'  
Taints:             <none>

hai@master:~$ kubectl taint node worker key=value:NoSchedule    ###  'best  importent" ###
node/worker tainted

hai@master:~$ kubectl describe  node worker | grep -i  taint     ###  'best  importent
Taints:             key=value:NoSchedule

hai@master:~$ kubectl get pods -o  wide 
NAME    READY   STATUS    RESTARTS   AGE   IP             NODE     NOMINATED NODE   READINESS GATES
nginx   1/1     Running   0          10m   10.244.3.172   worker   <none>           <none>

hai@master:~$ kubectl run webserver  --image=nginx
pod/webserver created

hai@master:~$ kubectl get pods 
NAME        READY   STATUS    RESTARTS   AGE
nginx       1/1     Running   0          14m
webserver   0/1     Pending   0          12s

hai@master:~$ kubectl run appserver  --image=nginx
pod/appserver created

hai@master:~$ kubectl run db server  --image=nginx
pod/db created

hai@master:~$ kubectl get pods
NAME        READY   STATUS    RESTARTS   AGE
appserver   0/1     Pending   0          11s     ###  'best  importent" ###
db          0/1     Pending   0          3s      ###  'best  importent" ###
nginx       1/1     Running   0          15m
webserver   0/1     Pending   0          62s
-=#########
hai@master:~$ kubectl describe node worker  |  grep -i taint      ###  'best  importent" ###
Taints:             key=value:NoSchedule

hai@master:~$ kubectl taint  node  worker key=value:NoSchedule-       ###  'best  importent" ###
node/worker untainted

hai@master:~$ kubectl describe node worker  |  grep -i taint
Taints:             <none>                                          ## "best

hai@master:~$ kubectl get pods  -o  wide 
NAME        READY   STATUS             RESTARTS     AGE     IP             NODE     NOMINATED NODE   READINESS GATES
appserver   1/1     Running            0            8m20s   10.244.3.175   worker   <none>           <none>
db          0/1     CrashLoopBackOff   1 (8s ago)   8m12s   10.244.3.173   worker   <none>           <none>
nginx       1/1     Running            0            23m     10.244.3.172   worker   <none>           <none>
webserver   1/1     Running            0            9m11s   10.244.3.174   worker   <none>           <none>


hai@master:~$ kubectl run app-server  --image=nginx
pod/app-server created

hai@master:~$ kubectl describe  node worker | grep -i  taint    ###  'best  importent" ###
Taints:             <none>

hai@master:~$ kubectl taint node worker key=value:NoSchedule   
node/worker tainted

hai@master:~$ kubectl describe  node worker | grep -i  taint      ###  'best  importent" ###
Taints:             key=value:NoSchedule

hai@master:~$ kubectl get pods -o  wide 
NAME    READY   STATUS    RESTARTS   AGE   IP             NODE     NOMINATED NODE   READINESS GATES
nginx   1/1     Running   0          10m   10.244.3.172   worker   <none>           <none>

hai@master:~$ kubectl run webserver  --image=nginx
pod/webserver created

hai@master:~$ kubectl get pods 
NAME        READY   STATUS    RESTARTS   AGE
nginx       1/1     Running   0          14m
webserver   0/1     Pending   0          12s

hai@master:~$ kubectl describe node worker  |  grep -i taint     ###  'best  importent" ###
Taints:             key=value:NoSchedule



hai@master:~$ kubectl taint  node  worker key=value:NoSchedule-
node/worker untainted


hai@master:~$ kubectl describe node worker  |  grep -i taint
Taints:             <none>


hai@master:~$ kubectl get pods  -o  wide
NAME        READY   STATUS             RESTARTS     AGE     IP             NODE     NOMINATED NODE   READINESS GATES

hai@master:~$ kubectl get pods 
NAME        READY   STATUS             RESTARTS      AGE
appserver   1/1     Running            0             11m

hai@master:~$ kubectl run appserver   --image=nginx
pod/appserver created


hai@master:~$ kubectl get pods -o  wide 
NAME        READY   STATUS    RESTARTS   AGE   IP             NODE     NOMINATED NODE   READINESS GATES
appserver   1/1     Running   0          22s   10.244.3.176   worker   <none>           <none>

hai@master:~$ kubectl  describe node  worker  |  grep -i  taint
Taints:             <none>

hai@master:~$ kubectl  taint node  worker key=value:NoExecute         or   ###  'best  importent" ### #####3==3###
node/worker tainted

hai@master:~$ kubectl  describe node  worker  |  grep -i  taint
Taints:             key=value:NoExecute

hai@master:~$ kubectl get nodes 
NAME     STATUS   ROLES           AGE    VERSION
master   Ready    control-plane   18d    v1.25.4
worker   Ready    <none>          2d1h   v1.25.4

hai@master:~$ kubectl get  pods -o  wide 
No resources found in default namespace.

hai@master:~$ kubectl  run   webserver  --image=nginx 
pod/webserver created
hai@master:~$ kubectl  run   appserver  --image=nginx
pod/appserver created
hai@master:~$ kubectl  run   dbserver  --image=nginx
pod/dbserver created

hai@master:~$ kubectl get  pods -o  wide
NAME        READY   STATUS    RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES
appserver   0/1     Pending   0          16s   <none>   <none>   <none>           <none>
dbserver    0/1     Pending   0          4s    <none>   <none>   <none>           <none>
webserver   0/1     Pending   0          30s   <none>   <none>   <none>           <none>
-#####################################################################################
hai@master:~$ kubectl get pods 
NAME        READY   STATUS    RESTARTS   AGE
appserver   0/1     Pending   0          27m
dbserver    0/1     Pending   0          27m
webserver   0/1     Pending   0          27m

hai@master:~$  kubectl describe  pod appserver
Name:             appserver    
Namespace:        default      
Status:           Pending      
IP:
IPs:              <none>
Containers:
  appserver:
    Image:        nginx
    Port:         <none>
    Host Port:    <none>
    Environment:  <none>
    
  Type     Reason            Age                  From               Message
  ----     ------            ----                 ----               -------
  Warning  FailedScheduling  4m15s (x6 over 29m)  default-scheduler  0/2 nodes are available: 1 node(s) had untolerated taint {key: value}, 1 node(s) 
  had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling.
####
hai@master:~$ kubectl  describe pod dbserver
Name:             dbserver
Namespace:        default
Status:           Pending
IP:
IPs:              <none>
Events:
  Type     Reason            Age                  From               Message
  ----     ------            ----                 ----               -------
  Warning  FailedScheduling  2m42s (x7 over 33m)  default-scheduler  0/2 nodes are available: 1 node(s) had untolerated taint {key: value}, 1 node(s) 
  had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling.

hai@master:~$ kubectl  describe pod webserver 
Name:             webserver
Namespace:        default
Status:           Pending
IP:
IPs:              <none>
Evets:
  Type     Reason            Age                  From               Message
  ----     ------            ----                 ----               -------
  Warning  FailedScheduling  3m41s (x7 over 34m)  default-scheduler  0/2 nodes are available: 1 node(s) had untolerated taint {key: value}, 1 node(s)
   had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling.

hai@master:~$ kubectl get pods -o wide 
NAME        READY   STATUS    RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES
appserver   0/1     Pending   0          42m   <none>   <none>   <none>           <none>
dbserver    0/1     Pending   0          41m   <none>   <none>   <none>           <none>
webserver   0/1     Pending   0          42m   <none>   <none>   <none>           <none>

hai@master:~$ kubectl taint   node  worker  key:NoExecute-          ###  'best  importent" ###
node/worker untainted

hai@master:~$ kubectl get pods  -o wide 
NAME        READY   STATUS    RESTARTS   AGE   IP             NODE     NOMINATED NODE   READINESS GATES
appserver   1/1     Running   0          43m   10.244.3.178   worker   <none>           <none>
dbserver    1/1     Running   0          43m   10.244.3.180   worker   <none>           <none>
webserver   1/1     Running   0          43m   10.244.3.177   worker   <none>           <none>

hai@master:~$ kubectl describe node  worker | grep -i  taint 
Taints:             <none>
====================================================================================================================================
LABELS    LABELS   LABELS     
hai@master:~$ kubectl label node worker  node=w1
node/worker labeled

---
hai@master:~$ kubectl describe node  worker
Name:               worker
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=worker
                    kubernetes.io/os=linux
                    node=w1                 ##"  best importent ---

2 00:09:05 -0800   Thu, 15 Dec 2022 19:32:07 -0800   KubeletReady                 kubelet is posting ready status. AppArmor enabled
Addresses:
  InternalIP:  192.168.68.145
  Hostname:    worker
Capacity:
  cpu:                4
  ephemeral-storage:  71151768Ki
  memory:             3983208Ki
Sy54
  Kernel Version:             5.15.0-56-generic
  OS Image:                   Ubuntu 20.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  containerd://1.6.10
  Kubelet Version:            v1.25.4
  Kube-Proxy Version:         v1.25.4eaker-jhrrx                  100m (2%)     100m (2%)   100Mi (2%)       100Mi (2%)     22h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                300m (7%)   300m (7%)
  memory             250Mi (6%)  250Mi (6%)

##########################-######

}}{{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}    ERROR  "ERROR "
hai@master:~$ sudo kubeadm init --pod-network-cidr=10.244.0.0/16
I1218 06:45:37.145708  633010 version.go:256] remote version is much newer: v1.26.0; falling back to: stable-1.25
[init] Using Kubernetes version: v1.25.5
[preflight] Running pre-flight checks
error execution phase preflight: [preflight] Some fatal errors occurred:
	[ERROR Port-6443]: Port 6443 is in use
	[ERROR Port-10259]: Port 10259 is in use
	[ERROR Port-10257]: Port 10257 is in use
	[ERROR FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml]: /etc/kubernetes/manifests/kube-apiserver.yaml already exists
	[ERROR FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml]: /etc/kubernetes/manifests/kube-controller-manager.yaml already exists
	[ERROR FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml]: /etc/kubernetes/manifests/kube-scheduler.yaml already exists
	[ERROR FileAvailable--etc-kubernetes-manifests-etcd.yaml]: /etc/kubernetes/manifests/etcd.yaml already exists
	[ERROR Port-2379]: Port 2379 is in use
	[ERROR Port-2380]: Port 2380 is in use
	[ERROR DirAvailable--var-lib-etcd]: /var/lib/etcd is not empty
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher

hai@master:~$ sudo rm -rf /etc/kubernetes/manifests/kube-apiserver.yaml 

hai@master:~$ sudo rm -rf /etc/kubernetes/manifests/kube-controller-manager.yaml 

hai@master:~$ sudo rm  -rf  /etc/kubernetes/manifests/kube-scheduler.yaml

hai@master:~$ sudo rm -rf /etc/kubernetes/manifests/etcd.yaml

   only MASTER   =:==  ERROR ERROR  ERROR  
https://stackoverflow.com/questions/72504257/i-encountered-when-executing-kubeadm-init-error-issue
ANSWER 

sudo rm -rf  /etc/kubernetes/kubelet.conf
sudo rm -rf /etc/kubernetes/pki
sudo systemctl restart containerd

[ERROR Port-6443]: Port 6443 is in use   }{}{{{{{{{[[   "error " ]]}}}}}}}

https://github.com/kubernetes/kubeadm/issues/339
https://github.com/kubernetes/kubeadm/issues/1145
https://discuss.kubernetes.io/t/kubeadm-init-fails/16888


$ netstat -lnp | grep 1025
tcp6       0      0 :::10251                :::*                    LISTEN      4366/kube-scheduler
tcp6       0      0 :::10252                :::*                    LISTEN      4353/kube-controlle
$ kill 4366
$ kill 4353


hai@master:~$ sudo netstat -lnp | grep 6443

sudo kill Process_PID




hai@master:~$ sudo netstat -lnp | grep 6443
hai@master:~$ sudo kubeadm init --pod-network-cidr=10.244.0.0/16
I1218 07:12:20.139997  637971 version.go:256] remote version is much newer: v1.26.0; falling back to: stable-1.25
[init] Using Kubernetes version: v1.25.5
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local master] and IPs [10.96.0.1 192.168.68.138]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [localhost master] and IPs [192.168.68.138 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [localhost master] and IPs [192.168.68.138 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
error execution phase kubeconfig/admin: a kubeconfig file "/etc/kubernetes/admin.conf" exists already but has got the wrong CA cert
To see the stack trace of this error execute with --v=5 or higher
hai@master:~$ kubectl get nodes 
The connection to the server 192.168.68.138:6443 was refused - did you specify the right host or port?
hai@master:~$ rm -rf  /etc/kubernetes/admin.conf
rm: cannot remove '/etc/kubernetes/admin.conf': Permission denied
hai@master:~$ sudo rm -rf  /etc/kubernetes/admin.conf


root@master:~# cd /etc/kubernetes/
root@master:/etc/kubernetes# ll
total 60
drwxr-xr-x   5 root root  4096 Dec 18 07:16 ./
drwxr-xr-x 141 root root 12288 Dec 18 06:53 ../
-rw-------   1 root root  5638 Dec 18 07:16 admin.conf
-rw-------   1 root root  5674 Nov 27 07:35 controller-manager.conf
-rw-------   1 root root  5650 Dec 18 07:16 kubelet.conf
drwxr-xr-x   2 root root  4096 Dec 18 06:47 manifests/
drwxr-xr-x   3 root root  4096 Dec 18 07:13 pki/
-rw-------   1 root root  5622 Nov 27 07:35 scheduler.conf
drwx------   4 root root  4096 Dec 13 09:10 tmp/
root@master:/etc/kubernetes# rm -rf  *
root@master:/etc/kubernetes# ll
total 16
drwxr-xr-x   2 root root  4096 Dec 18 07:16 ./
drwxr-xr-x 141 root root 12288 Dec 18 06:53 ../
root@master:/etc/kubernetes# 




Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.68.150:6443 --token dv9gyw.fo8f5ndssq1jh4xb \
	--discovery-token-ca-cert-hash sha256:9d71830a49fdc3e150b23c68d775586c3d5eeb9bba2ad715b79de2256b674309 
hai@master:~$ 




hai@master:~$ kubeadm token create --print-join-commandkubeadm join 192.168.68.150:6443 --token dv9gyw.fo8f5ndssq1jh4xb \
	--discovery-token-ca-cert-hash sha256:9d71830a49fdc3e150b23c68d775586c3d5eeb9bba2ad715b79de2256b674309 

https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-join/

 get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap.. 

hai@master:~/test$ kubectl get csr
NAME        AGE   SIGNERNAME                                    REQUESTOR                 REQUESTEDDURATION   CONDITION
csr-ksd9v   10m   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:sgn48v   <none>              Approved,Issued
csr-wffc8   16m   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:sgn48v   <none>              Pending
csr-ztxrb   21m   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:sgn48v   <none>              Pending

hai@master:~/test$ kubectl certificate approve csr-ksd9v
certificatesigningrequest.certificates.k8s.io/csr-ksd9v approved

hai@master:~$ kubectl label node ubuntu   node=worker2
node/ubuntu labeled

hai@master:~$ kubectl describe node ubuntu
Name:               ubuntu
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=ubuntu
                    kubernetes.io/os=linux
                    node=worker2       ##3=="ddf"
-###################
hai@master:~/test$ kubectl get node  -o wide 
NAME     STATUS   ROLES           AGE    VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME  
master   Ready    control-plane   19d    v1.25.4   192.168.68.138   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.12
"ubuntu" Ready    <none>          87m    v1.25.4   192.168.68.144   <none>        Ubuntu 20.04.5 LTS   5.15.0-53-generic   containerd://1.6.13
worker   Ready    <none>          3d3h   v1.25.4   192.168.68.145   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.10

apiVersion: v1
kind:  Pod
metadata:
  name: redis1
spec:
  containers:
  -   image: redis
      name:  redis
  nodeSelector: 
      node: "worker2"     

hai@master:~/test$ kubectl get po -o  wide 
NAME      READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES
freddis   1/1     Running   0          9s    10.244.4.14   "ubuntu"  <none>           <none>       #worker2
fredis    1/1     Running   0          39s   10.244.4.13   "ubuntu"   <none>           <none>
redis     1/1     Running   0          96s   10.244.4.12  "ubuntu"  <none>           <none>
hai@master:~/test$ 

https://kubernetes.io/docs/reference/
https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#pod-v1-core
https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#objectmeta-v1-meta

APIService [apiregistration/v1]
Binding [core/v1]
CSIDriver [storage/v1]
CSINode [storage/v1]
CSIStorageCapacity [storage/v1]
CSIStorageCapacity [storage/v1beta1]
CertificateSigningRequest [certificates/v1]
ClusterCIDR [networking/v1alpha1]
ClusterRole [rbac/v1]
ClusterRoleBinding [rbac/v1]
ComponentStatus [core/v1]
ConfigMap [core/v1]
ControllerRevision [apps/v1]
CronJob [batch/v1]
CustomResourceDefinition [apiextensions/v1]
DaemonSet [apps/v1]
Deployment [apps/v1]
EndpointSlice [discovery/v1]
Endpoints [core/v1]
Event [core/v1]
Event [events/v1]
Eviction [policy/v1]
FlowSchema [flowcontrol/v1beta3]
FlowSchema [flowcontrol/v1beta2]
HorizontalPodAutoscaler [autoscaling/v2]
HorizontalPodAutoscaler [autoscaling/v1]
Ingress [networking/v1]
IngressClass [networking/v1]
Job [batch/v1]
JobTemplateSpec [batch/v1]
Lease [coordination/v1]
LimitRange [core/v1]
LocalSubjectAccessReview [authorization/v1]
MutatingWebhookConfiguration [admissionregistration/v1]
Namespace [core/v1]
NetworkPolicy [networking/v1]
Node [core/v1]
PersistentVolume [core/v1]
PersistentVolumeClaim [core/v1]
PersistentVolumeClaimTemplate [core/v1]
Pod [core/v1]
PodDisruptionBudget [policy/v1]
PodScheduling [resource/v1alpha1]
PodTemplate [core/v1]
PodTemplateSpec [core/v1]
PriorityClass [scheduling/v1]
PriorityLevelConfiguration [flowcontrol/v1beta3]
PriorityLevelConfiguration [flowcontrol/v1beta2]
ReplicaSet [apps/v1]
ReplicationController [core/v1]
ResourceClaim [resource/v1alpha1]
ResourceClaimTemplate [resource/v1alpha1]
ResourceClaimTemplateSpec [resource/v1alpha1]
ResourceClass [resource/v1alpha1]
ResourceQuota [core/v1]
Role [rbac/v1]
RoleBinding [rbac/v1]
RuntimeClass [node/v1]
Scale [autoscaling/v1]
Secret [core/v1]
SelfSubjectAccessReview [authorization/v1]
SelfSubjectReview [authentication/v1alpha1]
SelfSubjectRulesReview [authorization/v1]
Service [core/v1]
ServiceAccount [core/v1]
StatefulSet [apps/v1]
StorageClass [storage/v1]
StorageVersion [apiserverinternal/v1alpha1]
SubjectAccessReview [authorization/v1]
TokenRequest [authentication/v1]
TokenReview [authentication/v1]
ValidatingAdmissionPolicy [admissionregistration/v1alpha1]
ValidatingAdmissionPolicyBinding [admissionregistration/v1alpha1]
ValidatingWebhookConfiguration [admissionregistration/v1]
VolumeAttachment [storage/v1]

https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#objectmeta-v1-meta
labels
object	   Map of string keys and values that can be used to organize and categorize (scope and select) objects.
             May match selectors of replication controllers and services. More info: http://kubernetes.io/docs/user-guide/labels
https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
PodSpec v1 core

https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#container-v1-core

image
string	  Container image name. More info: https://kubernetes.io/docs/concepts/containers/images This field is optional to allow higher level 
          config management to default or override container images in workload controllers like Deployments and StatefulSets.

https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#container-v1-core

ports
ContainerPort array
patch strategy: merge
patch merge key: containerPort	List of ports to expose from the container. Not specifying a port here DOES NOT prevent that port from being exposed. 
Any port which is listening on the default "0.0.0.0" address inside a container will be accessible from the network. Modifying this array with strategic 
merge patch may corrupt the data. For more information See https://github.com/kubernetes/kubernetes/issues/108255. Cannot be updated.

https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#containerport-v1-core

Field                	Description
containerPort
integer	            Number of port to expose on the pod's IP address. This must be a valid port number, 0 < x < 65536.

hostIP
string	           What host IP to bind the external port to.

hostPort
integer              	Number of port to expose on the host. If specified, this must be a valid port number, 0 < x < 65536. If HostNetwork is specified, this must match ContainerPort. Most containers do not need this.

name
string	             If specified, this must be an IANA_SVC_NAME and unique within the pod. Each named port in a pod must have a unique name. Name for the port that can be referred to by services.

protocol
string              	Protocol for port. Must be UDP, TCP, or SCTP. Defaults to "TCP".


======
https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#pod-v1-core

nodeSelector
object	NodeSelector is a selector which must be true for the pod to fit on a node. Selector which must match a node's labels for the pod to be scheduled on that node. More info: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/

=======================================================
https://kubernetes.io/docs/reference/

https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#replicationcontroller-v1-core

Group	   Version 	    Kind
core     	v1         	ReplicationController

apiVersion: v1
kind:  Pod
metadata:
  name: freddis
spec:
  containers:
  -   image: redis
      name:  redis
  nodeSelector: 
      node: "worker2"         ### label  

hai@master:~$ kubectl get ns
NAME              STATUS   AGE
anjireddy         Active   2d13h
default           Active   20d
kube-flannel      Active   20d
kube-node-lease   Active   20d
kube-public       Active   20d
kube-system       Active   20d
metallb-system    Active   20d
velpula           Active   2d13h
venkata           Active   2d13h
hai@master:~$ kubectl delete namespace anjireddy
namespace "anjireddy" deleted
hai@master:~$ kubectl delete namespace anjireddy   velpula  venkata
namespace "velpula" deleted
namespace "venkata" deleted
Error from server (NotFound): namespaces "anjireddy" not found

hai@master:~$ kubectl create namespace project1 
namespace/project1 created

hai@master:~$ kubectl create namespace project2
namespace/project2 created

hai@master:~$ kubectl get namespaces 
NAME              STATUS   AGE
default           Active   20d
kube-flannel      Active   20d
kube-node-lease   Active   20d
kube-public       Active   20d
kube-system       Active   20d
metallb-system    Active   20d
project1          Active   48s
project2          Active   42s

hai@master:~$ kubectl get pods  -n  project1   -o wide 
NAME        READY   STATUS    RESTARTS   AGE     IP             NODE     NOMINATED NODE   READINESS GATES
appserver   1/1     Running   0          2m13s   10.244.3.194   worker   <none>           <none>
dbserver    1/1     Running   0          92s     10.244.3.195   worker   <none>           <none>
webserver   1/1     Running   0          43s     10.244.3.196   worker   <none>           <none>

hai@master:~$ kubectl label node worker-2  node=w2
node/worker-2 labeled
hai@master:~$ kubectl describe node worker-2 
Name:               worker-2
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=worker-2
                    kubernetes.io/os=linux
                    node=w2
hai@master:~$ kubectl get nodes  --show-labels  -o  wide 
NAME       STATUS   ROLES           AGE     VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME     LABELS
master     Ready    control-plane   20d     v1.25.4   192.168.68.138   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.12   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=master,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node.kubernetes.io/exclude-from-external-load-balancers=
worker     Ready    <none>          3d23h   v1.25.4   192.168.68.145   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.10   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=worker,kubernetes.io/os=linux,node=w1
worker-2   Ready    <none>          33m     v1.25.4   192.168.68.144   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.13   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=worker-2,kubernetes.io/os=linux,node=w2
-#####################################################################################################################################
VIDEO  - 8  
Kubernetes-Day-8
https://www.youtube.com/watch?v=uUIobDBLw6o

root@master:~# cd /etc/kubernetes/                   
root@master:/etc/kubernetes# ll
total 56
drwxr-xr-x   5 root root  4096 Dec 13 09:09 ./
drwxr-xr-x 141 root root 12288 Dec 17 19:18 ../
-rwxrwxrwx   1 root root  5638 Dec 13 09:10 admin.conf*
-rw-------   1 root root  5674 Nov 27 07:35 controller-manager.conf
-rw-------   1 root root  1962 Nov 27 07:36 kubelet.conf
drwxr-xr-x   2 root root  4096 Nov 27 07:35 manifests/
drwxr-xr-x   3 root root  4096 Nov 27 07:35 pki/
-rw-------   1 root root  5622 Nov 27 07:35 scheduler.conf
drwx------   4 root root  4096 Dec 13 09:10 tmp/
root@master:/etc/kubernetes# cd manifests/                           ##  ok  best  importent "
root@master:/etc/kubernetes/manifests# ll
total 24
drwxr-xr-x 2 root root 4096 Nov 27 07:35 ./
drwxr-xr-x 5 root root 4096 Dec 13 09:09 ../
-rw------- 1 root root 2382 Nov 27 07:35 etcd.yaml
-rw------- 1 root root 4019 Nov 27 07:35 kube-apiserver.yaml
-rw------- 1 root root 3520 Nov 27 07:35 kube-controller-manager.yaml
-rw------- 1 root root 1440 Nov 27 07:35 kube-scheduler.yaml
root@master:/etc/kubernetes/manifests# 

BEFORE  BEFORE
root@master:/etc/kubernetes/manifests# cat  kube-apiserver.yaml 
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 192.168.68.138:6443
  creationTimestamp: null
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=192.168.68.138
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --enable-admission-plugins=NodeRestriction                    ##- target changed 
    - --enable-bootstrap-token-auth=true

++==### after  after after +++++++++++====================

root@master:/etc/kubernetes/manifests# cat  kube-apiserver.yaml 
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 192.168.68.138:6443
  creationTimestamp: null
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=192.168.68.138
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --enable-admission-plugins=NodeRestriction,PodNodeSelector       ==##  target changed
        - --enable-bootstrap-token-auth=true
ERRORS  RESULTS-- ::
hai@master:~$ kubectl get nodes -o  wide 
NAME       STATUS     ROLES           AGE    VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
master     Ready      control-plane   20d    v1.25.4   192.168.68.138   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.12
worker     Ready      <none>          4d1h   v1.25.4   192.168.68.145   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.10
worker-2   NotReady   <none>          172m   v1.25.4   192.168.68.144   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.13

hai@master:~$ kubectl get nodes -o  wide                              ##"
The connection to the server 192.168.68.138:6443 was refused - did you specify the right host or port?
hai@master:~$ kubectl get nodes -o  wide 
The connection to the server 192.168.68.138:6443 was refused - did you specify the right host or port?
hai@master:~$ kubectl get nodes -o  wide 
The connection to the server 192.168.68.138:6443 was refused - did you specify the right host or port?
hai@master:~$ kubectl get nodes -o  wide 
The connection to the server 192.168.68.138:6443 was refused - did you specify the right host or port?
hai@master:~$ kubectl get nodes -o  wide 
The connection to the server 192.168.68.138:6443 was refused - did you specify the right host or port?
hai@master:~$ kubectl get nodes -o  wide 
The connection to the server 192.168.68.138:6443 was refused - did you specify the right host or port?
hai@master:~$ kubectl get nodes -o  wide 
The connection to the server 192.168.68.138:6443 was refused - did you specify the right host or port?
hai@master:~$ kubectl get nodes -o  wide 
The connection to the server 192.168.68.138:6443 was refused - did you specify the right host or port?

hai@master:~$ kubectl get nodes -o  wide    #"- 
NAME       STATUS     ROLES           AGE    VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME  
master     Ready      control-plane   20d    v1.25.4   192.168.68.138   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.12
worker     Ready      <none>          4d1h   v1.25.4   192.168.68.145   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.10
worker-2   Ready      <none>          174m   v1.25.4   192.168.68.144   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.13
===========================================================================================================
labels  labels   binding  

"##"  before  ---- #####
apiVersion: v1
kind: Namespace
metadata:
  creationTimestamp: "2022-12-18T04:43:22Z"
  labels:
    kubernetes.io/metadata.name: project1
  name: project1
  resourceVersion: "392419"
  uid: 8f636166-d3fe-4b7d-8d66-81ea37653c33
spec:
  finalizers:
  - kubernetes
status:
  phase: Active

====#"after AFTER ==============
apiVersion: v1
kind: Namespace
metadata:
  annotations:
    scheduler.alpha.kuberntetes.io/node-selector:  "node=w1"        ####  watch objerve 
  creationTimestamp: "2022-12-18T04:43:22Z"
  labels:
    kubernetes.io/metadata.name: project1
  name: project1
  resourceVersion: "392419"
  uid: 8f636166-d3fe-4b7d-8d66-81ea37653c33
spec:
  finalizers:
  - kubernetes
status:
  phase: Active

https://www.guru99.com/the-vi-editor.html
VI Editing commands
i â€“ Insert at cursor (goes into insert mode)
a â€“ Write after cursor (goes into insert mode)
A â€“ Write at the end of line (goes into insert mode)
ESC â€“ Terminate insert mode
u â€“ Undo last change
U â€“ Undo all changes to the entire line
o â€“ Open a new line (goes into insert mode)
dd â€“ Delete line
3dd â€“ Delete 3 lines.
D â€“ Delete contents of line after the cursor
C â€“ Delete contents of a line after the cursor and insert new text. Press ESC key to end insertion.
dw â€“ Delete word
4dw â€“ Delete 4 words
cw â€“ Change word
x â€“ Delete character at the cursor
r â€“ Replace character
R â€“ Overwrite characters from cursor onward
s â€“ Substitute one character under cursor continue to insert
S â€“ Substitute entire line and begin to insert at the beginning of the line
~ â€“ Change case of individual character





Commands and their Description   VI  EDITOR LINUX 

k : Moves the cursor up one line.
j : Moves the cursor down one line.
h : Moves the cursor to the left one character position.
l : Moves the cursor to the right one character position.
0 or | : Positions cursor at beginning of line.
$ : Positions cursor at end of line.
W : Positions cursor to the next word.
B : Positions cursor to previous word.
( : Positions cursor to beginning of current sentence.
) : Positions cursor to beginning of next sentence.
H : Move to top of screen.
nH : Moves to nth line from the top of the screen.
M : Move to middle of screen.
L : Move to bottom of screen.
nL : Moves to nth line from the bottom of the screen.
colon along with x : Colon followed by a number would position the cursor on line number represented by x.

CTRL+d : Move forward 1/2 screen.
CTRL+f : Move forward one full screen.
CTRL+u : Move backward 1/2 screen.
CTRL+b : Move backward one full screen.
CTRL+e : Moves screen up one line.
CTRL+y : Moves screen down one line.
CTRL+u : Moves screen up 1/2 page.
CTRL+d : Moves screen down 1/2 page.
CTRL+b : Moves screen up one page.
CTRL+f : Moves screen down one page.
CTRL+I : Redraws screen.

Editing and inserting in Files(Entering and Replacing Text): To edit the file, we need to be in the insert mode. There are many ways to enter insert mode from the command mode

i : Inserts text before current cursor location.
I : Inserts text at beginning of current line.
a : Inserts text after current cursor location.
A : Inserts text at end of current line.
o : Creates a new line for text entry below cursor location.
O : Creates a new line for text entry above cursor location.
r : Replace single character under the cursor with the next character typed.
R : Replaces text from the cursor to right.
s : Replaces single character under the cursor with any number of characters.
S :Replaces entire line.

Deleting Characters: Here is the list of important commands which can be used to delete characters and lines in an opened file.

X Uppercase: Deletes the character before the cursor location.
x Lowercase : Deletes the character at the cursor location.
Dw : Deletes from the current cursor location to the next word.
d^ : Deletes from current cursor position to the beginning of the line.
d$ : Deletes from current cursor position to the end of the line.
Dd : Deletes the line the cursor is on.


Copy and Past Commands: Copy lines or words from one place and paste them on another place by using the following commands.

Yy : Copies the current line.
9yy : Yank current line and 9 lines below.
p : Puts the copied text after the cursor.
P : Puts the yanked text before the cursor.


Save and Exit Commands of the ex Mode : Need to press [Esc] key followed by the colon (:) before typing the following commands:

q : Quit
q! : Quit without saving changes i.e. discard changes.
r fileName : Read data from file called fileName.
wq : Write and quit (save and exit).
w fileName : Write to file called fileName (save as).
w! fileName : Overwrite to file called fileName (save as forcefully).
!cmd : Runs shell commands and returns to Command mode.

Searching and Replacing in (ex Mode): vi also has powerful search and replace capabilities. The formal syntax for searching is:

:s/string 
For example, suppose we want to search some text for the string â€œgeeksforgeeksâ€ Type the following and press ENTER:

:s/geeksforgeeks

===================================================================================
,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=worker2,kubernetes.io/os=linux,node=w2

####-==========  BEFORE   BEGORE ####################"##########

root@master:/etc/kubernetes/manifests# ll
total 24
drwxrwxrwx 2 root root 4096 Dec 18 09:12 ./
drwxr-xr-x 4 root root 4096 Dec 18 09:12 ../
-rw------- 1 root root 2382 Dec 18 09:12 etcd.yaml
-rw------- 1 root root 4019 Dec 18 09:12 kube-apiserver.yaml
-rw------- 1 root root 3520 Dec 18 09:12 kube-controller-manager.yaml
-rw------- 1 root root 1440 Dec 18 09:12 kube-scheduler.yaml
root@master:/etc/kubernetes/manifests# cat kube-apiserver.yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 192.168.68.150:6443
  creationTimestamp: null
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=192.168.68.150
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --enable-admission-plugins=NodeRestriction       ##  "best  ok   modifications 
    - --enable-bootstrap-token-auth=true


AFTER  ##########################======== AFTER
root@master:/etc/kubernetes/manifests# cat kube-apiserver.yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 192.168.68.150:6443
  creationTimestamp: null
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=192.168.68.150
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --enable-admission-plugins=NodeRestriction,PodNodeSelector   "modify "

######### before - ######################""
kubectl edit namespace  project1

apiVersion: v1
kind: Namespace
metadata:
  creationTimestamp: "2022-12-18T04:43:28Z"       " modify   "
  labels:
    kubernetes.io/metadata.name: project2
  name: project2
  resourceVersion: "392432"
  uid: bc6851bd-3ef2-42ba-bb63-8455cf5d566e
spec:
  finalizers:
  - kubernetes
status:
  phase: Active
~                
@@@@@@   AFTER -  AFTER  #######
apiVersion: v1
kind: Namespace
metadata:
   annotations:
      scheduler.alpha.kuberntes.io/node-selector:  "node=w2"       " modify  ""
  creationTimestamp: "2022-12-18T04:43:28Z"
  labels:
    kubernetes.io/metadata.name: project2
  name: project2
  resourceVersion: "392432"
  uid: bc6851bd-3ef2-42ba-bb63-8455cf5d566e
spec:
  finalizers:
  - kubernetes
status:
  phase: Active
~                
===""#########################"#########################################################"

hai@master:~$ kubectl  get nodes --show-labels
NAME      STATUS   ROLES           AGE     VERSION   LABELS
master    Ready    control-plane   17h     v1.25.4   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=master,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node.kubernetes.io/exclude-from-external-load-balancers=
worker1   Ready    <none>          3h4m    v1.25.4   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=worker1,kubernetes.io/os=linux,node=w1
worker2   Ready    <none>          4h10m   v1.25.4   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=worker2,kubernetes.io/os=linux,node=w2
hai@master:~$

hai@master:~$ kubectl get pods -n  project1  -o  wide
NAME        READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES
appserver   1/1     Running   0          2m48s   10.44.0.1   worker1   <none>           <none>
dbserver    1/1     Running   0          2m29s   10.44.0.2   worker1   <none>           <none>
webserver   1/1     Running   0          2m38s   10.36.0.1   worker1   <none>           <none>
hai@master:~$ kubectl get pods -n  project2  -o  wide
NAME   READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES
dev    1/1     Running   0          2m10s   10.44.0.3   worker2   <none>           <none>
prod   1/1     Running   0          2m18s   10.36.0.2   worker2  <none>           <none>
qa     1/1     Running   0          107s    10.36.0.3   worker2   <none>           <none>
hai@master:~$

###############################="############--============= remove  steps 

root@master:/etc/kubernetes/manifests# cat  kube-apiserver.yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 192.168.68.150:6443
  creationTimestamp: null
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=192.168.68.150
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --enable-admission-plugins=NodeRestriction     == "modify"

---------------------------------

hai@master:~$ kubectl edit namespace project1
namespace/project1 edited
hai@master:~$ kubectl edit namespace project1
# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: v1
kind: Namespace
metadata:                                       "modify "
  creationTimestamp: "2022-12-19T07:47:36Z"
  labels:
    kubernetes.io/metadata.name: project1
  name: project1
  resourceVersion: "35515"
  uid: 68d5d913-282a-48ea-a0e1-e32c10325d6d
spec:
  finalizers:
  - kubernetes
status:
  phase: Active
~

------------
hai@master:~$ kubectl edit namespace project2
# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: v1
kind: Namespace
metadata:
  annotations:                                           "delete tw0 lines
    scheduler.alpha.kuberntes.io/node-selector: node=w2
  creationTimestamp: "2022-12-19T07:47:41Z"
  labels:
    kubernetes.io/metadata.name: project2
  name: project2
  resourceVersion: "22268"
  uid: 9de248ba-35c0-43d2-8f3d-bd9aec56ebf4
spec:
  finalizers:
  - kubernetes
status:
  phase: Active
~
============================================================================================ "delete lable

hai@master:~$ kubectl label node worker1 node-
node/worker1 unlabeled
hai@master:~$ kubectl label node worker2  node-
node/worker2 unlabeled
hai@master:~$

###############################################################################################################################################
https://www.youtube.com/watch?v=9JMcGr6rbO8
Kubernetes-Day-9

Resource Quotas
When several users or teams share a cluster with a fixed number of nodes, there is a concern that one team could use more than its fair share of resources.

Resource quotas are a tool for administrators to address this concern.

A resource quota, defined by a ResourceQuota object, provides constraints that limit aggregate resource consumption per namespace. It can limit the quantity of objects that can be created in a namespace by type, as well as the total amount of compute resources that may be consumed by resources in that namespace.

Resource quotas work like this:

Different teams work in different namespaces. This can be enforced with RBAC.

The administrator creates one ResourceQuota for each namespace.

Users create resources (pods, services, etc.) in the namespace, and the quota system tracks usage to ensure it does not exceed hard resource limits defined in a ResourceQuota.

If creating or updating a resource violates a quota constraint, the request will fail with HTTP status code 403 FORBIDDEN with a message explaining the constraint that would have been violated.

If quota is enabled in a namespace for compute resources like cpu and memory, users must specify requests or limits for those values; otherwise, the quota system may reject pod creation. Hint: Use the LimitRanger admission controller to force defaults for pods that make no compute resource requirements.


Object Count Quota
You can set quota for the total number of certain resources of all standard, namespaced resource types using the following syntax:

count/persistentvolumeclaims
count/services
count/secrets
count/configmaps
count/replicationcontrollers
count/deployments.apps
count/replicasets.apps
count/statefulsets.apps
count/jobs.batch
count/cronjobs.batch
=====================
hai@master:~$ kubectl get ns
NAME              STATUS   AGE
default           Active   20h
kube-node-lease   Active   20h
kube-public       Active   20h
kube-system       Active   20h
project1          Active   5h45m
project2          Active   5h45m
hai@master:~$ kubectl describe namespace project1
Name:         project1
Labels:       kubernetes.io/metadata.name=project1
Annotations:  <none>
Status:       Active
No resource quota.      ##  ## see look  no   resource quota 
No LimitRange resource.

hai@master:~$ kubectl describe namespace project2
Name:         project2
Labels:       kubernetes.io/metadata.name=project2
Annotations:  <none>
Status:       Active
No resource quota.      ## see look  no   resource quota 
No LimitRange resource.

====================================
https://kubernetes.io/docs/reference/
https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#resourcequota-v1-core

ResourceQuota v1 core
Group  	Version	   Kind
core	   v1	       ResourceQuota

Field	Description
apiVersion
string	          APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources

kind
string	          Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds

metadata
ObjectMeta	      Standard object's metadata. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata

spec
ResourceQuotaSpec	  Spec defines the desired quota. https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

status
ResourceQuotaStatus   	Status defines the actual enforced quota and its current usage. https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#objectmeta-v1-meta

name
string	    Name must be unique within a namespace. Is required when creating resources, although some resources may allow a client to request 
             the generation of an appropriate name automatically. Name is primarily intended for creation idempotence and configuration definition. Cannot be updated.
            More info: http://kubernetes.io/docs/user-guide/identifiers#names


namespace
string	  Namespace defines the space within which each name must be unique. An empty namespace is equivalent to the "default" namespace, but "default" is the canonical representation. 
          Not all objects are required to be scoped to a namespace - the value of this field for those objects will be empty. Must be a DNS_LABEL. Cannot be updated. 
           More info: http://kubernetes.io/docs/user-guide/namespaces

https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#resourcequotaspec-v1-core

ResourceQuotaSpec v1 core
ResourceQuota [core/v1]

hard
object	     hard is the set of desired hard limits for each named resource. More info: https://kubernetes.io/docs/concepts/policy/resource-quotas/

scopeSelector
ScopeSelector    	scopeSelector is also a collection of filters like scopes that must match each object tracked by a quota but expressed using ScopeSelectorOperator in 
                  combination with possible values. For a resource to match, both scopes AND scopeSelector (if specified in spec), must be matched.

scopes
string             array	A collection of filters that must match each object tracked by a quota. If not specified, the quota matches all objects.

https://unofficial-kubernetes.readthedocs.io/en/latest/concepts/policy/resource-quotas/

apiVersion: v1
kind: ResourceQuota
metadata:
   name: cpuramhdd
   namespace: project1
spec:
  hard:
    pods:  "2"
    configmaps: "10"
    secrets: "9"
    services: "12"
    persistentvolumeclaims: "5"
====== " ""

hai@master:~/test$ kubectl apply -f requata.yaml
resourcequota/cpuramhdd created

hai@master:~/test$ kubectl get quota  -n project1
NAME        AGE   REQUEST                                                                                  LIMIT
cpuramhdd   42s   configmaps: 1/10, persistentvolumeclaims: 0/5, pods: 3/2, secrets: 0/9, services: 0/12  

hai@master:~/test$ kubectl describe  quota -n  project1
Name:                   cpuramhdd
Namespace:              project1
Resource                Used  Hard
--------                ----  ----
configmaps              1     10
persistentvolumeclaims  0     5
pods                    0     2     ## look see
secrets                 0     9
services                0     12
========="e

hai@master:~/test$ kubectl run sony   --image=nginx  -n  project1
pod/sony created
hai@master:~/test$ kubectl run dell   --image=nginx  -n  project1
pod/dell created
hai@master:~/test$ kubectl describe quota -n  project1
Name:                   cpuramhdd
Namespace:              project1
Resource                Used  Hard
--------                ----  ----
configmaps              1     10
persistentvolumeclaims  0     5
pods                    2     2     # look  see 
secrets                 0     9
services                0     12
--------

hai@master:~/test$ kubectl run hp    --image=nginx  -n  project1

Error from server (Forbidden): pods "hp" is forbidden: exceeded quota: cpuramhdd, requested: pods=1, used: pods=2, limited: pods=2
============================="""
apiVersion: v1
kind: ResourceQuota
metadata:
   name: cpuramhdd
   namespace: project1
spec:
  hard:
    pods:  "2"
    configmaps: "10"
    secrets: "9"
    services: "12"
    persistentvolumeclaims: "5"
    limits.memory:  "800Mi"
    limits.cpu:  "5"

hai@master:~/test$ kubectl apply  -f requata.yaml
resourcequota/cpuramhdd configured

hai@master:~/test$ kubectl describe quota  -n project1
Name:                   cpuramhdd
Namespace:              project1
Resource                Used  Hard
--------                ----  ----
configmaps              1     10
limits.cpu              0     5
limits.memory           0     800Mi
persistentvolumeclaims  0     5
pods                    2     2
secrets                 0     9
services                0     12
hai@master:~/test$
###"""#########################################################################

Kubernetes-Day-10
https://www.youtube.com/watch?v=mGQ29xYeCrM

hai@master:~/test$ kubectl run devserver  --image=nginx  -n   project1
Error from server (Forbidden): pods "devserver" is forbidden: failed quota: cpuramhdd: must specify limits.cpu for: devserver; limits.memory for: devserver
hai@master:~/test$

apiVersion: v1
kind: Pod
metadata:
   name: quotapod
   namespace: project1
spec:
  containers:
    -  image: nginx
       name: quotapod
       resources:
          limits:  
             memory: "120Mi"
             cpu: "0.2"       
https://kubernetes.io/docs/reference/

hai@master:~$ kubectl explain pod
KIND:     Pod
VERSION:  v1

DESCRIPTION:
     Pod is a collection of containers that can run on a host. This resource is
     created by clients and scheduled onto hosts.

FIELDS:
   apiVersion   <string>
     APIVersion defines the versioned schema of this representation of an
     object. Servers should convert recognized schemas to the latest internal
     value, and may reject unrecognized values. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources

   kind <string>
     Kind is a string value representing the REST resource this object
     represents. Servers may infer this from the endpoint the client submits
     requests to. Cannot be updated. In CamelCase. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds

   metadata     <Object>
     Standard object's metadata. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata

   spec <Object>
     Specification of the desired behavior of the pod. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

   status       <Object>
     Most recently observed status of the pod. This data may not be up to date.
     Populated by the system. Read-only. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

hai@master:~$ kubectl explain pod --recursive | less

KIND:     Pod
VERSION:  v1

DESCRIPTION:
     Pod is a collection of containers that can run on a host. This resource is
     created by clients and scheduled onto hosts.

FIELDS:
   apiVersion   <string>
   kind <string>
   metadata     <Object>
      annotations       <map[string]string>
      creationTimestamp <string>
      deletionGracePeriodSeconds        <integer>
      deletionTimestamp <string>
      finalizers        <[]string>
      generateName      <string>
      generation        <integer>
      labels    <map[string]string>
      managedFields     <[]Object>
         apiVersion     <string>
         fieldsType     <string>
         fieldsV1       <map[string]>

===================



hai@master:~$ kubectl explain pod --recursive | grep -i  container
     Pod is a collection of containers that can run on a host. This resource is
      containers        <[]Object>
                  containerName <string>
            containerPort       <integer>
      ephemeralContainers       <[]Object>
                  containerName <string>
            containerPort       <integer>
         targetContainerName    <string>
      initContainers    <[]Object>
                  containerName <string>
            containerPort       <integer>
                  containerName <string>
                        containerName   <string>
      containerStatuses <[]Object>
         containerID    <string>
               containerID      <string>
               containerID      <string>
      ephemeralContainerStatuses        <[]Object>
         containerID    <string>
               containerID      <string>
               containerID      <string>
      initContainerStatuses     <[]Object>
         containerID    <string>
               containerID      <string>
               containerID      <string>
#########################"""
hai@master:~$ kubectl get quota -n  project1
NAME        AGE    REQUEST                                                                                  LIMIT
cpuramhdd   142m   configmaps: 1/10, persistentvolumeclaims: 0/5, pods: 2/2, secrets: 0/9, services: 0/12   limits.cpu: 0 /5, limits.memory: 0/800Misources.

hai@master:~$ kubectl describe quota -n project1
Name:                   cpuramhdd
Namespace:              project1
Resource                Used  Hard
--------                ----  ----
configmaps              1     10
limits.cpu              0     5
limits.memory           0     800Mi
persistentvolumeclaims  0     5
pods                    2     2
secrets                 0     9
services                0     12

hai@master:~$ kubectl apply -f test/podcpu.yaml   -n project1
pod/quotapod created
--------
apiVersion: v1
kind: Pod
metadata:
   name: quotapod
   namespace: project1
spec:
  containers:
    -  image: nginx
       name: quotapod
       resources:
          limits:  
             memory: "120Mi"
             cpu: "05"       
----"
hai@master:~$ kubectl describe quota -n project1
Name:                   cpuramhdd
Namespace:              project1
Resource                Used   Hard
--------                ----   ----
configmaps              1      10
limits.cpu              200m   5
limits.memory           120Mi  800Mi
persistentvolumeclaims  0      5
pods                    1      2
secrets                 0      9
services                0      12
======================================================== """
apiVersion: v1
kind: Pod
metadata:
   name: quotapod1
   namespace: project1
spec:
  containers:
    -  image: nginx
       name: quotapod1
       resources:
          limits:
             memory: "700Mi"
             cpu: "0.2"
====="
hai@master:~/test$ kubectl apply -f podcpu.yaml  -n  project1
Error from server (Forbidden): error when creating "podcpu.yaml": pods "quotapod1" is forbidden: exceeded quota: cpuramhdd, requested: limits.memory=700Mi, used: limits.memory=120Mi, limited: limits.memory=800Mi
-----------------++++++++++++++++++++++++++++====================="""
What is the meaning of CPU and core in Kubernetes?
https://stackoverflow.com/questions/53255956/what-is-the-meaning-of-cpu-and-core-in-kubernetes

If I want to set the CPU request/limit for pod, should the maximum resource that I have be 1000m or 4000m?

1000m (milicores) = 1 core = 1 vCPU = 1 AWS vCPU = 1 GCP Core.
100m (milicores) = 0.1 core = 0.1 vCPU = 0.1 AWS vCPU = 0.1 GCP Core.

8000m = 8 cores = 8 vCPUs
1 CPU = 1000 millicores/millicpu

https://franciscomelojr.ca/2021/11/24/milli-cores-and-cpu-metrics/
spec:
  containers:
  - name: app
    image: images.my-company.example/app:v4
    resources:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"

https://www.alibabacloud.com/blog/kubernetes-assign-cpu-resource-defaults-and-limits-to-containers_594832

1, 2, 2.5 ... defines 1, 2 and 2.5 CPUs
1000m, 2500m, 150m ... defines 1 CPU, 2.5 CPUs and 0.150 CPUs
The second syntax uses Millicores. 1000m equals one CPU on all computers.
( One Millicores is 1/1000 of a CPU, therefore 1000m equals 1 CPU )
A four core server has a CPU capacity of 4000m.

apiVersion: v1
kind: Pod
metadata:
  name: mybench-pod
spec:
  containers:
  - name: mybench-container
    image: mytutorials/centos:bench
    imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'echo The CPU Bench Pod is Running ; sleep 3600']
        resources:
      limits:
        cpu: "1"
      requests:
        cpu: 500m
        
  restartPolicy: Never
-------
apiVersion: v1
kind: LimitRange
metadata:
  name: mycpu-limit-range
spec:
  limits:
  - default:
      cpu: 0.75
    defaultRequest:
      cpu: 0.25
    type: Container

--------------------
apiVersion: v1
kind: LimitRange
metadata:
  name: mycpu-limit-range
spec:
  limits:
  - default:
      cpu: 0.75
    defaultRequest:
      cpu: 0.25

    max:
      cpu: "2000m"
    min:
      cpu: "200m"

    type: Container
--------------------
apiVersion: v1
kind: Pod
metadata:
  name: mybench-pod
spec:
  containers:
  - name: mybench-container
    image: mytutorials/centos:bench
    imagePullPolicy: IfNotPresent
    
    command: ['sh', '-c', 'echo The CPU Bench Pod is Running ; sleep 3600']
    
    resources:
      limits:
        cpu: "1000m"
      requests:
        cpu: 300m
        
  restartPolicy: Never    
=================================================================== """ #################################
LimitRange v1 core
Group    	Version	  Kind
core	    v1	      LimitRange

https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/


1.   https://kubernetes.io/examples/admin/resource/limit-mem-cpu-container.yaml          ## yaml file downloded

2.   https://kubernetes.io/examples/admin/resource/limit-range-pod-1.yaml               ## yaml file downloded

3.  https://kubernetes.io/examples/admin/resource/limit-mem-cpu-pod.yaml               ## yaml file downloded


4.  https://kubernetes.io/examples/admin/resource/limit-range-pod-2.yaml                 ## yaml file downloded

--------'"""

hai@master:~$ cat limit-mem-cpu-container.yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: limit-mem-cpu-per-container
spec:
  limits:
  - max:
      cpu: "800m"
      memory: "1Gi"
    min:
      cpu: "100m"
      memory: "99Mi"
    default:
      cpu: "700m"
      memory: "900Mi"
    defaultRequest:
      cpu: "110m"
      memory: "111Mi"
    type: Container

---------------
https://kubernetes.io/docs/reference/

https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#limitrange-v1-core

spec
LimitRangeSpec  	Spec defines the limits enforced. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#limitrangespec-v1-core
LimitRangeList v1 core
Field	Description
apiVersion
string	APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources

items
LimitRange array	Items is a list of LimitRange objects. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/

kind
string	Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds

metadata
ListMeta	Standard list metadata. More info: https://git.k8s.i
---
https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#limitrangespec-v1-core

Field	Description
limits
LimitRangeItem array	  Limits is the list of LimitRangeItem objects that are enforced.

https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#limitrangeitem-v1-core
Field	Description
default
object  	Default resource requirement limit value by resource name if resource limit is omitted.

defaultRequest
object	  DefaultRequest is the default resource requirement request value by resource name if resource request is omitted.

max
object	  Max usage constraints on this kind by resource name.

maxLimitRequestRatio
object	   MaxLimitRequestRatio if specified, the named resource must have a request and limit that are both non-zero where limit divided by request is less than or equal to the enumerated value; this represents the max burst for the named resource.

min
object	  Min usage constraints on this kind by resource name.

type
string    Type of resource that this limit applies to.

==========="""
hai@master:~$ kubectl apply -f limit-
limit-mem-cpu-container.yaml  limit-mem-cpu-pod.yaml        limit-range-pod-1.yaml        limit-range-pod-2.yaml

hai@master:~$ kubectl apply -f limit-mem-cpu-container.yaml
limitrange/limit-mem-cpu-per-container created

hai@master:~$ kubectl get limitrange
NAME                          CREATED AT
limit-mem-cpu-per-container   2022-12-20T06:40:37Z
------
hai@master:~$ kubectl describe limitrange
Name:       limit-mem-cpu-per-container
Namespace:  default
Type        Resource  Min   Max   Default Request  Default Limit  Max Limit/Requ           est Ratio
----        --------  ---   ---   ---------------  -------------  --------------           ---------
Container   cpu       100m  800m  110m             700m           -
Container   memory    99Mi  1Gi   111Mi            900Mi          -
hai@master:~$
=============================================================================

hai@master:~$ cat limit-range-pod-1.yaml
apiVersion: v1
kind: Pod
metadata:
  name: busybox1
spec:
  containers:
  - name: busybox-cnt01
    image: busybox:1.28
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo hello from cnt01; sleep 10;done"]
    resources:
      requests:
        memory: "100Mi"
        cpu: "100m"
      limits:
        memory: "200Mi"
        cpu: "500m"
  - name: busybox-cnt02
    image: busybox:1.28
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo hello from cnt02; sleep 10;done"]
    resources:
      requests:
        memory: "100Mi"
        cpu: "100m"
  - name: busybox-cnt03
    image: busybox:1.28
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo hello from cnt03; sleep 10;done"]
    resources:
      limits:
        memory: "200Mi"
        cpu: "500m"
  - name: busybox-cnt04
    image: busybox:1.28
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo hello from cnt04; sleep 10;done"]
#########  abobe file is  edit ----  
apiVersion: v1
kind: Pod
metadata:
  name: busybox1
spec:
  containers:
  - name: busybox-cnt01
    image: busybox:1.28
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo hello from cnt01; sleep 10;done"]
    resources:
      requests:
        memory: "100Mi"
        cpu: "100m"
      limits:
        memory: "200Mi"
        cpu: "500m"
---
hai@master:~$ kubectl describe limitrange
Name:       limit-mem-cpu-per-container
Namespace:  default
Type        Resource  Min   Max   Default Request  Default Limit  Max Limit/Request Ratio
----        --------  ---   ---   ---------------  -------------  -----------------------
Container   memory    99Mi  1Gi   111Mi            900Mi          -
Container   cpu       100m  800m  110m             700m          

hai@master:~$ kubectl apply -f modifylimitrange.yaml
pod/busybox1 created

hai@master:~$ kubectl get pods -o wide
NAME       READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES
busybox1   1/1     Running   0          3m41s   10.44.0.1   worker2   <none>           <none>

hai@master:~$ kubectl describe pod busybox1
Name:             busybox1
Namespace:        default
Containers:
  busybox-cnt01:
    Container ID:  containerd://b240b323de07c2a598f6a482b2a46888f730f66b42beae38fc5a04fd161bdd16
    Image:         busybox:1.28
    Image ID:      docker.io/library/busybox@sha256:141c253bc4c3fd0a201d32dc1f493bcf3fff003b6df416dea4f41046e0f37d47
    Command:
      /bin/sh        ##  look  
    Args:                      ## look
      -c
      while true; do echo hello from cnt01; sleep 10;done        ## look  
    State:          Running
      Started:      Mon, 19 Dec 2022 23:25:55 -0800
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     500m
      memory:  200Mi
    Requests:
      cpu:        100m
      memory:     100Mi
    Environment:  <none>

Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  5m12s  default-scheduler  Successfully assigned default/busybox1 to worker2
  Normal  Pulling    5m11s  kubelet            Pulling image "busybox:1.28"
  Normal  Pulled     5m4s   kubelet            Successfully pulled image "busybox:1.28" in 6.824356546s
  Normal  Created    5m4s   kubelet            Created container busybox-cnt01
  Normal  Started    5m4s   kubelet            Started container busybox-cnt01

=================================="""
hai@master:~$ kubectl describe limitrange
Name:       limit-mem-cpu-per-container
Namespace:  default
Type        Resource  Min   Max   Default Request  Default Limit  Max Limit/Request Ratio
----        --------  ---   ---   ---------------  -------------  -----------------------
Container   cpu       100m  800m  110m             700m           -
Container   memory    99Mi  1Gi   111Mi            900Mi          -

apiVersion: v1
kind: Pod
metadata:
  name: busybox1
spec:
  containers:
  - name: busybox-cnt01
    image: busybox:1.28
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo hello from cnt01; sleep 10;done"]
    resources:
      requests:
        memory: "100Mi"
        cpu: "100m"
      limits:
        memory: "200Mi"
        cpu: "900m"          ##  look  see  modify 

hai@master:~$ nano modifylimitrange.yaml
hai@master:~$ kubectl apply -f modifylimitrange.yaml
Error from server (Forbidden): error when creating "modifylimitrange.yaml": pods "busybox1" is forbidden: maximum cpu usage per Container is 800m, but limit is 900m

===================================================================="
hai@master:~$ kubectl apply -f limit-
limit-mem-cpu-container.yaml  limit-mem-cpu-pod.yaml        limit-range-pod-1.yaml        limit-range-pod-2.yaml

hai@master:~$ kubectl apply -f limit-mem-cpu-pod.yaml
limitrange/limit-mem-cpu-per-pod created

hai@master:~$ kubectl describe limitrange
Name:       limit-mem-cpu-per-container
Namespace:  default
Type        Resource  Min   Max   Default Request  Default Limit  Max Limit/Request Ratio
----        --------  ---   ---   ---------------  -------------  -----------------------
Container   cpu       100m  800m  110m             700m           -
Container   memory    99Mi  1Gi   111Mi            900Mi          -

Name:       limit-mem-cpu-per-pod
Namespace:  default
Type        Resource  Min  Max  Default Request  Default Limit  Max Limit/Request Ratio
----        --------  ---  ---  ---------------  -------------  -----------------------
Pod         memory    -    2Gi  -                -              -
Pod         cpu       -    2    -                -              -
------==================================================================="""
Name:       limit-mem-cpu-per-pod
Namespace:  default
Type        Resource  Min  Max  Default Request  Default Limit  Max Limit/Request Ratio
----        --------  ---  ---  ---------------  -------------  -----------------------
Pod         cpu       -    2    -                -              -
Pod         memory    -    2Gi  -                -              -

apiVersion: v1
kind: Pod
metadata:
  name: busybox2
spec:
  containers:
  - name: busybox-cnt01
    image: busybox:1.28
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo hello from cnt01; sleep 10;done"]
    resources:
      requests:
        memory: "100Mi"
        cpu: "100m"
      limits:
        memory: "3Gi"
        cpu: "500m"

hai@master:~$ kubectl apply -f 3gbpod.yaml
Error from server (Forbidden): error when creating "3gbpod.yaml": pods "busybox2" is forbidden: maximum memory usage per Container is 1Gi, but limit is 3Gi
======================================================================================="""
Name:       limit-mem-cpu-per-pod
Namespace:  default
Type        Resource  Min  Max  Default Request  Default Limit  Max Limit/Request Ratio
----        --------  ---  ---  ---------------  -------------  -----------------------
Pod         cpu       -    2    -                -              -
Pod         memory    -    2Gi  -                -              -


apiVersion: v1
kind: LimitRange
metadata:
  name: limit-mem-cpu-per-pod
spec:
  limits:
  - max:
      cpu: "2"
      memory: "2Gi"
    min:
      cpu:  "1"
      memory:  "1.3Gi"
    type: Pod
"""
hai@master:~$ nano lmtr2-1.yaml
hai@master:~$ kubectl  apply -f lmtr2-1.yaml
limitrange/limit-mem-cpu-per-pod configured
hai@master:~$ kubectl describe limitrange
Name:       limit-mem-cpu-per-container
Namespace:  default
Type        Resource  Min   Max   Default Request  Default Limit  Max Limit/Request Ratio
----        --------  ---   ---   ---------------  -------------  -----------------------
Container   cpu       100m  800m  110m             700m           -
Container   memory    99Mi  1Gi   111Mi            900Mi          -



hai@master:~$ kubectl describe limitrange
Name:       limit-mem-cpu-per-container
Namespace:  default
Type        Resource  Min   Max   Default Request  Default Limit  Max Limit/Request Ratio
----        --------  ---   ---   ---------------  -------------  -----------------------
Container   cpu       100m  800m  110m             700m           -
Container   memory    99Mi  1Gi   111Mi            900Mi          -


Name:       limit-mem-cpu-per-pod
Namespace:  default
Type        Resource  Min  Max  Default Request  Default Limit  Max Limit/Request Ratio
----        --------  ---  ---  ---------------  -------------  -----------------------
Pod         cpu       -    2    -                -              -
Pod         memory    -    2Gi  -                -              -

----
apiVersion: v1
kind: Pod
metadata:
  name: busybox1
spec:
  containers:
  - name: busybox-cnt01
    image: busybox:1.28
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo hello from cnt01; sleep 10;done"]
    resources:
      requests:                    ### MINIMUM 
        memory: "1000Mi"
        cpu: "1000m"
      limits:
        memory: "1200Mi"                   ## MAXIMUM 
        cpu: "1000m"

hai@master:~$ kubectl apply -f  modifylimitrange.yaml
Error from server (Forbidden): error when creating "modifylimitrange.yaml": pods "busybox1" is forbidden: minimum memory usage per Pod is 1395864371200m, but request is 1048576k
=============================#####################################"""
hai@master:~$ kubectl describe limitrange
Name:       limit-mem-cpu-per-container
Namespace:  default
Type        Resource  Min   Max   Default Request  Default Limit  Max Limit/Request Ratio
----        --------  ---   ---   ---------------  -------------  -----------------------
Container   cpu       100m  800m  110m             700m           -
Container   memory    99Mi  1Gi   111Mi            900Mi          -

Name:       limit-mem-cpu-per-pod
Namespace:  default
Type        Resource  Min  Max  Default Request  Default Limit  Max Limit/Request Ratio
----        --------  ---  ---  ---------------  -------------  -----------------------
Pod         cpu       -    2    -                -              -
Pod         memory    -    2Gi  -                -              -

hai@master:~$ cat  modifylimitrange.yaml
apiVersion: v1
kind: Pod
metadata:
  name: busybox1
spec:
  containers:
  - name: busybox-cnt01
    image: busybox:1.28
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo hello from cnt01; sleep 10;done"]
    resources:
      requests:                               ## MINIMUM LOOK  SEE 
        memory: "100Mi"
        cpu: "100m"
      limits:
        memory: "120Mi"                           ####  MAXIMUM 
        cpu: "100m"

========== --------
hai@master:~$ kubectl get pods -o wide
NAME       READY   STATUS    RESTARTS   AGE    IP          NODE      NOMINATED NODE   READINESS GATES
busybox1   1/1     Running   0          9m2s   10.44.0.1   worker2   <none>           <none>

hai@master:~$ kubectl describe pod busybox1
Name:             busybox1
Namespace:        default
Node:             worker2/192.168.68.151

Containers:
  busybox-cnt01:
    Container ID:  containerd://c93faf1cc3e0ad2b0f48b9d1e3586530e16c8d0bc14caa6ca818b08227f9a2f2
    Image:         busybox:1.28
    Image ID:      docker.io/library/busybox@sha256:141c253bc4c3fd0a201d32dc1f493bcf3fff003b6df416dea4f41046e0f37d47
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/sh
    Args:
      -c
      while true; do echo hello from cnt01; sleep 10;done
    State:          Running
      Started:      Tue, 20 Dec 2022 03:41:11 -0800
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  120Mi
    Requests:
      cpu:        100m
      memory:     100Mi
  ========================================================================"""
  default       default    default              look   see
hai@master:~$ kubectl get pods -o wide
NAME       READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES
busybox1   1/1     Running   0          12m   10.44.0.1   worker2   <none>           <none>
ngidnx     1/1     Running   0          12s   10.36.0.2   worker1   <none>           <none>
nginx      1/1     Running   0          21s   10.36.0.1   worker1   <none>           <none>
nginxd     1/1     Running   0          15s   10.44.0.2   worker2   <none>           <none>

hai@master:~$ kubectl describe pod  nginx
Name:             nginx
Namespace:        default
Priority:         0
Containers:
  nginx:                                "watch look  see
    Limits:
      cpu:     700m                 ## default limits is taken
      memory:  900Mi
    Requests:
      cpu:        110m
      memory:     111Mi
   
hai@master:~$ kubectl describe limitrange
Name:       limit-mem-cpu-per-container
Namespace:  default
Type        Resource  Min   Max   Default Request  Default Limit  Max Limit/Request Ratio
----        --------  ---   ---   ---------------  -------------  -----------------------
Container   cpu       100m  800m  110m             700m           -                           ## default limits is taken
Container   memory    99Mi  1Gi   111Mi            900Mi          -

Name:       limit-mem-cpu-per-pod
Namespace:  default
Type        Resource  Min  Max  Default Request  Default Limit  Max Limit/Request Ratio
----        --------  ---  ---  ---------------  -------------  -----------------------
Pod         cpu       -    2    -                -              -
Pod         memory    -    2Gi  -                -              -
-----------  2nd  pod  ###########################"""

hai@master:~$ kubectl describe pod nginxd
Name:             nginxd
Namespace:        default
Containers:
  nginxd:
    Limits:
      cpu:     700m
      memory:  900Mi
    Requests:
      cpu:        110m
      memory:     111Mi
#############################################################################################################################################

hai@master:~$ cat limit-range-pod-2.yaml
apiVersion: v1
kind: Pod
metadata:
  name: busybox2
spec:
  containers:
  - name: busybox-cnt01
    image: busybox:1.28
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo hello from cnt01; sleep 10;done"]
    resources:
      requests:
        memory: "100Mi"                        ## look see 
        cpu: "100m"
      limits:
        memory: "200Mi"                         ## look see
        cpu: "500m"
  - name: busybox-cnt02
    image: busybox:1.28
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo hello from cnt02; sleep 10;done"]
    resources:
      requests:                 ## look  
        memory: "100Mi"
        cpu: "100m"            %%%  no  limits are  mection  
  - name: busybox-cnt03
    image: busybox:1.28
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo hello from cnt03; sleep 10;done"]
    resources:
      limits:
        memory: "200Mi"       ##  look  see 
        cpu: "500m"
  - name: busybox-cnt04       %% no  menction resource requests 
    image: busybox:1.28
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo hello from cnt04; sleep 10;done"]

ERRORS= : 
hai@master:~$ kubectl apply -f 4cont.yaml
Error from server (Forbidden): error when creating "4cont.yaml": pods "busybox2" is forbidden: [maximum cpu usage per Pod is 2, but limit is 2400m, maximum memory usage per Pod is 2Gi, but limit is 2306867200]
#########################################"""
MODIFY THE ABOVE  CODE  ADD THE MAXIMUM 
apiVersion: v1
kind: Pod
metadata:
  name: busybox2
spec:
  containers:
  - name: busybox-cnt01
    image: busybox:1.28
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo hello from cnt01; sleep 10;done"]
    resources:
      requests:                 ## minimum 
        memory: "100Mi"
        cpu: "100m"
      limits:
        memory: "200Mi"            ## maximum 
        cpu: "500m"
  - name: busybox-cnt02
    image: busybox:1.28
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo hello from cnt02; sleep 10;done"]
    resources:
      limits:
        memory: "100Mi"        ##  look   only  maximum
        cpu: "100m"
hai@master:~$ kubectl apply -f 4cont.yaml
pod/busybox2 created

hai@master:~$ kubectl get pods -o wide
NAME       READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES
busybox2   2/2     Running   0          3m57s   10.44.0.1   worker2   <none>           <none>
           ^^  """ see  look   one pod in tow 2 containers in a single pod 
hai@master:~$ kubectl describe pod  busybox2
Name:             busybox2
Namespace:        default
Priority:         0
Service Account:  default
Node:             worker2/192.168.68.151
Status:           Running
IP:               10.44.0.1
IPs:
  IP:  10.44.0.1
Containers:
  busybox-cnt01:              ## container 1
    Container ID:  containerd://5727cf1e21c36b915fa921e042ec5abfb841926cbdc01d8690274604b2627138
    Image:         busybox:1.28
    Image ID:      docker.io/library/busybox@sha256:141c253bc4c3fd0a201d32dc1f493bcf3fff003b6df416dea4f41046e0f37d47
    Limits:
      cpu:     500m
      memory:  200Mi
    Requests:
      cpu:        100m
      memory:     100Mi
   
  busybox-cnt02:                ## container - 2
    Container ID:  containerd://682fedac9ed1322b98ce15697ac20432b0877e05f7c4749ee933a62a9c863950
    Image:         busybox:1.28
    Image ID:      docker.io/library/busybox@sha256:141c253bc4c3fd0a201d32dc1f493bcf3fff003b6df416dea4f41046e0f37d47
    Limits:
      cpu:     100m
      memory:  100Mi
    Requests:
      cpu:        100m
      memory:     100Mi

#############################################################################"""
https://www.mirantis.com/blog/kubernetes-replication-controller-replica-set-and-deployments-understanding-replication-options/
https://ryaneschinger.com/blog/rolling-updates-kubernetes-replication-controllers-vs-deployments/


                     Replication  or Pod  controller
                kubernetes Replication  type  { OR }  POD Controller type

replication-controller            Replicaset                 Deployments 


1111 .replication controller : - 

replication controller only supports equality-based selectors

The Replication Controller is the original form of replication in Kubernetes

The Replication Controller uses equality-based selectors to manage the pods.

The rolling-update command works with Replication Controllers
Replica Controller is deprecated and replaced by ReplicaSets.

A replication controller continuously monitors the list of running pods by running a reconciliation loop and ensures that the specified number of replicas are always running.
   It maintains the replicas in two ways:

By creating new replicas if the actual replicas are less than the desired replicas, and

By removing extra replicas if the actual replicas are more than the desired replicas. This can be possible if,
A pod is created of the same type manually.

A label of an existing podâ€™s changes to a value which is same as the replication controller label-selector.
Someone decreases the desired number of pods.

Ensures that a specified number of pod replicas are running at any one time.

o"""#################################################################


222.. Replica Set  :-

ReplicaSet is the next-generation ReplicationController that supports the new set-based label selector

ReplicaSets are a higher-level API that gives the ability to easily run multiple instances of a given pod

 rolling-update cannot support 

ReplicaSets Controller uses set-based selectors to manage the pods.

The rolling-update command wonâ€™t work with ReplicaSets.

Deployments are recommended over ReplicaSets.

ReplicaSet is the next-generation ReplicationController that supports the new set-based label selector

Itâ€™s mainly used by Deployment as a mechanism to orchestrate pod creation, deletion and updates.



o"""##############################################################

333.  Deployments -: 

Deployment is a higher-level API object that updates its underlying Replica Sets and their Pods in a similar fashion as kubectl rolling-update.

 rolling-update can support 

â€¢ Deployments can be used for describing the state of a particular application component as a Pod template.

A Deployment provides declarative updates for Pods and Replicaset.

Deployments wrap up Pods and ReplicaSets into a nice package that is capable of deploying your applications.

A deployment is an object in Kubernetes that lets you manage a set of identical pods.

Without a deployment, youâ€™d need to create, update, and delete a bunch of pods manually.

With a deployment, you declare a single object in a YAML file. This object is responsible for creating the pods, making sure they stay up to date 
and ensuring there are enough of them running You can also easily autoscale your applications using a Kubernetes deployment.

we also update the image in the pod and after updating the pod if the image is failing then we able to roll out it to the old image, as called as â€Rolling Back a Deployment â€
using deployment we can undo deployment, pause deployment, resume deployment
----------===
Kubernetes-Day-11    see  LOOK 
https://www.youtube.com/watch?v=veiVV4FeB1U

https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#replicationcontroller-v1-core
https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#replicationcontrollerstatus-v1-core

ReplicationControllerSpec v1 core
ReplicationController [core/v1]
Field	Description
minReadySeconds
integer	Minimum number of seconds for which a newly created pod should be ready without any of its container crashing, for it to be considered available. Defaults to 0 (pod will be considered available as soon as it is ready)

replicas
integer	Replicas is the number of desired replicas. This is a pointer to distinguish between explicit zero and unspecified. Defaults to 1. More info: https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller#what-is-a-replicationcontroller

selector
object	Selector is a label query over pods that should match the Replicas count. If Selector is empty, it is defaulted to the labels present on the Pod template. Label keys and values that must match in order to be controlled by this replication controller, if empty defaulted to labels on Pod template. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors

template
PodTemplateSpec	Template is the object that describes the pod that will be created if insufficient replicas are detected. This takes precedence over a TemplateRef. More info: https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller#pod-template

ReplicationControllerStatus v1 core
 Appears In:
ReplicationController [core/v1]
Field	Description
availableReplicas
integer	The number of available replicas (ready for at least minReadySeconds) for this replication controller.

conditions
ReplicationControllerCondition array
patch strategy: merge
patch merge key: type	Represents the latest available observations of a replication controller's current state.

fullyLabeledReplicas
integer	The number of pods that have labels matching the labels of the pod template of the replication controller.

observedGeneration
integer	ObservedGeneration reflects the generation of the most recently observed replication controller.

readyReplicas
integer	The number of ready replicas for this replication controller.

replicas
nteger	Replicas is the most recently observed number of replicas. More info: https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller#what-is-a-replicationcontroller

apiVersion: v1
kind: ReplicationController
metadata:
   name: rc
spec:
   replicas: 3
   selector: 
      team: dev   
   template: 
      metadata:
        name: rc
        labels:
           team: dev    ###  look  see 
      spec:
        containers:
           - name: rc
             image: nginx
             ports:
                - containerPort: 80

hai@master:~/test$ kubectl get po  -o wide
NAME       READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE               READINESS GATES
rc-5qm5v   1/1     Running   0          2m57s   10.36.0.1   worker1   <none>                       <none>
rc-9s2bg   1/1     Running   0          2m57s   10.44.0.1   worker2   <none>                       <none>
rc-mhqfk   1/1     Running   0          2m57s   10.44.0.2   worker2   <none>                       <none>
---"""
hai@master:~/test$ kubectl describe pod  rc-5qm5v
Name:             rc-5qm5v
Namespace:        default
Labels:           team=dev
IPs:
  IP:           10.36.0.1
Controlled By:  ReplicationController/rc     ###  see look 
Containers:
  rc:
    Image:          nginx
    Port:           80/TCP

hai@master:~$ kubectl explain rc             or   kubectl explain  rc  --recursive | less
KIND:     ReplicationController
VERSION:  v1

DESCRIPTION:
     ReplicationController represents the configuration of a replication
     controller.

FIELDS:
   apiVersion	<string>
     APIVersion defines the versioned schema of this representation of an
     object. Servers should convert recognized schemas to the latest internal
     value, and may reject unrecognized values. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources

   kind	<string>
     Kind is a string value representing the REST resource this object
     represents. Servers may infer this from the endpoint the client submits
     requests to. Cannot be updated. In CamelCase. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds

   metadata	<Object>
     If the Labels of a ReplicationController are empty, they are defaulted to
     be the same as the Pod(s) that the replication controller manages. Standard
     object's metadata. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata

   spec	<Object>
     Spec defines the specification of the desired behavior of the replication
     controller. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

   status	<Object>
     Status is the most recently observed status of the replication controller.
     This data may be out of date by some window of time. Populated by the
     system. Read-only. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

------"""
hai@master:~$ kubectl get rc
NAME   DESIRED   CURRENT   READY   AGE
rc     3         3         3       112m

hai@master:~$ kubectl get rc  --show-labels
NAME   DESIRED   CURRENT   READY   AGE    LABELS
rc     3         3         3       112m   team=dev

hai@master:~$ kubectl get rc,pods  --show-labels
NAME                       DESIRED   CURRENT   READY   AGE    LABELS
replicationcontroller/rc   3         3         3       113m   team=dev

NAME           READY   STATUS    RESTARTS   AGE    LABELS
pod/rc-5qm5v   1/1     Running   0          113m   team=dev
pod/rc-9s2bg   1/1     Running   0          113m   team=dev
pod/rc-mhqfk   1/1     Running   0          113m   team=dev
hai@master:~$

hai@master:~/test$ kubectl get all
NAME           READY   STATUS    RESTARTS   AGE
pod/rc-hkzwb   1/1     Running   0          71s
pod/rc-r7hr6   1/1     Running   0          71s
pod/rc-vfxx4   1/1     Running   0          71s

NAME                       DESIRED   CURRENT   READY   AGE
replicationcontroller/rc   3         3         3       121m

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   2d17h
---------------------"""
hai@master:~/test$ kubectl get all  --all-namespaces
NAMESPACE     NAME                                 READY   STATUS             RESTARTS         A                                                                                GE
default       pod/rc-hkzwb                         1/1     Running            0                5                                                                                m12s
default       pod/rc-r7hr6                         1/1     Running            0                5                                                                                m12s
default       pod/rc-vfxx4                         1/1     Running            0                5                                                                                m12s
kube-system   pod/coredns-565d847f94-q5zw9         0/1     Running            292 (5m4s ago)   2                                                                                d
kube-system   pod/coredns-565d847f94-rg789         0/1     CrashLoopBackOff   289 (14s ago)    2                                                                                d
kube-system   pod/etcd-master                      1/1     Running            6 (42h ago)      2                                                                                d17h
kube-system   pod/kube-apiserver-master            1/1     Running            2 (42h ago)      4                                                                                7h
kube-system   pod/kube-controller-manager-master   1/1     Running            8 (42h ago)      2                                                                                d17h
kube-system   pod/kube-proxy-6t6qp                 1/1     Running            6 (42h ago)      2                                                                                d17h
kube-system   pod/kube-proxy-ffdgc                 1/1     Running            4 (42h ago)      2                                                                                d4h
kube-system   pod/kube-proxy-gq4v9                 1/1     Running            3 (42h ago)      2                                                                                d3h
kube-system   pod/kube-scheduler-master            1/1     Running            8 (42h ago)      2                                                                                d17h
kube-system   pod/weave-net-6bpzq                  2/2     Running            15 (42h ago)     2                                                                                d17h
kube-system   pod/weave-net-8gkx4                  2/2     Running            9 (42h ago)      2                                                                                d4h
kube-system   pod/weave-net-f27sl                  2/2     Running            6 (42h ago)      2                                                                                d3h

NAMESPACE   NAME                       DESIRED   CURRENT   READY   AGE
default     replicationcontroller/rc   3         3         3       125m

NAMESPACE     NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                                                                                                  AGE
default       service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP                                                                                                  2d17h
kube-system   service/kube-dns     ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP                                                                                   2d17h

NAMESPACE     NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   N                                                                                ODE SELECTOR            AGE
kube-system   daemonset.apps/kube-proxy   3         3         3       3            3           k                                                                                ubernetes.io/os=linux   2d17h
kube-system   daemonset.apps/weave-net    3         3         3       3            3           <                                                                                none>                   2d17h

NAMESPACE     NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
kube-system   deployment.apps/coredns   0/2     2            0           2d17h

NAMESPACE     NAME                                 DESIRED   CURRENT   READY   AGE
kube-system   replicaset.apps/coredns-565d847f94   2         2         0       2d17h
======================================#################################"""
Every 2.0s: kubectl get pods --show-labels -o wide                                             master: Wed Dec 21 03:28:08 2022

NAME        READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
appserver   1/1     Running   0          3m55s   10.44.0.1   worker2   <none>           <none>            run=appserver   ## look   see
dbserver    1/1     Running   0          3m45s   10.36.0.1   worker1   <none>           <none>            run=dbserver

""" AFTER  AFTER AFTER 
hai@master:~/test$ kubectl label  pod  appserver  run-
pod/appserver unlabeled
hai@master:~/test$ kubectl label pod dbserver  run-
pod/dbserver unlabeled
-----"""
Every 2.0s: kubectl get pods --show-labels -o wide                                             master: Wed Dec 21 03:34:55 2022

NAME        READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS  
appserver   1/1     Running   0          10m   10.44.0.1   worker2   <none>           <none>            <none>    ## watch look see 
dbserver    1/1     Running   0          10m   10.36.0.1   worker1   <none>           <none>            <none>
=============================================#######################################################"""

hai@master:~$ kubectl run   hero1 --image=nginx
pod/hero1 created
hai@master:~$ kubectl run   hero --image=nginx
pod/hero created

hai@master:~/test$ kubectl get pods  --show-labels  -o wide
NAME    READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
hero    1/1     Running   0          9m19s   10.36.0.1   worker1   <none>           <none>            run=hero
hero1   1/1     Running   0          9m28s   10.44.0.1   worker2   <none>           <none>            run=hero1

hai@master:~$ kubectl label pod hero run-
pod/hero unlabeled
hai@master:~$ kubectl label pod hero1 run-
pod/hero1 unlabeled

Every 2.0s: kubectl get pods --show-labels -o wide                                             master: Wed Dec 21 04:00:36 2022
NAME    READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
hero    1/1     Running   0          11m   10.36.0.1   worker1   <none>           <none>            <none>
hero1   1/1     Running   0          11m   10.44.0.1   worker2   <none>           <none>            <none>
--"
hai@master:~$ kubectl label pod  hero   godfather=megastar      # see look 
pod/hero labeled
hai@master:~$ kubectl label pod  hero1   puspa=alluarjun
pod/hero1 labeled

Every 2.0s: kubectl get pods --show-labels -o wide                                             master: Wed Dec 21 04:28:20 2022

NAME    READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
hero    1/1     Running   0          38m   10.36.0.1   worker1   <none>           <none>            godfather=megastar   ## see  look 
hero1   1/1     Running   0          38m   10.44.0.1   worker2   <none>           <none>            puspa=alluarjun


###################################################################################################################"

hai@master:~$ kubectl  run  prod  --image=nginx
pod/prod created

hai@master:~$ kubectl get pods -o wide
NAME   READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   REA                            DINESS GATES
prod   1/1     Running   0          23s   10.44.0.1   worker2   <none>           <no                            ne>

hai@master:~$ kubectl get pods  --show-labels  -o wide
NAME   READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GAT                ES   LABELS
prod   1/1     Running   0          3m26s   10.44.0.1   worker2   <none>           <none>                            run=prod  # watch

hai@master:~$ kubectl label pod  prod run-
pod/prod unlabeled

hai@master:~$ kubectl get pods  --show-labels  -o wide
NAME   READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
prod   1/1     Running   0          7m48s   10.44.0.1   worker2   <none>           <none>            <none>    ##  watch 

--"""
####   BEFORE  AFTER  

hai@master:~$ kubectl label pod  prod  godfather=megastar
pod/prod labeled

hai@master:~$ kubectl get pod --show-labels  -o wide
NAME   READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
prod   1/1     Running   0          13m   10.44.0.1   worker2   <none>           <none>            godfather=megastar  ## LOOK SEE 
                   SAME LABEL USING ONE POD IS  "RUNNING"

apiVersion: v1
kind: ReplicationController
metadata:
   name: rc
spec:
   replicas: 3    # # given 3  but  deploy only 2 
   selector:
        godfather: megastar    ### see look label  
   template:
      metadata:
        name: rc
        labels:
           godfather: megastar  ## see look label 
      spec:
        containers:
           - name: rc
             image: nginx
             ports:
                - containerPort: 80
--"
hai@master:~/test$ kubectl get rc,pods  --show-labels   -o wide
NAME                       DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES   SELECTOR             LABELS
replicationcontroller/rc   3         3         3       20m   rc           nginx    godfather=megastar   godfather=megastar

NAME           READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/prod       1/1     Running   0          44m   10.44.0.1   worker2   <none>           <none>            godfather=megastar
pod/rc-f4nnf   1/1     Running   0          20m   10.36.0.1   worker1   <none>           <none>            godfather=megastar
pod/rc-jz5vp   1/1     Running   0          20m   10.44.0.2   worker2   <none>           <none>            godfather=megastar
==="
hai@master:~/test$ kubectl apply -f rc.yaml
replicationcontroller/rc created

hai@master:~/test$ kubectl get pods  --show-labels  -o  wide
NAME       READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
prod       1/1     Running   0          24m   10.44.0.1   worker2   <none>           <none>            godfather=megastar  # old manuval deploy 

rc-f4nnf   1/1     Running   0          33s   10.36.0.1   worker1   <none>           <none>            godfather=megastar  # through  replicas 
rc-jz5vp   1/1     Running   0          33s   10.44.0.2   worker2   <none>           <none>            godfather=megastar  #  through replicas

--"

hai@master:~/test$ kubectl get pods -o wide
NAME       READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES
prod       1/1     Running   0          26m     10.44.0.1   worker2   <none>           <none>    # manuval deploy 

rc-f4nnf   1/1     Running   0          2m16s   10.36.0.1   worker1   <none>           <none>   # through replica
rc-jz5vp   1/1     Running   0          2m16s   10.44.0.2   worker2   <none>           <none>   # through  replica

hai@master:~/test$ kubectl describe pod prod
Name:             prod
Namespace:        default
Labels:           godfather=megastar          \\\\\\\\\\\\
Controlled By:  ReplicationController/rc      ===============
Containers:
  prod:
    Container ID:   containerd://213aa8b302937e24956e1056acb1118a96dae0aadbb79694cff5692b0417147c
    Image:          nginx
--"
hai@master:~/test$ kubectl describe pod rc-f4nnf
Name:             rc-f4nnf
Namespace:        default
Labels:           godfather=megastar        ////////////////
Controlled By:  ReplicationController/rc    ==================
Containers:
  rc:
    Container ID:   containerd://09fd1264c34de5b38daaea0a1257bace078751ccf897b0d5c5315f0b7baf4837
    Image:          nginx
--"
hai@master:~/test$ kubectl describe pod rc-jz5vp
Name:             rc-jz5vp
Namespace:        default
Labels:           godfather=megastar        /////////////////
Controlled By:  ReplicationController/rc    =========================
Containers:
  rc:
    Container ID:   containerd://6bc069d2cd33fc6bad00e5f7f3934aeaa466d127ee6ea839f5a05124651ab552
    Image:          nginx

\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
IMPORTENT  
hai@master:~/test$ kubectl get pods  --show-labels
NAME       READY   STATUS    RESTARTS   AGE   LABELS
prod       1/1     Running   0          50m   godfather=megastar
rc-f4nnf   1/1     Running   0          26m   godfather=megastar
rc-jz5vp   1/1     Running   0          26m   godfather=megastar
---"
hai@master:~/test$ kubectl label pod  prod  godfather-
pod/prod unlabeled

hai@master:~/test$ kubectl get pods  --show-labels
NAME       READY   STATUS              RESTARTS   AGE   LABELS
prod       1/1     Running             0          52m   <none>
rc-2h669   0/1     ContainerCreating   0          2s    godfather=megastar
rc-f4nnf   1/1     Running             0          28m   godfather=megastar
rc-jz5vp   1/1     Running             0          28m   godfather=megastar

hai@master:~/test$ kubectl get pods  --show-labels
NAME       READY   STATUS    RESTARTS   AGE   LABELS
prod       1/1     Running   0          52m   <none>
rc-2h669   1/1     Running   0          8s    godfather=megastar
rc-f4nnf   1/1     Running   0          28m   godfather=megastar
rc-jz5vp   1/1     Running   0          28m   godfather=megastar

hai@master:~/test$ kubectl get rc,pod  --show-labels  -o wide
NAME                       DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES   SELECTOR             LABELS
replicationcontroller/rc   3         3         3       32m   rc           nginx    godfather=megastar   godfather=megastar

NAME           READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/prod       1/1     Running   0          56m     10.44.0.1   worker2   <none>           <none>            <none>
pod/rc-2h669   1/1     Running   0          3m57s   10.36.0.2   worker1   <none>           <none>            godfather=megastar
pod/rc-f4nnf   1/1     Running   0          32m     10.36.0.1   worker1   <none>           <none>            godfather=megastar
pod/rc-jz5vp   1/1     Running   0          32m     10.44.0.2   worker2   <none>           <none>            godfather=megastar
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"
hai@master:~/test$ kubectl get rc,pod  --show-labels  -o wide
NAME                       DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES   SELECTOR             LABELS
replicationcontroller/rc   3         3         3       32m   rc           nginx    godfather=megastar   godfather=megastar

NAME           READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/prod       1/1     Running   0          56m     10.44.0.1   worker2   <none>           <none>            <none>
pod/rc-2h669   1/1     Running   0          3m57s   10.36.0.2   worker1   <none>           <none>            godfather=megastar
pod/rc-f4nnf   1/1     Running   0          32m     10.36.0.1   worker1   <none>           <none>            godfather=megastar
pod/rc-jz5vp   1/1     Running   0          32m     10.44.0.2   worker2   <none>           <none>            godfather=megastar

hai@master:~/test$ kubectl label pod prod  godfather=megastar
pod/prod labeled "


hai@master:~$ kubectl get rc,pod  --show-labels  -o wide
NAME                       DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES   SELECTOR             LABELS
replicationcontroller/rc   3         3         3       40m   rc           nginx    godfather=megastar   godfather=megastar

NAME           READY   STATUS        RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/prod       1/1     Running       0          64m   10.44.0.1   worker2   <none>           <none>            godfather=megastar
"pod/rc-2h669   1/1     Terminating   0          12m   10.36.0.2   worker1   <none>           <none>            godfather=megastar"
pod/rc-f4nnf   1/1     Running       0          40m   10.36.0.1   worker1   <none>           <none>            godfather=megastar
pod/rc-jz5vp   1/1     Running       0          40m   10.44.0.2   worker2   <none>           <none>            godfather=megastar

hai@master:~$ kubectl get rc,pod  --show-labels  -o wide
NAME                       DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES   SELECTOR             LABELS
replicationcontroller/rc   3         3         3       40m   rc           nginx    godfather=megastar   godfather=megastar

NAME           READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/prod       1/1     Running   0          64m   10.44.0.1   worker2   <none>           <none>            godfather=megastar
pod/rc-f4nnf   1/1     Running   0          40m   10.36.0.1   worker1   <none>           <none>            godfather=megastar
pod/rc-jz5vp   1/1     Running   0          40m   10.44.0.2   worker2   <none>           <none>            godfather=megastar
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
delete replicacontroller "
hai@master:~$ kubectl get rc,pod  --show-labels  -o wide
NAME                       DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES   SELECTOR             LABELS
replicationcontroller/rc   3         3         3       40m   rc           nginx    godfather=megastar   godfather=megastar

NAME           READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/prod       1/1     Running   0          64m   10.44.0.1   worker2   <none>           <none>            godfather=megastar
pod/rc-f4nnf   1/1     Running   0          40m   10.36.0.1   worker1   <none>           <none>            godfather=megastar
pod/rc-jz5vp   1/1     Running   0          40m   10.44.0.2   worker2   <none>           <none>            godfather=megastar
--"
hai@master:~$ kubectl delete rc  --cascade=false  rc
warning: --cascade=false is deprecated (boolean value) and can be replaced with --cascade=orphan.    ## importent 
replicationcontroller "rc" deleted
-"
hai@master:~/test$ kubectl get rc,pod  --show-labels  -o wide
NAME           READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/prod       1/1     Running   0          76m   10.44.0.1   worker2   <none>           <none>            godfather=megastar
pod/rc-f4nnf   1/1     Running   0          52m   10.36.0.1   worker1   <none>           <none>            godfather=megastar
pod/rc-jz5vp   1/1     Running   0          52m   10.44.0.2   worker2   <none>           <none>            godfather=megastar
hai@master:~/test$

=====================================================================================================================================


hai@master:~$ kubectl describe rc  rc1
Name:         rc1
Namespace:    default
Selector:     godfather=megastar
Labels:       godfather=megastar
Annotations:  <none>
Replicas:     3 current / 3 desired
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  godfather=megastar
  Containers:
   rc1:
    Image:        nginx:1.16
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:           <none>
hai@master:~$


hai@master:~/test$ cat rc.yaml
apiVersion: v1
kind: ReplicationController
metadata:
   name: rc1
spec:
   replicas: 3
   selector:
      godfather: megastar
   template:
      metadata:
        name: rc
        labels:
           godfather: megastar
      spec:
        containers:
           - name: rc
             image: nginx:1.16
             ports:
                - containerPort: 80
"
hai@master:~/test$ kubectl apply   -f rc.yaml
replicationcontroller/rc1 created

hai@master:~$ kubectl get rc,pod  --show-labels  -o wide
NAME                        DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES       SELECTOR             LABELS
replicationcontroller/rc1   3         3         0       9s    rc1          nginx:1.16   godfather=megastar   godfather=megastar

NAME            READY   STATUS              RESTARTS   AGE   IP       NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/rc1-dlvpn   0/1     ContainerCreating   0          9s    <none>   worker2   <none>           <none>            godfather=megastar
pod/rc1-f2tvp   0/1     ContainerCreating   0          9s    <none>   worker1   <none>           <none>            godfather=megastar
pod/rc1-hn4cm   0/1     ContainerCreating   0          9s    <none>   worker2   <none>           <none>            godfather=megastar
===-"
hai@master:~$ kubectl get rc,pod  --show-labels  -o wide
NAME                        DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES         SELECTOR             LABELS
replicationcontroller/rc1   3         3         3       17s   rc1          "nginx:1.16 "    godfather=megastar   godfather=megastar

NAME            READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/rc1-dlvpn   1/1     Running   0          17s   10.44.0.1   worker2   <none>           <none>            godfather=megastar
pod/rc1-f2tvp   1/1     Running   0          17s   10.36.0.1   worker1   <none>           <none>            godfather=megastar
pod/rc1-hn4cm   1/1     Running   0          17s   10.44.0.2   worker2   <none>           <none>            godfather=megastar
------
hai@master:~/test$ kubectl describe pod rc1-dlvpn
Name:             rc1-dlvpn
Namespace:        default
Controlled By:  ReplicationController/rc1
Containers:
  rc1:
    Image:          "nginx:1.16"    "## look see "

==============================
rolling  update 


apiVersion: v1
kind: ReplicationController
metadata:
   name: rc1
spec:
   replicas: 3
   selector:
      godfather: megastar
   template:
      metadata:
        name: rc1
        labels:
           godfather: megastar
      spec:
        containers:
           - name: rc1
             image: nginx:1.18
             ports:
                - containerPort: 80
"
hai@master:~/test$ kubectl apply -f rc1.18up.yaml
replicationcontroller/rc1 configured


NAME                        DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES       SELECTOR             LABELS
replicationcontroller/rc1   3         3         3       91m   rc1          "nginx:1.16"   godfather=megastar   godfather=megastar

NAME            READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/rc1-dlvpn   1/1     Running   0          91m   10.44.0.1   worker2   <none>           <none>            godfather=megastar
pod/rc1-f2tvp   1/1     Running   0          91m   10.36.0.1   worker1   <none>           <none>            godfather=megastar
pod/rc1-hn4cm   1/1     Running   0          91m   10.44.0.2   worker2   <none>           <none>            godfather=megastar
"
hai@master:~$ kubectl get rc,pod --show-labels  -o wide
NAME                        DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES       SELECTOR             LABELS
replicationcontroller/rc1   3         3         3       91m   rc1          "nginx:1.18"   godfather=megastar   godfather=megastar

NAME            READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/rc1-dlvpn   1/1     Running   0          91m   10.44.0.1   worker2   <none>           <none>            godfather=megastar
pod/rc1-f2tvp   1/1     Running   0          91m   10.36.0.1   worker1   <none>           <none>            godfather=megastar
pod/rc1-hn4cm   1/1     Running   0          91m   10.44.0.2   worker2   <none>           <none>            godfather=megastar



hai@master:~$ kubectl describe rc  rc1
Name:         rc1
Namespace:    default
Selector:     godfather=megastar
Labels:       godfather=megastar
Annotations:  <none>
Replicas:     3 current / 3 desired
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  godfather=megastar
  Containers:
   rc1:
    Image:        nginx:1.16
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:           <none>
hai@master:~$

 kubectl rolling-update rc1   --update-period=10s -f rc.yaml     , this feather not avilable this market 
 ==============================####################################################################################
             "   ReplicaSet   ""
https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#replicaset-v1-apps
https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#replicasetstatus-v1-apps
hai@master:~/test$ kubectl explain rs 

apiVersion: apps/v1
kind: ReplicaSet
metadata:  
   name: rs1
spec:
  replicas: 3
  selector: 
    matchExpressions:
       - key: team
         operator: In
         values:
           -  dev
           -  prod
           -  test
  template: 
    metadata:
      name: rs1
      labels:
        team: dev
    spec: 
      containers:
        - name: nginx
          image: nginx:1.16
          ports:
            - containerPort: 80
"
hai@master:~/test$  kubectl get rs,pod  --show-labels  -o  wide
NAME                  DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES       SELECTOR                  LABELS
replicaset.apps/rs1   3         3         3       80s   nginx        nginx:1.16   team in (dev,prod,test)   <none>

NAME            READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/rs1-7c7cg   1/1     Running   0          80s   10.44.0.2   worker2   <none>           <none>            team=dev
pod/rs1-dkjgp   1/1     Running   0          80s   10.44.0.1   worker2   <none>           <none>            team=dev
pod/rs1-lv2tw   1/1     Running   0          80s   10.36.0.1   worker1   <none>           <none>            team=dev
"------
hai@master:~/test$ kubectl run a  --image=nginx
pod/a created
hai@master:~/test$ kubectl run b  --image=nginx
pod/b created
hai@master:~/test$ kubectl run c  --image=nginx
pod/c created

hai@master:~$ kubectl get  rs,pod   --show-labels  -o wide
NAME    READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/a   1/1     Running   0          32s   10.44.0.1   worker2   <none>           <none>            "run=a"
pod/b   1/1     Running   0          25s   10.36.0.1   worker1   <none>           <none>            run=b    # look see 
pod/c   1/1     Running   0          20s   10.44.0.2   worker2   <none>           <none>            run=c
"========
hai@master:~/test$ kubectl label pod a  run-
pod/a unlabeled
hai@master:~/test$ kubectl label pod b  run-
pod/b unlabeled
hai@master:~/test$ kubectl label pod c  run-
pod/c unlabeled
hai@master:~$ kubectl get  rs,pod   --show-labels  -o wide
NAME    READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/a   1/1     Running   0          3m10s   10.44.0.1   worker2   <none>           <none>            <none>   # look see 
pod/b   1/1     Running   0          3m3s    10.36.0.1   worker1   <none>           <none>           " <none>"
pod/c   1/1     Running   0          2m58s   10.44.0.2   worker2   <none>           <none>            <none>
"=========
hai@master:~/test$ kubectl label  pod  a  team=dev
pod/a labeled
hai@master:~/test$ kubectl label  pod  b  team=dev
pod/b labeled
hai@master:~/test$ kubectl label  pod  c  team=dev
pod/c labeled

hai@master:~$ kubectl get  rs,pod   --show-labels  -o wide
NAME    READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/a   1/1     Running   0          6m6s    10.44.0.1   worker2   <none>           <none>            team=dev"
pod/b   1/1     Running   0          5m59s   10.36.0.1   worker1   <none>           <none>            team=dev
pod/c   1/1     Running   0          5m54s   10.44.0.2   worker2   <none>           <none>            team=dev

"  ======
hai@master:~$ kubectl get  rs,pod   --show-labels  -o wide
NAME                  DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES       SELECTOR                                    LABELS
replicaset.apps/rs1   3         3         3       2m47s   nginx        nginx:1.16   team in (dev,prod,test),team notin (test)   <none>

NAME    READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/a   1/1     Running   0          14m   10.44.0.1   worker2   <none>           <none>            team=dev
pod/b   1/1     Running   0          14m   10.36.0.1   worker1   <none>           <none>            team=dev
pod/c   1/1     Running   0          14m   10.44.0.2   worker2   <none>           <none>            team=dev
"==============
hai@master:~/test$ kubectl delete rs  --cascade=false  rs1      ##  importnet ok 
warning: --cascade=false is deprecated (boolean value) and can be replaced with --cascade=orphan.
replicaset.apps "rs1" deleted

hai@master:~$ kubectl get  rs,pod   --show-labels  -o wide
NAME    READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/a   1/1     Running   0          14m   10.44.0.1   worker2   <none>           <none>            team=dev
pod/b   1/1     Running   0          14m   10.36.0.1   worker1   <none>           <none>            team=dev
pod/c   1/1     Running   0          14m   10.44.0.2   worker2   <none>           <none>            team=dev
"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
hai@master:~$ kubectl get  rs,pod   --show-labels  -o wide
NAME    READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/a   1/1     Running   0          24m   10.44.0.1   worker2   <none>           <none>            team=prod
pod/b   1/1     Running   0          24m   10.36.0.1   worker1   <none>           <none>            team=dev
pod/c   1/1     Running   0          24m   10.44.0.2   worker2   <none>           <none>            team=test"

apiVersion: apps/v1
kind: ReplicaSet
metadata:
   name: rs1
spec:
  replicas: 3
  selector:
    matchExpressions:
       - key: team                          ## see  look  2 keys   2 operators 
         operator: In
         values:
           -  dev
           -  prod
           -  test
       - key: team                 ## see  look   2 keys 2 operators     
         operator: NotIn
         values:
           -  test
  template:
    metadata:
      name: rs1
      labels:
        team: dev
    spec:
      containers:
        - name: nginx
          image: nginx:1.16
          ports:
            - containerPort: 80



hai@master:~$ kubectl get  rs,pod   --show-labels  -o wide
NAME                  DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES       SELECTOR                                        LABELS
replicaset.apps/rs1   3         3         2       0s    nginx        nginx:1.16   team in (dev,prod,test),team notin (test)        <none>

NAME            READY   STATUS              RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/a           1/1     Running             0          25m   10.44.0.1   worker2   <none>           <none>            team=prod
pod/b           1/1     Running             0          24m   10.36.0.1   worker1   <none>           <none>            team=dev
pod/c           1/1     Running             0          24m   10.44.0.2   worker2   <none>           <none>            team=test                                                  
pod/rs1-sr64m   0/1     ContainerCreating   0          0s    <none>      worker1   <none>           <none>            team=dev"

hai@master:~$ kubectl get  rs,pod   --show-labels  -o wide
NAME                  DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES       SELECTOR                                       LABELS
replicaset.apps/rs1   3         3         2       1s    nginx        nginx:1.16   team in (dev,prod,test),team notin (test)       <none>

NAME            READY   STATUS              RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/a           1/1     Running             0          25m   10.44.0.1   worker2   <none>           <none>            team=prod
pod/b           1/1     Running             0          24m   10.36.0.1   worker1   <none>           <none>            team=dev
pod/c           1/1     Running             0          24m   10.44.0.2   worker2   <none>           <none>            team=test
pod/rs1-sr64m   0/1     ContainerCreating   0          2s    <none>      worker1   <none>           <none>            team=dev"

hai@master:~$ kubectl get  rs,pod   --show-labels  -o wide
NAME                  DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES       SELECTOR                                        LABELS
replicaset.apps/rs1   3         3         2       3s    nginx        nginx:1.16   team in (dev,prod,test),team notin (test)        <none>

NAME            READY   STATUS              RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/a           1/1     Running             0          25m   10.44.0.1   worker2   <none>           <none>            team=prod
pod/b           1/1     Running             0          24m   10.36.0.1   worker1   <none>           <none>            team=dev
pod/c           1/1     Running             0          24m   10.44.0.2   worker2   <none>           <none>            team=test
pod/rs1-sr64m   0/1     ContainerCreating   0          3s    <none>      worker1   <none>           <none>            team=dev"

hai@master:~$ kubectl get  rs,pod   --show-labels  -o wide
NAME                  DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES       SELECTOR                                    LABELS
replicaset.apps/rs1   3         3         3       21s   nginx        nginx:1.16   team in (dev,prod,test),team notin (test)   <none>

NAME            READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/a           1/1     Running   0          25m   10.44.0.1   worker2   <none>           <none>            team=prod
pod/b           1/1     Running   0          25m   10.36.0.1   worker1   <none>           <none>            team=dev
pod/c           1/1     Running   0          25m   10.44.0.2   worker2   <none>           <none>            team=test
pod/rs1-sr64m   1/1     Running   0          21s   10.36.0.2   worker1   <none>           <none>            team=dev

hai@master:~/test$ kubectl  describe pod a
Name:             a
Namespace:        default
Labels:           team=prod
Controlled By:  "ReplicaSet/rs1"    #  see look rs controlled 

hai@master:~/test$ kubectl  describe pod b
Name:             b
Name:             a
Namespace:        default
Labels:            team=dev
Controlled By:  "ReplicaSet/rs1"    #  see look rs controlled 


hai@master:~/test$ kubectl  describe pod rs1-sr64m
Name:             rs1-sr64m
Namespace:        default
Labels:           team=dev
Controlled By:  "ReplicaSet/rs1

hai@master:~/test$ kubectl  describe pod c
Name:             c
Namespace:        default
Labels:           team=test     #  " SEE LOOK  NO  CONTROLLED "
NO NO NO  " no CONTROLLED" 
========================="
hai@master:~$ kubectl get  rs,pod   --show-labels  -o wide

NAME                  DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES       SELECTOR                                    LABELS
replicaset.apps/rs1   3         3         3       45m   nginx        nginx:1.16   team in (dev,prod,test),team notin (test)   <none>

NAME            READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/a           1/1     Running   0          70m   10.44.0.1   worker2   <none>           <none>            team=prod
pod/b           1/1     Running   0          70m   10.36.0.1   worker1   <none>           <none>            team=dev
pod/c           1/1     Running   0          70m   10.44.0.2   worker2   <none>           <none>            "team=test"
pod/rs1-sr64m   1/1     Running   0          45m   10.36.0.2   worker1   <none>           <none>            team=dev
"--
hai@master:~/test$ kubectl delete rs  rs1
replicaset.apps "rs1" deleted

hai@master:~$ kubectl get  rs,pod   --show-labels  -o wide
NAME                  DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES       SELECTOR                                    LABELS
replicaset.apps/rs1   3         3         3       45m   nginx        nginx:1.16   team in (dev,prod,test),team notin (test)   <none>

NAME            READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/a           1/1     Running   0          70m   10.44.0.1   worker2   <none>           <none>            team=prod
pod/b           1/1     Running   0          70m   10.36.0.1   worker1   <none>           <none>            team=dev
pod/c           1/1     Running   0          70m   10.44.0.2   worker2   <none>           <none>            team=test
pod/rs1-sr64m   1/1     Running   0          45m   10.36.0.2   worker1   <none>           <none>            team=dev

hai@master:~$ kubectl get  rs,pod   --show-labels  -o wide
NAME            READY   STATUS        RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/a           1/1     "Terminating "  0          72m   10.44.0.1   worker2   <none>           <none>            team=prod
pod/b           1/1     "Terminating"   0          72m   10.36.0.1   worker1   <none>           <none>            team=dev
pod/c           1/1     Running       0          72m   10.44.0.2   worker2   <none>           <none>            team=test
pod/rs1-sr64m   1/1     "Terminating"   0          47m   10.36.0.2   worker1   <none>           <none>            team=dev"

hai@master:~$ kubectl get  rs,pod   --show-labels  -o wide
NAME    READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/c   1/1     Running   0          72m   10.44.0.2   worker2   <none>           <none>            team=test
################################################################################################################################
---     DEPLOYMENT ==   

https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/

https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#deployment-v1-apps

Deployment v1 apps
Group     	Version	         Kind
apps	      v1	             Deployment

DeploymentSpec v1 apps
 Appears In:
Deployment [apps/v1]
Field	Description
minReadySeconds
integer	Minimum number of seconds for which a newly created pod should be ready without any of its container crashing, for it to be considered available. Defaults to 0 (pod will be considered available as soon as it is ready)

paused
boolean	Indicates that the deployment is paused.

progressDeadlineSeconds
integer	The maximum time in seconds for a deployment to make progress before it is considered to be failed. The deployment controller will continue to process failed deployments and a condition with a ProgressDeadlineExceeded reason will be surfaced in the deployment status. Note that progress will not be estimated during the time a deployment is paused. Defaults to 600s.

replicas
integer	Number of desired pods. This is a pointer to distinguish between explicit zero and not specified. Defaults to 1.

revisionHistoryLimit
integer	The number of old ReplicaSets to retain to allow rollback. This is a pointer to distinguish between explicit zero and not specified. Defaults to 10.
selector
LabelSelector	Label selector for pods. Existing ReplicaSets whose pods are selected by this will be the ones affected by this deployment. It must match the pod template's labels.

strategy
DeploymentStrategy
patch strategy: retainKeys	The deployment strategy to use to replace existing pods with new ones.

template
PodTemplateSpec	Template describes the pods that will be created.

kubectl explain deployment --recursive | less

  spec <Object>
      minReadySeconds   <integer>
      paused    <boolean>
      progressDeadlineSeconds   <integer>
      replicas  <integer>
      revisionHistoryLimit      <integer>
      selector  <Object>
         matchExpressions       <[]Object>
            key <string>
            operator    <string>
            values      <[]string>
         matchLabels    <map[string]string>
      strategy  <Object>
         rollingUpdate  <Object>
            maxSurge    <string>
            maxUnavailable      <string>
         type   <string>
      template  <Object>
=======
apiVersion: apps/v1
kind: Deployment
metadata: 
   name: web-app
spec:
  replicas: 3
  selector: 
     matchLabels:
        app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec: 
      containers:
      -  name: nginx
         image: nginx:1.7.1
         ports:
           - containerPort: 80


hai@master:~$ kubectl get deployment -o wide
No resources found in default namespace.

hai@master:~$ kubectl get deployment -o wide
NAME      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR
web-app   0/3     0            0           5s    nginx        nginx:1.7.1   app=nginx"


hai@master:~$ kubectl get all -o wide
NAME                           READY   STATUS              RESTARTS   AGE   IP       NODE      NOMINATED NODE   READINESS GATES
pod/web-app-788b5bb875-77k9f   0/1     ContainerCreating   0          39s   <none>   worker1   <none>           <none>
pod/web-app-788b5bb875-fxtck   0/1     ContainerCreating   0          39s   <none>   worker2   <none>           <none>
pod/web-app-788b5bb875-ltzhf   0/1     ContainerCreating   0          39s   <none>   worker2   <none>           <none>

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3d16h   <none>
NAME                      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR
deployment.apps/web-app   0/3     3            0           43s   nginx        nginx:1.7.1   app=nginx
NAME                                 DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR
replicaset.apps/web-app-788b5bb875   3         3         0       42s   nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875"


hai@master:~$ kubectl get rs,pod  --show-labels  -o  wide
NAME                                 DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/web-app-788b5bb875   3         3         3       46m   nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875

NAME                           READY   STATUS    RESTARTS        AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/web-app-788b5bb875-77k9f   1/1     Running   1 (2m28s ago)   46m   10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-fxtck   1/1     Running   1 (3m46s ago)   46m   10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-ltzhf   1/1     Running   1 (3m45s ago)   46m   10.44.0.2   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875

=================================
hai@master:~/test$ kubectl describe deployment
Name:                   web-app
Namespace:              default
CreationTimestamp:      Thu, 22 Dec 2022 01:55:39 -0800
Labels:                 <none>
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.7.1     "### see look "
    Port:         80/TCP
 Conditions:
  Type           Status  Reason
  ----           ------  ------
  Progressing    True    NewReplicaSetAvailable
  Available      True    MinimumReplicasAvailable
OldReplicaSets:  <none>                               "###  see look" 
NewReplicaSet:   web-app-788b5bb875 (3/3 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  71m   deployment-controller  Scaled up replica set web-app-788b5bb875 to 3"

hai@master:~$ kubectl rollout history deployment.apps/web-app
deployment.apps/web-app
REVISION  CHANGE-CAUSE        "## see look "
1         <none>

hai@master:~$ kubectl get all --show-labels  -o  wide
NAME                           READY   STATUS    RESTARTS      AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/web-app-788b5bb875-77k9f   1/1     Running   1 (48m ago)   91m   10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-fxtck   1/1     Running   1 (49m ago)   91m   10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-ltzhf   1/1     Running   1 (49m ago)   91m   10.44.0.2   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3d18h   <none>     component=apiserver,provider=kubernetes

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/web-app   3/3     3            3           91m   nginx        nginx:1.7.1   app=nginx   <none>

NAME                                 DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/web-app-788b5bb875   3         3         3       91m   nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875"


hai@master:~/test$ kubectl delete -f  dep.yaml
deployment.apps "web-app" deleted"

hai@master:~$ kubectl get all --show-labels  -o  wide
NAME                           READY   STATUS        RESTARTS      AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/web-app-788b5bb875-77k9f   1/1     "Terminating"   1 (49m ago)   93m   10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-"788b5bb875"-fxtck   1/1     "Terminating"   1 (50m ago)   93m   10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-ltzhf   1/1     "Terminating"   1 (50m ago)   93m   10.44.0.2   worker2   <none>           <none>            "app=nginx,pod-template-hash=788b5bb875"

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3d18h   <none>     component=apiserver,provider=kubernetes


hai@master:~$ kubectl get all --show-labels  -o  wide
NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3d18h   <none>     component=apiserver,provider=kubernetes

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
hai@master:~/test$ kubectl delete -f  dep.yaml
deployment.apps "web-app" deleted"

hai@master:~$ kubectl get all --show-labels  -o  wide
NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3d18h   <none>     component=apiserver,provider=kubernetes"

hai@master:~/test$ kubectl apply -f  dep.yaml  --record=true         " ## importent see best ok "
Flag --record has been deprecated, --record will be removed in the future
deployment.apps/web-app created

------
hai@master:~$ kubectl get all --show-labels  -o  wide
NAME                           READY   STATUS              RESTARTS   AGE   IP       NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/web-app-788b5bb875-dtbbt   0/1     "ContainerCreating"   0          1s    <none>   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-lvrmz   0/1     "ContainerCreating "  0          1s    <none>   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-rq7p4   0/1     "ContainerCreating "  0          1s    <none>   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3d18h   <none>     component=apiserver,provider=kubernetes

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/web-app   0/3     3            0           1s    nginx        nginx:1.7.1   app=nginx   <none>

NAME                                 DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/web-app-788b5bb875   3         3         0       1s    nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875"
-------
hai@master:~$ kubectl get all --show-labels  -o  wide
NAME                           READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/web-app-"788b5bb875-dtbbt"   1/1     Running   0          22s   10.44.0.2   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-"788b5bb875-lvrmz"   1/1     Running   0          22s   10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-"788b5bb875-rq7p4"   1/1     Running   0          22s   10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3d18h   <none>     component=apiserver,provider=kubernetes

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/web-app   3/3     3            3           22s   nginx        nginx:1.7.1   app=nginx   <none>

NAME                                 DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                 LABELS   ##"old id safe  look see "
replicaset.apps/web-app-788b5bb875   3         3         3       22s   nginx        nginx:1.7.1   "app=nginx,pod-template-hash=788b5bb875 "  "app=nginx,pod-template-hash=788b5bb875"
----------"

hai@master:~$ kubectl describe  deployment
Name:                   web-app
Namespace:              default
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.7.1
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   web-app-788b5bb875 (3/3 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  13m   deployment-controller  Scaled up replica set web-app-"788b5bb875" to 3"
-----
hai@master:~$ kubectl rollout history deployment.apps/web-app
deployment.apps/web-app
REVISION  CHANGE-CAUSE
1         kubectl apply --filename=dep.yaml --record=true     "###  see look modifying "


apiVersion: apps/v1
kind: Deployment
metadata: 
   name: web-app
spec:
  replicas: 3
  selector: 
     matchLabels:
        app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec: 
      containers:
      -  name: nginx
         image: nginx:1.9.1    "## see  look  update "
         ports:
           - containerPort: 80









hai@master:~$ kubectl get all --show-labels -o wide
NAME                           READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/web-app-788b5bb875-dtbbt   1/1     Running   0          37m   10.44.0.2   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-lvrmz   1/1     Running   0          37m   10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-rq7p4   1/1     Running   0          37m   10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3d19h   <none>     component=apiserver,provider=kubernetes

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/web-app   3/3     3            3           37m   nginx        "nginx:1.7.1 "  app=nginx   <none>

NAME                                 DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/web-app-788b5bb875   3         3         3       37m   nginx       " nginx:1.7.1"   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
#"/////////////////////////////////////////////

hai@master:~$ kubectl get all --show-labels -o wide
NAME                           READY   STATUS              RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/web-app-788b5bb875-dtbbt   1/1     Running             0          38m   10.44.0.2   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-lvrmz   1/1     Running             0          38m   10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-rq7p4   1/1     Running             0          38m   10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-"f4488db8f-s89g9    0/1     ContainerCreating"   0          0s    <none>      worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3d19h   <none>     component=apiserver,provider=kubernetes

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/web-app   3/3     1            3           38m   nginx        nginx:1.9.1   app=nginx   <none>

NAME                                 DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/web-app-788b5bb875   3         3         3       38m   nginx        "nginx:1.7.1"   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/web-app-f4488db8f    1         1         0       0s    nginx        "nginx:1.9.1  " app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f
"#\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\//////////////////\\\\\\\\\/////?///\\/\/\/\/\\/\/

hai@master:~$ kubectl get all --show-labels -o wide
NAME                           READY   STATUS              RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/web-app-788b5bb875-dtbbt   1/1     Running             0          38m   10.44.0.2   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-lvrmz   1/1     Running             0          38m   10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
"pod/web-app-f4488db8f-p9kvn    0/1     ContainerCreating   "0          9s    <none>      worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/web-app-f4488db8f-s89g9    1/1     Running             0          39s   10.36.0.2   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3d19h   <none>     component=apiserver,provider=kubernetes

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/web-app   3/3     2            3           38m   nginx        nginx:1.9.1   app=nginx   <none>

NAME                                 DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/web-app-788b5bb875   2         2         2       38m   nginx        "nginx:1.7.1"   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/web-app-f4488db8f    2         2         1       39s   nginx        "nginx:1.9.1"   app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f
"#\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\//////////////////\\\\\\\\\/////?///\\/\/\/\/\\/\/

hai@master:~$ kubectl get all --show-labels -o wide
NAME                          READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/web-app-f4488db8f-jmv9w   1/1     Running   0          82s     10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/web-app-f4488db8f-p9kvn   1/1     Running   0          110s    10.44.0.3   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/web-app-f4488db8f-s89g9   1/1     Running   0          2m20s   10.36.0.2   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3d19h   <none>     component=apiserver,provider=kubernetes

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/web-app   3/3     3            3           40m   nginx        nginx:1.9.1   app=nginx   <none>

NAME                                 DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/web-app-788b5bb875   0         0         0       40m     nginx       " nginx:1.7.1 "  app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/web-app-f4488db8f    3         3         3       2m20s   nginx        "nginx:1.9.1  " app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f
hai@master:~$
---////\\\\\\\///////////'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"

hai@master:~$ kubectl rollout history deployment.apps/web-app
deployment.apps/web-app
REVISION  CHANGE-CAUSE
1         kubectl apply --filename=dep.yaml --record=true
2         <none>"
=\\//\\\\\\\\\\\\\\\\\\\\\\

hai@master:~$ kubectl describe deployment web-app
Name:                   web-app
Namespace:              default
Labels:                 <none>
Selector:               app=nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.9.1    " # see look "
    Port:         80/TCP
OldReplicaSets:  <none>
NewReplicaSet:   web-app-f4488db8f (3/3 replicas created)"
######################################################################################################################
 kubectl rollout undo deployment.apps/web-app --to-revision=1   "# importent  "" 

hai@master:~/test$  kubectl rollout undo deployment.apps/web-app --to-revision=1
deployment.apps/web-app rolled back
////"
hai@master:~/test$  kubectl rollout history deployment.apps/web-app
deployment.apps/web-app
REVISION  CHANGE-CAUSE
2         <none>
"3         kubectl apply --filename=dep.yaml --record=true "   
+++++++++"
hai@master:~$ kubectl describe deployment web-app
Name:                   web-app
Annotations:            deployment.kubernetes.io/revision: 3
                        kubernetes.io/change-cause: kubectl apply --filename=dep.yaml --record=true
Selector:               app=nginx
Replicas:               3 desired | 1 updated | 4 total | 3 available | 1 unavailable
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.7.1
"OldReplicaSets:  web-app-f4488db8f (3/3 replicas created)
NewReplicaSet:   web-app-788b5bb875 (1/1 replicas created)"
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  49m   deployment-controller  Scaled up replica set web-app-f4488db8f to 1
  Normal  ScalingReplicaSet  48m   deployment-controller  Scaled down replica set web-app-788b5bb875 to 2 from 3
  Normal  ScalingReplicaSet  48m   deployment-controller  Scaled up replica set web-app-f4488db8f to 2 from 1
  Normal  ScalingReplicaSet  48m   deployment-controller  Scaled down replica set web-app-788b5bb875 to 1 from 2
  Normal  ScalingReplicaSet  48m   deployment-controller  Scaled up replica set web-app-f4488db8f to 3 from 2 
  Normal  ScalingReplicaSet  48m   deployment-controller  Scaled down replica set web-app-788b5bb875 to 0 from 1
  "Normal  ScalingReplicaSet  1s    deployment-controller  Scaled up replica set web-app-788b5bb875 to 1 from 0""
-\\\\\\//////////////\\\\\\/\/\/\/\/\////

hai@master:~$ kubectl describe deployment web-app
Name:                   web-app
Namespace:              default
Annotations:            deployment.kubernetes.io/revision: 3
                        kubernetes.io/change-cause: kubectl apply --filename=dep.yaml --record=true
Selector:               app=nginx
Replicas:               3 desired | 3 updated | 4 total | 3 available | 1 unavailable
  Labels:  app=nginx                            "see look"
  Containers:
   nginx:
    Image:        nginx:1.7.1
    Port:         80/TCP
OldReplicaSets:  web-app-f4488db8f (1/1 replicas created)     "## look see "
NewReplicaSet:   web-app-788b5bb875 (3/3 replicas created)   " ##  look see "
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  49m   deployment-controller  Scaled up replica set web-app-f4488db8f to 1
  Normal  ScalingReplicaSet  48m   deployment-controller  Scaled down replica set web-app-788b5bb875 to 2 from 3
  Normal  ScalingReplicaSet  48m   deployment-controller  Scaled up replica set web-app-f4488db8f to 2 from 1
  Normal  ScalingReplicaSet  48m   deployment-controller  Scaled down replica set web-app-788b5bb875 to 1 from 2
  Normal  ScalingReplicaSet  48m   deployment-controller  Scaled up replica set web-app-f4488db8f to 3 from 2
  Normal  ScalingReplicaSet  48m   deployment-controller  Scaled down replica set web-app-788b5bb875 to 0 from 1
  Normal  ScalingReplicaSet  12s   deployment-controller  Scaled up replica set web-app-788b5bb875 to 1 from 0
  Normal  ScalingReplicaSet  8s    deployment-controller  Scaled down replica set web-app-f4488db8f to 2 from 3
  Normal  ScalingReplicaSet  8s    deployment-controller  Scaled up replica set web-app-788b5bb875 to 2 from 1
  Normal  ScalingReplicaSet  3s    deployment-controller  Scaled down replica set web-app-f4488db8f to 1 from 2
"  Normal  ScalingReplicaSet  3s    deployment-controller  Scaled up replica set web-app-788b5bb875 to 3 from 2""
=////////////////////  \\\\\\\\\\\\\\\//
hai@master:~$ kubectl describe deployment web-app
Name:                   web-app
Namespace:              default
Annotations:            deployment.kubernetes.io/revision: 3
                        kubernetes.io/change-cause: kubectl apply --filename=dep.yaml --record=true
Selector:               app=nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate           "see look "
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.7.1
    Port:         80/TCP
OldReplicaSets:  <none>               "##see  look "
NewReplicaSet:   web-app-788b5bb875 (3/3 replicas created)  "# see look"
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  49m   deployment-controller  Scaled up replica set web-app-f4488db8f to 1
  Normal  ScalingReplicaSet  49m   deployment-controller  Scaled down replica set web-app-788b5bb875 to 2 from 3
  Normal  ScalingReplicaSet  49m   deployment-controller  Scaled up replica set web-app-f4488db8f to 2 from 1
  Normal  ScalingReplicaSet  48m   deployment-controller  Scaled down replica set web-app-788b5bb875 to 1 from 2
  Normal  ScalingReplicaSet  48m   deployment-controller  Scaled up replica set web-app-f4488db8f to 3 from 2
  Normal  ScalingReplicaSet  48m   deployment-controller  Scaled down replica set web-app-788b5bb875 to 0 from 1
  Normal  ScalingReplicaSet  14s   deployment-controller  Scaled up replica set web-app-788b5bb875 to 1 from 0
  Normal  ScalingReplicaSet  10s   deployment-controller  Scaled down replica set web-app-f4488db8f to 2 from 3
  Normal  ScalingReplicaSet  10s   deployment-controller  Scaled up replica set web-app-788b5bb875 to 2 from 1
  Normal  ScalingReplicaSet  5s    deployment-controller  Scaled down replica set web-app-f4488db8f to 1 from 2
  Normal  ScalingReplicaSet  5s    deployment-controller  Scaled up replica set web-app-788b5bb875 to 3 from 2
  Normal  ScalingReplicaSet  1s    deployment-controller  Scaled down replica set web-app-f4488db8f to 0 from 1"
==/++++++++++++++++++++++++\\\\\\//////////\\\\=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
hai@master:~$ kubectl get all --show-labels -o wide
NAME                           READY   STATUS              RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/web-app-788b5bb875-8ffnd   0/1     ContainerCreating   0          2s    <none>      worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-dnjgt   1/1     Running             0          6s    10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-f4488db8f-jmv9w    1/1     Running             0          48m   10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
"pod/web-app-f4488db8f-p9kvn    1/1     Terminating "        0          48m   10.44.0.3   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/web-app-f4488db8f-s89g9    1/1     Running             0          49m   10.36.0.2   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3d19h   <none>     component=apiserver,provider=kubernetes

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/web-app   3/3     2            3           87m   nginx        nginx:1.7.1   app=nginx   <none>

NAME                                 DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/web-app-788b5bb875   2         2         1       87m   nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/web-app-f4488db8f    2         2         2       49m   nginx        "nginx:1.9.1 "  app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f"
=======/////////////////\\\\\\\\\\\\\\\\\\\\\\\\\\\

hai@master:~$ kubectl get all --show-labels -o wide
NAME                           READY   STATUS              RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
"pod/web-app-788b5bb875-8ffnd   0/1     ContainerCreating"   0          3s    <none>      worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-dnjgt   1/1     Running             0          7s    10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-f4488db8f-jmv9w    1/1     Running             0          48m   10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/web-app-f4488db8f-p9kvn    0/1     Terminating         0          48m   10.44.0.3   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/web-app-f4488db8f-s89g9    1/1     Running             0          49m   10.36.0.2   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3d19h   <none>     component=apiserver,provider=kubernetes

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/web-app   3/3     2            3           87m   nginx        nginx:1.7.1   app=nginx   <none>

NAME                                 DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/web-app-788b5bb875   2         2         1       87m   nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/web-app-f4488db8f    2         2         2       49m   nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f
---==##################################"
hai@master:~$ kubectl get all --show-labels -o wide
NAME                           READY   STATUS              RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/web-app-788b5bb875-8ffnd   0/1     ContainerCreating   0          4s    <none>      worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-dnjgt   1/1     Running             0          8s    10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-f4488db8f-jmv9w    1/1     Running             0          48m   10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/web-app-f4488db8f-s89g9    1/1     Running             0          49m   10.36.0.2   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3d19h   <none>     component=apiserver,provider=kubernetes

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/web-app   3/3     2            3           87m   nginx        nginx:1.7.1   app=nginx   <none>

NAME                                 DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/web-app-788b5bb875 "  2         2         1       87m   nginx        nginx:1.7.1"   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/web-app-f4488db8f    2         2         2       49m   nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f
##############################\\\\\//////////////////////////////////"

hai@master:~$ kubectl get all --show-labels -o wide
NAME                           READY   STATUS              RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/web-app-788b5bb875-25t9m   0/1     ContainerCreating   0          0s    <none>      worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
"pod/web-app-788b5bb875-8ffnd   1/1     Running"             0          5s    10.36.0.3   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-dnjgt   1/1     Running             0          9s    10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-f4488db8f-jmv9w    1/1     Terminating         0          48m   10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/web-app-f4488db8f-s89g9    1/1     Running             0          49m   10.36.0.2   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3d19h   <none>     component=apiserver,provider=kubernetes

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/web-app   3/3     3            3           87m   nginx        nginx:1.7.1   app=nginx   <none>

NAME                                 DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/web-app-788b5bb875   "3         3         2       87m   nginx        nginx:1.7.1 "  app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/web-app-f4488db8f    1         1         1       49m   nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f
========================/////////////////////////"
hai@master:~$ kubectl get all --show-labels -o wide
NAME                           READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/web-app-788b5bb875-25t9m   1/1     Running   0          5m6s    10.44.0.2   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-8ffnd   1/1     Running   0          5m11s   10.36.0.3   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-dnjgt   1/1     Running   0          5m15s   10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3d19h   <none>     component=apiserver,provider=kubernetes

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/web-app   3/3     3            3           92m   nginx        nginx:1.7.1   app=nginx   <none>

NAME                                 DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/web-app-788b5bb875   3         3         3       92m   nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/web-app-f4488db8f    0         0         0       54m   nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f
"/////////////////////////+++++++++++++=======================##########################################

hai@master:~$ kubectl rollout pause  deployment.apps/web-app
deployment.apps/web-app paused "

apiVersion: apps/v1
kind: Deployment
metadata:
   name: web-app
spec:
  replicas: 3
  selector:
     matchLabels:
        app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      -  name: nginx
         image: nginx:1.7.1
         ports:
           - containerPort: 80"

hai@master:~/test$ kubectl apply -f depup1.18.yaml --record=true
Flag --record has been deprecated, --record will be removed in the future
deployment.apps/web-app configured"

hai@master:~$ kubectl rollout resume  deployment.apps/web-app
deployment.apps/web-app resumed "

hai@master:~/test$ kubectl scale  deployment.apps/web-app  --replicas=10
deployment.apps/web-app scaled

hai@master:~$ kubectl get all  --show-labels  -o wide
NAME                           READY   STATUS              RESTARTS   AGE    IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/web-app-788b5bb875-25t9m   1/1     Running             0          4h2m   10.44.0.2   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-54zb9   0/1     ContainerCreating   0          27s    <none>      worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-8ffnd   1/1     Running             0          4h2m   10.36.0.3   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-dnjgt   1/1     Running             0          4h2m   10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-f5vvt   0/1     ContainerCreating   0          32s    <none>      worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-fnv65   0/1     ContainerCreating   0          27s    <none>      worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-gw8jm   0/1     ContainerCreating   0          27s    <none>      worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-nrz2l   0/1     Pending             0          27s    <none>      worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-pqj9s   0/1     ContainerCreating   0          30s    <none>      worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/web-app-788b5bb875-zr7p7   0/1     ContainerCreating   0          29s    <none>      worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3d23h   <none>     component=apiserver,provider=kubernetes

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/web-app   3/10    10           3           5h30m   nginx        nginx:1.7.1   app=nginx   <none>

NAME                                 DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/web-app-788b5bb875   10        10        3       5h30m   nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb
replicaset.apps/web-app-f4488db8f    0         0         0       4h52m   nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db

###########################################################==========================================
rollout  method   2   nd   3 rd    

"2 and method"    kubectl set image  deployment.apps/web-app  nginx=nginx:1.9.1   --record=true 


hai@master:~$  kubectl set image  deployment.apps/web-app  nginx=nginx:1.9.1   --record=true
Flag --record has been deprecated, --record will be removed in the future
deployment.apps/web-app image updated
hai@master:~$ kubectl get all -o wide
NAME                          READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES
pod/web-app-f4488db8f-bs9st   1/1     Running   0          4m5s    10.36.0.2   worker1   <none>           <non  e>
pod/web-app-f4488db8f-lzjh6   1/1     Running   0          4m10s   10.36.0.6   worker1   <none>           <non  e>
pod/web-app-f4488db8f-wm965   1/1     Running   0          4m8s    10.36.0.8   worker1   <none>           <non  e>

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d12h   <none>

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR
deployment.apps/web-app   3/3     3            3           18h   nginx        nginx:1.9.1   app=nginx

NAME                                 DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR
replicaset.apps/web-app-788b5bb875   0         0         0       18h   nginx        nginx:1.7.1   app=nginx,po                                            d-template-hash=788b5bb875
replicaset.apps/web-app-f4488db8f    3         3         3       17h   nginx        "nginx:1.9.1 "  app=nginx,po                                            d-template-hash=f4488db8f

hai@master:~$ kubectl get all -o wide
NAME                           READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES
pod/web-app-788b5bb875-7d5qs   1/1     Running   0          4s    10.44.0.2   worker2   <none>           <none>
pod/web-app-788b5bb875-g2l86   1/1     Running   0          6s    10.36.0.1   worker1   <none>           <none>
pod/web-app-788b5bb875-knpsb   1/1     Running   0          7s    10.44.0.1   worker2   <none>           <none>

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d12h   <none>

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR
deployment.apps/web-app   3/3     3            3           18h   nginx        nginx:1.7.1   app=nginx

NAME                                 DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR
replicaset.apps/web-app-788b5bb875   3         3         3       18h   nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/web-app-f4488db8f    0         0         0       17h   nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f



"3rd  method"     kubectl set image  deployment.apps/web-app     

hai@master:~$ kubectl get all -o wide
NAME                          READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES
pod/web-app-f4488db8f-bs9st   1/1     Running   0          4m5s    10.36.0.2   worker1   <none>           <non  e>
pod/web-app-f4488db8f-lzjh6   1/1     Running   0          4m10s   10.36.0.6   worker1   <none>           <non  e>
pod/web-app-f4488db8f-wm965   1/1     Running   0          4m8s    10.36.0.8   worker1   <none>           <non  e>

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d12h   <none>

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR
deployment.apps/web-app   3/3     3            3           18h   nginx        nginx:1.9.1   app=nginx

NAME                                 DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR
replicaset.apps/web-app-788b5bb875   0         0         0       18h   nginx        nginx:1.7.1   app=nginx,po                                            d-template-hash=788b5bb875
replicaset.apps/web-app-f4488db8f    3         3         3       17h   nginx        "nginx:1.9.1 "  app=nginx,po                                            d-template-hash=f4488db8f

hai@master:~$ kubectl get all -o wide
NAME                           READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES
pod/web-app-788b5bb875-7d5qs   1/1     Running   0          4s    10.44.0.2   worker2   <none>           <none>
pod/web-app-788b5bb875-g2l86   1/1     Running   0          6s    10.36.0.1   worker1   <none>           <none>
pod/web-app-788b5bb875-knpsb   1/1     Running   0          7s    10.44.0.1   worker2   <none>           <none>

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d12h   <none>

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR
deployment.apps/web-app   3/3     3            3           18h   nginx        nginx:1.7.1   app=nginx

NAME                                 DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR
replicaset.apps/web-app-788b5bb875   3         3         3       18h   nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/web-app-f4488db8f    0         0         0       17h   nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f

#############################################################################################################################################################################
https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#alpha
                    deployment 

apiVersion: apps/v1
kind: Deployment
metadata: 
   name: nginxsurge
spec:
  replicas: 5
  strategy:
    rollingUpdate:
       maxSurge: 1                    ## see look 1
       maxUnavailable: 0
    type: RollingUpdate     
  selector:
    matchLabels: 
       app: nginx
  template:
    metadata:
      labels: 
        app: nginx
    spec:
      containers:
        -  name: nginx
           image: nginx:1.7.1
           ports:
             - containerPort: 80

/////////////////////////////////////////////////////////\\\\
hai@master:~$ kubectl get all --show-labels    -o wide
NAME                              READY   STATUS              RESTARTS   AGE   IP       NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/nginxsurge-788b5bb875-4gsn4   0/1     "ContainerCreating "  0          2s    <none>   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-55h47   0/1     ContainerCreating   0          2s    <none>   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-bjk8p   0/1     ContainerCreating   0          2s    <none>   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-fvltd   0/1     ContainerCreating   0          2s    <none>   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-pj5kd   0/1     "ContainerCreating "  0          2s    <none>   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d14h   <none>     component=apiserver,provider=kubernetes

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/nginxsurge   0/5     5            0           2s    nginx        nginx:1.7.1   app=nginx   <none>

NAME                                    DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/nginxsurge-788b5bb875   5         5        " 0  "     2s    nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875"
======================================================================""

hai@master:~$ kubectl get all --show-labels    -o wide
NAME                              READY   STATUS              RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/nginxsurge-788b5bb875-4gsn4   0/1     ContainerCreating   0          3s    <none>      worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-55h47   0/1     ContainerCreating   0          3s    <none>      worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-bjk8p   1/1    " Running "            0          3s    10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-fvltd   0/1     ContainerCreating   0          3s    <none>      worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-pj5kd   0/1     ContainerCreating   0          3s    <none>      worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d14h   <none>     component=apiserver,provider=kubernetes

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/nginxsurge   1/5     5            1           3s    nginx        nginx:1.7.1   app=nginx   <none>

NAME                                    DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/nginxsurge-788b5bb875   5         5        " 1 "      3s    nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875"

hai@master:~$ kubectl get all --show-labels    -o wide
NAME                              READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/nginxsurge-788b5bb875-4gsn4   1/1     Running   0          4s    10.36.0.2   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-55h47   1/1     Running   0          4s    10.44.0.2   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-bjk8p   1/1     Running   0          4s    10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-fvltd   1/1     Running   0          4s    10.44.0.3   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-pj5kd   1/1     Running   0          4s    10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d14h   <none>     component=apiserver,provider=kubernetes

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/nginxsurge   5/5     5            5           4s    nginx        nginx:1.7.1   app=nginx   <none>

NAME                                    DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/nginxsurge-788b5bb875   5         5         "5  "     4s    nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
hai@master:~$ kubectl get all --show-labels    -o wide
==+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

apiVersion: apps/v1
kind: Deployment
metadata: 
   name: nginxsurge
spec:
  replicas: 5
  strategy:
    rollingUpdate:
       maxSurge: 2                ## see look  2 
       maxUnavailable: 0
    type: RollingUpdate     
  selector:
    matchLabels: 
       app: nginx
  template:
    metadata:
      labels: 
        app: nginx
    spec:
      containers:
        -  name: nginx
           image: nginx:1.9.1    #  see look 
           ports:
             - containerPort: 80
---"             
hai@master:~$ kubectl get all --show-labels    -o wide
NAME                              READY   STATUS              RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/nginxsurge-788b5bb875-4gsn4   1/1     Running             0          32m   10.36.0.2   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-55h47   1/1     Running             0          32m   10.44.0.2   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-bjk8p   1/1     Running             0          32m   10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-fvltd   1/1     Running             0          32m   10.44.0.3   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-pj5kd   1/1     Running             0          32m   10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
"pod/nginxsurge-f4488db8f-k7d7v    0/1     ContainerCreating"   0          1s    <none>      worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
"pod/nginxsurge-f4488db8f-xczlt    0/1     ContainerCreating"   0          1s    <none>      worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d14h   <none>     component=apiserver,provider=kubernetes

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/nginxsurge   5/5    " 2  "          5           32m   nginx        nginx:1.9.1   app=nginx   <none>

NAME                                    DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/nginxsurge-788b5bb875   5         5         5       32m   nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/nginxsurge-f4488db8f    2 "        2         0       1s    nginx        nginx:1.9.1  " app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f
----==== "
hai@master:~$ kubectl get all --show-labels    -o wide
NAME                              READY   STATUS              RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/nginxsurge-788b5bb875-4gsn4   1/1     Running             0          32m   10.36.0.2   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-55h47   1/1     Running             0          32m   10.44.0.2   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-bjk8p   1/1     Running             0          32m   10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
"pod/nginxsurge-788b5bb875-fvltd   1/1     Terminating  "       0          32m   10.44.0.3   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
"pod/nginxsurge-788b5bb875-pj5kd   1/1     Terminating "        0          32m   10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-f4488db8f-k7d7v    1/1     Running             0          2s    10.36.0.3   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-w45lj    0/1     Con"tainerCrea"ting   0          0s    <none>      worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-xczlt    1/1     Running             0          2s    10.44.0.4   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-xxmtg    0/1     Con"tainerCrea"ting   0          0s    <none>      worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d14h   <none>     component=apiserver,provider=kubernetes

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/nginxsurge   5/5    " 4 "           5           32m   nginx        nginx:1.9.1   app=nginx   <none>

NAME                                    DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/nginxsurge-788b5bb875   3         3         3       32m   nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/nginxsurge-f4488db8f   " 4         4         2       2s    nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f  "  app=nginx,pod-template-hash=f4488db8f
=====------"

hai@master:~$ kubectl get all --show-labels    -o wide
NAME                              READY   STATUS              RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/nginxsurge-788b5bb875-4gsn4   1/1     Running             0          32m   10.36.0.2   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-55h47   1/1     Running             0          32m   10.44.0.2   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-bjk8p   1/1     Running             0          32m   10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-pj5kd   1/1     "Terminating "        0          32m   10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-f4488db8f-k7d7v    1/1     Running             0          4s    10.36.0.3   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-w45lj    0/1     ContainerCreating   0          2s    <none>      worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-xczlt    1/1     Running             0          4s    10.44.0.4   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-xxmtg    0/1     ContainerCreating   0          2s    <none>      worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d14h   <none>     component=apiserver,provider=kubernetes

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/nginxsurge   5/5     4            5           32m   nginx        nginx:1.9.1   app=nginx   <none>

NAME                                    DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/nginxsurge-788b5bb875   3         3         3       32m   nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/nginxsurge-f4488db8f    4         4         2       4s    nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f
======================================================="
hai@master:~$ kubectl get all --show-labels    -o wide
NAME                              READY   STATUS              RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/nginxsurge-788b5bb875-4gsn4   1/1     Running             0          32m   10.36.0.2   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-55h47   1/1     "Terminating"         0          32m   10.44.0.2   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-bjk8p   1/1     "Terminating "        0          32m   10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-f4488db8f-k7d7v    1/1     Running             0          4s    10.36.0.3   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-r969w    0/1     ContainerCreating   0          0s    <none>      worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-w45lj    1/1     Running             0          2s    10.36.0.4   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-xczlt    1/1     Running             0          4s    10.44.0.4   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-xxmtg    1/1     Running             0          2s    10.44.0.3   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d14h   <none>     component=apiserver,provider=kubernetes

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/nginxsurge   5/5     5            5           32m   nginx        nginx:1.9.1   app=nginx   <none>

NAME                                    DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/nginxsurge-788b5bb875   1         1         1       32m   nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/nginxsurge-f4488db8f    5         5         4       4s    nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f

================================================== "
hai@master:~$ kubectl get all --show-labels    -o wide
NAME                              READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/nginxsurge-788b5bb875-4gsn4   1/1     Running   0          32m   10.36.0.2   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-f4488db8f-k7d7v    1/1     Running   0          6s    10.36.0.3   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-r969w    1/1     Running   0          2s    10.44.0.2   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-w45lj    1/1     Running   0          4s    10.36.0.4   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-xczlt    1/1     Running   0          6s    10.44.0.4   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-xxmtg    1/1     Running   0          4s    10.44.0.3   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d14h   <none>     component=apiserver,provider=kubernetes

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/nginxsurge   6/5     5            6           32m   nginx        nginx:1.9.1   app=nginx   <none>

NAME                                    DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/nginxsurge-788b5bb875   0         1         1       32m   nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/nginxsurge-f4488db8f    5         5         5       6s    nginx        "nginx:1.9.1 "  app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f

##################################################################### "
apiVersion: apps/v1
kind: Deployment
metadata: 
   name: nginxsurge
spec:
  replicas: 5
  strategy:
    rollingUpdate:
       maxSurge: 0            ## see look       
       maxUnavailable: 5     ## see   look  
    type: RollingUpdate     
  selector:
    matchLabels: 
       app: nginx
  template:
    metadata:
      labels: 
        app: nginx
    spec:
      containers:
        -  name: nginx
           image: nginx:1.7.1   # see look 
           ports:
             - containerPort: 80
"
hai@master:~$ kubectl get all --show-labels    -o wide
NAME                              READY   STATUS              RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/nginxsurge-788b5bb875-qxgvj   1/1    " Terminating "        0          2m28s   10.44.0.5   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-r8qsj   1/1     "Terminating"         0          2m28s   10.36.0.5   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-tgb9t   1/1     "Terminating  "       0          2m28s   10.36.0.2   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-tv7d8   1/1    " Terminating   "      0          2m28s   10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-xvsf9   1/1     "Terminating "        0          2m28s   10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-f4488db8f-96l9k    0/1     ContainerCreating   0          1s      <none>      worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-bxcsf    0/1     ContainerCreating   0          1s      <none>      worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-grcpq    0/1     ContainerCreating   0          1s      <none>      worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-jcn62    0/1     ContainerCreating   0          1s      <none>      worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-qctxm    0/1     ContainerCreating   0          1s      <none>      worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d16h   <none>     component=apiserver,provider=kubernetes

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE    CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/nginxsurge   0/5     5            0           123m   nginx        nginx:1.9.1   app=nginx   <none>

NAME                                    DESIRED   CURRENT   READY   AGE    CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/nginxsurge-788b5bb875   0         0         0       123m   nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/nginxsurge-f4488db8f    "5         5         0   "    91m    nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f
=/////////////////////\\\\\\\\\\\\\\\\\\\\\\\\\\\\///////\\\\\\\\\\\\\\\\\\\\\""

hai@master:~$ kubectl get all --show-labels    -o wide
NAME                              READY   STATUS              RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/nginxsurge-788b5bb875-r8qsj   1/1     Terminating         0          2m29s   10.36.0.5   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-tgb9t   1/1     Terminating         0          2m29s   10.36.0.2   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-xvsf9   1/1     Terminating         0          2m29s   10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-f4488db8f-96l9k    0/1     "ContainerCreating "  0          2s      <none>      worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-bxcsf    0/1     "ContainerCreating "  0          2s      <none>      worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-grcpq    0/1     "ContainerCreating "  0          2s      <none>      worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-jcn62    0/1    " ContainerCreating  " 0          2s      <none>      worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-qctxm    0/1     "ContainerCreating "  0          2s      <none>      worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d16h   <none>     component=apiserver,provider=kubernetes

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE    CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/nginxsurge   0/5     5            0           123m   nginx        nginx:1.9.1   app=nginx   <none>

NAME                                    DESIRED   CURRENT   READY   AGE    CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/nginxsurge-788b5bb875   0         0         0       123m   nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/nginxsurge-f4488db8f  "  5         5         0  "     91m    nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f
////////////////////\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\///////////////////////\\\\\\\\\\\\\\\\\\"""

hai@master:~$ kubectl get all --show-labels    -o wide
NAME                             READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/nginxsurge-f4488db8f-96l9k   1/1     Running   0          5s    10.44.0.3   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-bxcsf   1/1     Running   0          5s    10.44.0.4   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-grcpq   1/1     Running   0          5s    10.36.0.4   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-jcn62   1/1     Running   0          5s    10.36.0.3   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-qctxm   1/1     Running   0          5s    10.44.0.2   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d16h   <none>     component=apiserver,provider=kubernetes

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE    CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/nginxsurge   5/5     5            5           123m   nginx        nginx:1.9.1   app=nginx   <none>

NAME                                    DESIRED   CURRENT   READY   AGE    CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/nginxsurge-788b5bb875   0         0         0       123m   nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/nginxsurge-f4488db8f   " 5         5         5  "     91m    nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++////////////////\\\\\\\\\""
apiVersion: apps/v1
kind: Deployment
metadata: 
   name: nginxsurge
spec:
  replicas: 5
  strategy:
    rollingUpdate:
       maxSurge: 62%           # see look 
       maxUnavailable:        # see look 
    type: RollingUpdate     
  selector:
    matchLabels: 
       app: nginx
  template:
    metadata:
      labels: 
        app: nginx
    spec:
      containers:
        -  name: nginx
           image: nginx:1.7.1
           ports:
             - containerPort: 80
"
hai@master:~$ kubectl get all --show-labels    -o wide
NAME                             READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/nginxsurge-f4488db8f-96l9k   1/1     Running   0          16m   10.44.0.3   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-bxcsf   1/1     Running   0          16m   10.44.0.4   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-grcpq   1/1     Running   0          16m   10.36.0.4   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-jcn62   1/1     Running   0          16m   10.36.0.3   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-qctxm   1/1     Running   0          16m   10.44.0.2   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d16h   <none>     component=apiserver,provider=kubernetes

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE    CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/nginxsurge   5/5     5            5           140m   nginx        nginx:1.9.1   app=nginx   <none>

NAME                                    DESIRED   CURRENT   READY   AGE    CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/nginxsurge-788b5bb875   0         0         0       140m   nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/nginxsurge-f4488db8f    5         5         5       107m   nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f"
///////////////////////\\\\\\\\\\\\\/////////\\\\\\\/\\\\\\\\/\\\\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\\/\/\/\\/
hai@master:~$ kubectl get all --show-labels    -o wide
NAME                              READY   STATUS              RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/nginxsurge-788b5bb875-2n65k   0/1     "ContainerCreating"   0          1s    <none>      worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-4w6jp   0/1     "ContainerCreating"   0          1s    <none>      worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-9kph7   0/1     "ContainerCreating "  0          1s    <none>      worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-dz598   0/1    " ContainerCreating"   0          1s    <none>      worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-kfn6n   0/1    " ContainerCreating"   0          1s    <none>      worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-f4488db8f-96l9k    1/1     Running             0          16m   10.44.0.3   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-bxcsf    1/1     Running             0          16m   10.44.0.4   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-grcpq    1/1     Running             0          16m   10.36.0.4   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-jcn62    1/1     Running             0          16m   10.36.0.3   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-qctxm    1/1     "Terminating  "       0          16m   10.44.0.2   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d16h   <none>     component=apiserver,provider=kubernetes

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE    CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/nginxsurge   4/5     5            4           140m   nginx        nginx:1.7.1   app=nginx   <none>

NAME                                    DESIRED   CURRENT   READY   AGE    CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/nginxsurge-788b5bb875   5         5         0       140m   nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/nginxsurge-f4488db8f    4         4         4       108m   nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f"
######\\\\\\\\\\\\\\\\\\\\\\\\\\\/////////////////////////////////////////////

hai@master:~$ kubectl get all --show-labels    -o wide
NAME                              READY   STATUS              RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/nginxsurge-788b5bb875-2n65k   1/1     Running             0          4s    10.36.0.5   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-4w6jp   0/1     ContainerCreating   0          4s    <none>      worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-9kph7   1/1     Running             0          4s    10.36.0.2   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-dz598   1/1     Running             0          4s    10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-kfn6n   0/1     ContainerCreating   0          4s    <none>      worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-f4488db8f-96l9k    1/1     Running             0          16m   10.44.0.3   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-bxcsf    1/1     Running             0          16m   10.44.0.4   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/"nginxsurge-f4488db8f-grcpq    1/1     Terminating   "      0          16m   10.36.0.4   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-jcn62    1/1     Running             0          16m   10.36.0.3   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d16h   <none>     component=apiserver,provider=kubernetes

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE    CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/nginxsurge   6/5     5            6           140m   nginx        nginx:1.7.1   app=nginx   <none>

NAME                                    DESIRED   CURRENT   READY   AGE    CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/nginxsurge-788b5bb875   5         5         3       140m   nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/nginxsurge-f4488db8f    1         3         3       108m   nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f
=///////////////////////////\\\\\\\\\\\\\\\\\\\\\\\\\\\/////////\\/\/\/\/\\\/\\/\/\\/\/\\/\/

hai@master:~$ kubectl get all --show-labels    -o wide
NAME                              READY   STATUS        RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/nginxsurge-788b5bb875-2n65k   1/1     Running       0          5s    10.36.0.5   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-4w6jp   1/1     Running       0          5s    10.44.0.5   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-9kph7   1/1     Running       0          5s    10.36.0.2   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-dz598   1/1     Running       0          5s    10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-kfn6n   1/1     Running       0          5s    10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-f4488db8f-96l9k    1/1     Terminating   0          16m   10.44.0.3   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-bxcsf    1/1     Terminating   0          16m   10.44.0.4   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-grcpq    1/1     Terminating   0          16m   10.36.0.4   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-jcn62    1/1     Terminating   0          16m   10.36.0.3   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d16h   <none>     component=apiserver,provider=kubernetes

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE    CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/nginxsurge   5/5     5            5           140m   nginx        nginx:1.7.1   app=nginx   <none>

NAME                                    DESIRED   CURRENT   READY   AGE    CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/nginxsurge-788b5bb875   5         5         5       140m   nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/nginxsurge-f4488db8f    0         0         0       108m   nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f"
=======================================================================================
hai@master:~$ kubectl get all --show-labels    -o wide
NAME                              READY   STATUS        RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/nginxsurge-788b5bb875-2n65k   1/1     Running       0          6s    10.36.0.5   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-4w6jp   1/1     Running       0          6s    10.44.0.5   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-9kph7   1/1     Running       0          6s    10.36.0.2   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-dz598   1/1     Running       0          6s    10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-kfn6n   1/1     Running       0          6s    10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-f4488db8f-jcn62    1/1     Terminating   0          16m   10.36.0.3   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d16h   <none>     component=apiserver,provider=kubernetes

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE    CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/nginxsurge   5/5     5            5           140m   nginx        nginx:1.7.1   app=nginx   <none>

NAME                                    DESIRED   CURRENT   READY   AGE    CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/nginxsurge-788b5bb875   5         5         5       140m   nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/nginxsurge-f4488db8f    0         0         0       108m   nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f
####################################################################################################################################
                                     RECREATE DEPLOYMENT 
apiVersion: apps/v1
kind: Deployment
metadata: 
   name: nginxsurge
spec:
  replicas: 5
  strategy:
    
    type:  Recreate       ## look see  importent 
  selector:
    matchLabels: 
       app: nginx
  template:
    metadata:
      labels: 
        app: nginx
    spec:
      containers:
        -  name: nginx
           image: nginx:1.7.1
           ports:
             - containerPort: 80

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
hai@master:~$ kubectl get all --show-labels    -o wide
NAME                             READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/nginxsurge-f4488db8f-6tntc   1/1     Running   0          64s   10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-8mnz4   1/1     Running   0          64s   10.36.0.2   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-mjhxl   1/1     Running   0          64s   10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-qw9p8   1/1     Running   0          64s   10.44.0.3   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-wqxt7   1/1     Running   0          64s   10.44.0.2   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d17h   <none>     component=apiserver,provider=kubernetes

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/nginxsurge   5/5     5            5           11m   nginx        nginx:1.9.1   app=nginx   <none>

NAME                                    DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/nginxsurge-788b5bb875   0         0         0       11m     nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/nginxsurge-f4488db8f    5         5         5       7m15s   nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f"
==\\\\\\\\\\\\\\\\\\\\\\\\\\\/////////////////////////\\\\\\\\\\\\\\\\\\\

hai@master:~$ kubectl get all --show-labels    -o wide
NAME                             READY   STATUS        RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/nginxsurge-f4488db8f-6tntc   1/1     "erminating"'   0          92s   10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-8mnz4   1/1     "Terminating"   0          92s   10.36.0.2   worker1   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-mjhxl   1/1     "Terminating "  0          92s   10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-qw9p8   1/1     Terminating   0          92s   10.44.0.3   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-wqxt7   1/1     "Terminating "  0          92s   10.44.0.2   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d17h   <none>     component=apiserver,provider=kubernetes

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/nginxsurge   0/5     0            0           11m   nginx        nginx:1.7.1   app=nginx   <none>

NAME                                    DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/nginxsurge-788b5bb875   0         0         0       11m     nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/nginxsurge-f4488db8f    0         0         0       7m43s   nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f"
\\\\\\\\\\\\\|||\/\/\/\/\/\//\\////////////\\\\\\\\\\\\\\/\//\/\/\/\\\\\\\/////"

hai@master:~$ kubectl get all --show-labels    -o wide
NAME                             READY   STATUS        RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/nginxsurge-f4488db8f-mjhxl   0/1     "Terminating "  0          92s   10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-qw9p8   1/1     "Terminating "  0          92s   10.44.0.3   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f
pod/nginxsurge-f4488db8f-wqxt7   0/1     "Terminating "  0          92s   10.44.0.2   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d17h   <none>     component=apiserver,provider=kubernetes

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/nginxsurge   0/5     0            0           11m   nginx        nginx:1.7.1   app=nginx   <none>

NAME                                    DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/nginxsurge-788b5bb875   0         0         0       11m     nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/nginxsurge-f4488db8f    0         0         0       7m43s   nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f"
==#####\\\\\\\\\\\\\\\\\\\\\///////////\\\\\\\\\\\/\\\\\\\\\\////////////\\\\

hai@master:~$ kubectl get all --show-labels    -o wide
NAME                             READY   STATUS        RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
"pod/nginxsurge-f4488db8f-qw9p8   1/1     Terminating "  0          93s   10.44.0.3   worker2   <none>           <none>            app=nginx,pod-template-hash=f4488db8f

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d17h   <none>     component=apiserver,provider=kubernetes

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/nginxsurge   0/5     0            0           11m   nginx        nginx:1.7.1   app=nginx   <none>

NAME                                    DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/nginxsurge-788b5bb875   0         0         0       11m     nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/nginxsurge-f4488db8f    0         0         0       7m44s   nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f"
=============\\\\\\\\\\\\\\\\\\\\\\////////////////////////\\\\\\\\\\////////
 
hai@master:~$ kubectl get all --show-labels    -o wide
NAME                              READY   STATUS              RESTARTS   AGE   IP       NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/nginxsurge-788b5bb875-4jqzq   0/1     ContainerCreating   0          2s    <none>   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-ck9tr   0/1     ContainerCreating   0          2s    <none>   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-fxflw   0/1     ContainerCreating   0          2s    <none>   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-qvkxl   0/1     ContainerCreating   0          2s    <none>   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-sdlgd   0/1     ContainerCreating   0          2s    <none>   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d17h   <none>     component=apiserver,provider=kubernetes

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/nginxsurge   0/5     5            0           11m   nginx        nginx:1.7.1   app=nginx   <none>

NAME                                    DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/nginxsurge-788b5bb875   5         5         0       11m     nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/nginxsurge-f4488db8f    0         0         0       7m46s   nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f"
\\\\\\\\/\//////////\\\\\\\\\\\\\/\\\\\\\\//////////////\/////\\\\\\\\\\\/\
hai@master:~$ kubectl get all --show-labels    -o wide
NAME                              READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/nginxsurge-788b5bb875-4jqzq   1/1     Running   0          7s    10.44.0.1   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-ck9tr   1/1     Running   0          7s    10.36.0.2   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-fxflw   1/1     Running   0          7s    10.44.0.3   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-qvkxl   1/1     Running   0          7s    10.36.0.1   worker1   <none>           <none>            app=nginx,pod-template-hash=788b5bb875
pod/nginxsurge-788b5bb875-sdlgd   1/1     Running   0          7s    10.44.0.2   worker2   <none>           <none>            app=nginx,pod-template-hash=788b5bb875

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d17h   <none>     component=apiserver,provider=kubernetes

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR    LABELS
deployment.apps/nginxsurge   5/5     5            5           12m   nginx        nginx:1.7.1   app=nginx   <none>

NAME                                    DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES        SELECTOR                                 LABELS
replicaset.apps/nginxsurge-788b5bb875   5         5         5       12m     nginx        nginx:1.7.1   app=nginx,pod-template-hash=788b5bb875   app=nginx,pod-template-hash=788b5bb875
replicaset.apps/nginxsurge-f4488db8f    0         0         0       7m51s   nginx        nginx:1.9.1   app=nginx,pod-template-hash=f4488db8f    app=nginx,pod-template-hash=f4488db8f
#############################################################################################################################################"
                                                  SERVICES 
1. CLUSTER IP                            2. NODEPORT                     3. LOADBALANCER                         4. EXTRENALNAME

apiVersion: apps/v1
kind: Deployment
metadata:
   name: webserver
   labels: 
      app: webserver
spec:
   replicas: 1
   strategy:
      type: Recreate    # see  look  
   selector: 
      matchLabels:
        app: webserver
   template:
      metadata:
         labels:
            app: webserver
      spec:
        containers:
          - name: nginx
            image: nginx:1.7.1
            ports:
              - containerPort: 80  
----"                                
hai@master:~$ kubectl get all --show-labels -o wide
NAME                             READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/"webserver-744d6b7964-vzg2t   1/1     Running "  0          32s   10.44.0.1   worker2   <none>           <none>            app=webserver,pod-template-hash=744d6b7964

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d20h   <none>     component=apiserver,provider=kubernetes

NAME                        READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR        LABELS
deployment.apps/webserver   1/1     1            1           32s   nginx        nginx:1.7.1   app=webserver   app=webserver

NAME                                   DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                     LABELS
replicaset.apps/webserver-744d6b7964   1         1         1       32s   nginx        "nginx:1.7.1"   app=webserver,pod-template-hash=744d6b7964   app=webserver,pod-template-hash=744d6b7964"

apiVersion: apps/v1
kind: Deployment
metadata:
   name: dbserver
   labels: 
      app: dbserver
spec:
   replicas: 1
   strategy:
      type: Recreate
   selector: 
      matchLabels:
        app: dbserver
   template:
      metadata:
         labels:
            app: dbserver
      spec:
        containers:
          - name: nginx
            image: nginx:1.7.1
            ports:
              - containerPort: 80                    
--"
hai@master:~$ kubectl get all --show-labels -o wide
NAME                             READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
"pod/dbserver-5b844594d8-h67hh "   1/1     Running   0          7s    10.36.0.1   worker1   <none>           <none>            app=dbserver,pod-template-hash=5b844594d8
"pod/webserver-744d6b7964-vzg2t "  1/1     Running   0          11m   10.44.0.1   worker2   <none>           <none>            app=webserver,pod-template-hash=744d6b7964

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d20h   <none>     component=apiserver,provider=kubernetes

NAME                        READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR        LABELS
deployment.apps/dbserver    1/1     1            1           7s    nginx        nginx:1.7.1   app=dbserver    app=dbserver
deployment.apps/webserver   1/1     1            1           11m   nginx        nginx:1.7.1   app=webserver   app=webserver

NAME                                   DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                     LABELS
replicaset.apps/dbserver-5b844594d8    1         1         1       7s    nginx        nginx:1.7.1   app=dbserver,pod-template-hash=5b844594d8    app=dbserver,pod-template-hash=5b844594d8
replicaset.apps/webserver-744d6b7964   1         1         1       11m   nginx        nginx:1.7.1   app=webserver,pod-template-hash=744d6b7964   app=webserver,pod-template-hash=744d6b7964"
===================/////////////////////////////
hai@master:~$ kubectl get all  --show-labels  -o wide
NAME                             READY   STATUS    RESTARTS   AGE    IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/dbserver-5b844594d8-h67hh    1/1     Running   0          101m   10.36.0.1   worker1   <none>           <none>            app=dbserver,pod-template-hash=5b844594d8
pod/webserver-744d6b7964-vzg2t   1/1     Running   0          112m   10.44.0.1   worker2   <none>           <none>            app=webserver,pod-template-hash=744d6b7964

"NAME      "           TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d22h   <none>     component=apiserver,provider=kubernetes

NAME                        READY   UP-TO-DATE   AVAILABLE   AGE    CONTAINERS   IMAGES        SELECTOR        LABELS
deployment.apps/dbserver    1/1     1            1           101m   nginx        nginx:1.7.1   app=dbserver    app=dbserver
deployment.apps/webserver   1/1     1            1           112m   nginx        nginx:1.7.1   app=webserver   app=webserver

NAME                                   DESIRED   CURRENT   READY   AGE    CONTAINERS   IMAGES        SELECTOR                                     LABELS
replicaset.apps/dbserver-5b844594d8    1         1         1       101m   nginx        nginx:1.7.1   app=dbserver,pod-template-hash=5b844594d8    app=dbserver,pod-template-hash=5b844594d8
replicaset.apps/webserver-744d6b7964   1         1         1       112m   nginx        nginx:1.7.1   app=webserver,pod-template-hash=744d6b7964   app=webserver,pod-template-hash=744d6b7964
#+++++++++++++++++++++++++++++++++++++++++++++++
https://cloud.google.com/kubernetes-engine/docs/how-to/exposing-apps

kubectl expose deployment dbserver --type=ClusterIP  --type=ClusterIP  --port  80  --target-port  8085

kubectl expose deployment my-deployment --name my-cip-service --type ClusterIP --protocol TCP --port 80 --target-port 8080 "




hai@master:~$ kubectl get all  --show-labels  -o wide
NAME                             READY   STATUS    RESTARTS   AGE    IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/dbserver-5b844594d8-h67hh    1/1     Running   0          133m   10.36.0.1   worker1   <none>           <none>            app=dbserver,pod-template-hash=5b844594d8
pod/webserver-744d6b7964-vzg2t   1/1     Running   0          144m   10.44.0.1   worker2   <none>           <none>            app=webserver,pod-template-hash=744d6b7964

NAME                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE     SELECTOR       LABELS
"service/dbserver     ClusterIP   10.98.65.253 "  <none>        80/TCP    2s      app=dbserver   app=dbserver
service/kubernetes   ClusterIP   10.96.0.1      <none>        443/TCP   4d22h   <none>         component=apiserver,provider=kubernetes

NAME                        READY   UP-TO-DATE   AVAILABLE   AGE    CONTAINERS   IMAGES        SELECTOR        LABELS
deployment.apps/dbserver    1/1     1            1           133m   nginx        nginx:1.7.1   app=dbserver    app=dbserver
deployment.apps/webserver   1/1     1            1           144m   nginx        nginx:1.7.1   app=webserver   app=webserver

NAME                                   DESIRED   CURRENT   READY   AGE    CONTAINERS   IMAGES        SELECTOR                                     LABELS
replicaset.apps/dbserver-5b844594d8    1         1         1       133m   nginx        nginx:1.7.1   app=dbserver,pod-template-hash=5b844594d8    app=dbserver,pod-template-hash=5b844594d8
replicaset.apps/webserver-744d6b7964   1         1         1       144m   nginx        nginx:1.7.1   app=webserver,pod-template-hash=744d6b7964   app=webserver,pod-template-hash=744d6b7964"
///////////////////
kubectl expose deployment dbserver --type=ClusterIP  --type=ClusterIP  --port  80  --target-port  8085


hai@master:~/test$ kubectl describe service dbserver
Name:              dbserver
Namespace:         default
Labels:            app=dbserver
Annotations:       <none>
Selector:          app=dbserver
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:               " 10.98.65.253 "   #  look see 
IPs:               10.98.65.253
Port:              <unset>  80/TCP
TargetPort:        8085/TCP
Endpoints:         10.36.0.1:8085
Session Affinity:  None
Events:            <none>"
--===========
apiVersion: apps/v1
kind: Deployment
metadata:
   name: dbserver
   labels: 
      app: dbserver
spec:
   replicas: 4    "  # SEE LOOK "
   strategy:
      type: Recreate
   selector: 
      matchLabels:
        app: dbserver
   template:
      metadata:
         labels:
            app: dbserver
      spec:
        containers:
          - name: nginx
            image: nginx:1.7.1
            ports:
              - containerPort: 80                    
++++++++++++++++++++++++++++++++++++++++++++++++++:"
hai@master:~$ kubectl get all  --show-labels  -o wide
NAME                             READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/dbserver-5b844594d8-h67hh    1/1     Running   0          3h      10.36.0.1   worker1   <none>           <none>            app=dbserver,pod-template-hash=5b844594d8
pod/dbserver-5b844594d8-kts25    1/1     Running   0          9s      10.36.0.2   worker1   <none>           <none>            app=dbserver,pod-template-hash=5b844594d8
pod/dbserver-5b844594d8-qxvz2    1/1     Running   0          9s      10.44.0.2   worker2   <none>           <none>            app=dbserver,pod-template-hash=5b844594d8
pod/dbserver-5b844594d8-zhvfc    1/1     Running   0          9s      10.44.0.3   worker2   <none>           <none>            app=dbserver,pod-template-hash=5b844594d8
pod/webserver-744d6b7964-vzg2t   1/1     Running   0          3h12m   10.44.0.1   worker2   <none>           <none>            app=webserver,pod-template-hash=744d6b7964

NAME                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE     SELECTOR       LABELS
service/dbserver     ClusterIP   10.98.65.253   <none>        80/TCP    47m     app=dbserver   app=dbserver
service/kubernetes   ClusterIP   10.96.0.1      <none>        443/TCP   4d23h   <none>         component=apiserver,provider=kubernetes

NAME                        READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS   IMAGES        SELECTOR        LABELS
deployment.apps/dbserver    4/4     4            4           3h      nginx        nginx:1.7.1   app=dbserver    app=dbserver
deployment.apps/webserver   1/1     1            1           3h12m   nginx        nginx:1.7.1   app=webserver   app=webserver

NAME                                   DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES        SELECTOR                                     LABELS
replicaset.apps/dbserver-5b844594d8    4         4         4       3h      nginx        nginx:1.7.1   app=dbserver,pod-template-hash=5b844594d8    app=dbserver,pod-template-hash=5b844594d8
replicaset.apps/webserver-744d6b7964   1         1         1       3h12m   nginx        nginx:1.7.1   app=webserver,pod-template-hash=744d6b7964   app=webserver,pod-template-hash=744d6b7964"
=======================
hai@master:~$ kubectl get svc -o wide
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE     SELECTOR
"dbserver "    ClusterIP   "10.98.65.253 "  <none>        80/TCP    55m     app=dbserver
kubernetes   ClusterIP   10.96.0.1      <none>        443/TCP   4d23h   <none>"

hai@master:~$ kubectl describe  service  dbserver
Name:              dbserver
Namespace:         default
Labels:            app=dbserver
Annotations:       <none>
Selector:          app=dbserver
Type:             " ClusterIP"
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.98.65.253    " # see look "
IPs:               10.98.65.253
Port:              <unset>  80/TCP
TargetPort:        8085/TCP
Endpoints:        " 10.36.0.1:8085,10.36.0.2:8085,10.44.0.2:8085 + 1 more..."
Session Affinity:  None
Events:            <none>
============================================================================="

hai@master:~/test$ kubectl  expose deployment  webserver  --type=ClusterIP
service/webserver exposed

hai@master:~$ kubectl get all  --show-labels  -o wide
NAME                             READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/dbserver-5b844594d8-h67hh    1/1     Running   0          3h20m   10.36.0.1   worker1   <none>           <none>            app=dbserver,pod-template-hash=5b844594d8
pod/dbserver-5b844594d8-kts25    1/1     Running   0          20m     10.36.0.2   worker1   <none>           <none>            app=dbserver,pod-template-hash=5b844594d8
pod/dbserver-5b844594d8-qxvz2    1/1     Running   0          20m     10.44.0.2   worker2   <none>           <none>            app=dbserver,pod-template-hash=5b844594d8
pod/dbserver-5b844594d8-zhvfc    1/1     Running   0          20m     10.44.0.3   worker2   <none>           <none>            app=dbserver,pod-template-hash=5b844594d8
pod/webserver-744d6b7964-vzg2t   1/1     Running   0          3h32m   10.44.0.1   worker2   <none>           <none>            app=webserver,pod-template-hash=744d6b7964

NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE     SELECTOR        LABELS
service/dbserver     ClusterIP   10.98.65.253     <none>        80/TCP    67m     app=dbserver    app=dbserver
service/kubernetes   ClusterIP   10.96.0.1        <none>        443/TCP   4d23h   <none>          component=apiserver,provider=kubernetes
service/webserver    ClusterIP  " 10.111.157.160  " <none>       " 80/TCP "   112s    app=webserver   app=webserver

NAME                        READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS   IMAGES        SELECTOR        LABELS
deployment.apps/dbserver    4/4     4            4           3h20m   nginx        nginx:1.7.1   app=dbserver    app=dbserver
deployment.apps/webserver   1/1     1            1           3h32m   nginx        nginx:1.7.1   app=webserver   app=webserver

NAME                                   DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES        SELECTOR                                     LABELS
replicaset.apps/dbserver-5b844594d8    4         4         4       3h20m   nginx        nginx:1.7.1   app=dbserver,pod-template-hash=5b844594d8    app=dbserver,pod-template-hash=5b844594d8
replicaset.apps/webserver-744d6b7964   1         1         1       3h32m   nginx        nginx:1.7.1   app=webserver,pod-template-hash=744d6b7964   app=webserver,pod-template-hash=744d6b7964"

hai@master:~/test$ kubectl describe service webserver
Name:              webserver
Namespace:         default
Labels:            app=webserver
Annotations:       <none>
Selector:          app=webserver
Type:             " ClusterIP"
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                "10.111.157.160"
IPs:               10.111.157.160
Port:              <unset>  80/TCP
TargetPort:        80/TCP
Endpoints:         10.44.0.1:80
Session Affinity:  None
Events:            <none>
+++++++++++++++++++++++++++++++++
https://kubernetes.io/docs/concepts/services-networking/service/

apiVersion: apps/v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: webserver
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
++++++++++++++++++++"
hai@master:~$ kubectl get all  --show-labels  -o wide
NAME                             READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/dbserver-5b844594d8-h67hh    1/1     Running   0          3h41m   10.36.0.1   worker1   <none>           <none>            app=dbserver,pod-template-hash=5b844594d8
pod/dbserver-5b844594d8-kts25    1/1     Running   0          41m     10.36.0.2   worker1   <none>           <none>            app=dbserver,pod-template-hash=5b844594d8
pod/dbserver-5b844594d8-qxvz2    1/1     Running   0          41m     10.44.0.2   worker2   <none>           <none>            app=dbserver,pod-template-hash=5b844594d8
pod/dbserver-5b844594d8-zhvfc    1/1     Running   0          41m     10.44.0.3   worker2   <none>           <none>            app=dbserver,pod-template-hash=5b844594d8
pod/"webserver-744d6b7964-vzg2t   1/1     Running   0          3h53m   10.44.0.1 "  worker2   <none>           <none>            app=webserver,pod-template-hash=744d6b7964

NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE   SELECTOR        LABELS
service/dbserver     ClusterIP   10.98.65.253     <none>        80/TCP    88m   app=dbserver    app=dbserver
service/kubernetes   ClusterIP   10.96.0.1        <none>        443/TCP   5d    <none>          component=apiserver,provider=kubernetes
service/"my-service   ClusterIP   10.96.213.135"    <none>        80/TCP    4s    app=webserver   <none>
service/webserver    ClusterIP   10.111.157.160   <none>        80/TCP    22m   app=webserver   app=webserver

NAME                        READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS   IMAGES        SELECTOR        LABELS
deployment.apps/dbserver    4/4     4            4           3h41m   nginx        nginx:1.7.1   app=dbserver    app=dbserver
deployment.apps/webserver   1/1     1            1           3h53m   nginx        nginx:1.7.1   app=webserver   app=webserver

NAME                                   DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES        SELECTOR                                     LABELS
replicaset.apps/dbserver-5b844594d8    4         4         4       3h41m   nginx        nginx:1.7.1   app=dbserver,pod-template-hash=5b844594d8    app=dbserver,pod-template-hash=5b844594d8
replicaset.apps/webserver-744d6b7964   1         1         1       3h53m   nginx        nginx:1.7.1   app=webserver,pod-template-hash=744d6b7964   app=webserver,pod-template-hash=744d6b7964"

hai@master:~$ kubectl describe service my-service
Name:              my-service
Namespace:         default
Labels:            <none>
Annotations:       <none>
Selector:          app=webserver
Type:             " ClusterIP"
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:               " 10.96.213.135"
IPs:               10.96.213.135
Port:              <unset>  80/TCP
TargetPort:        9376/TCP
Endpoints:         "10.44.0.1:9376"
Session Affinity:  None
Events:            <none>
#####################################################################################"

hai@master:~$ kubectl get all  --show-labels  -o wide
NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE     SELECTOR        LABELS
service/"kubernetes"   ClusterIP   10.96.0.1        <none>        443/TCP   5d      <none>          "component=apiserver,provider=kubernetes""


hai@master:~$ kubectl get namespaces  --show-labels  -o  wide
NAME              STATUS   AGE    LABELS
default           Active   5d     kubernetes.io/metadata.name=default
kube-node-lease   Active   5d     kubernetes.io/metadata.name=kube-node-lease
kube-public       Active   5d     kubernetes.io/metadata.name=kube-public
"kube-system       Active   5d     kubernetes.io/metadata.name=kube-system"
project1          Active   4d9h   kubernetes.io/metadata.name=project1
project2          Active   4d9h   kubernetes.io/metadata.name=project2
----------------"
hai@master:~$  kubectl get pod -n kube-system  --show-labels   -o wide
NAME                             READY   STATUS    RESTARTS          AGE     IP               NODE      NOMINATED NODE   READINESS GATES   LABELS
coredns-565d847f94-q5zw9         1/1     Running   465 (5h10m ago)   4d7h    10.32.0.3        master    <none>           <none>            k8s-app=kube-dns,pod-template-hash=565d847f94
coredns-565d847f94-rg789         1/1     Running   454 (5h10m ago)   4d7h    10.32.0.2        master    <none>           <none>            k8s-app=kube-dns,pod-template-hash=565d847f94
etcd-master                      1/1     Running   10 (5h10m ago)    5d      192.168.68.150   master    <none>           <none>            component=etcd,tier=control-plane
kube-apiserver-master            1/1     Running   6 (5h10m ago)     4d6h    192.168.68.150   master    <none>           <none>            component=kube-apiserver,tier=control-plane
kube-controller-manager-master   1/1     Running   16 (5h10m ago)    5d      192.168.68.150   master    <none>           <none>            component=kube-controller-manager,tier=control-plane
kube-proxy-6t6qp                 1/1     Running   10 (5h10m ago)    5d      192.168.68.150   master    <none>           <none>            controller-revision-hash=cc5dd58d4,k8s-app=kube-proxy,pod-template-generation=1
kube-proxy-ffdgc                 1/1     Running   8 (5h11m ago)     4d11h   192.168.68.151   worker2   <none>           <none>            controller-revision-hash=cc5dd58d4,k8s-app=kube-proxy,pod-template-generation=1
kube-proxy-gq4v9                 1/1     Running   7 (5h11m ago)     4d10h   192.168.68.145   worker1   <none>           <none>            controller-revision-hash=cc5dd58d4,k8s-app=kube-proxy,pod-template-generation=1
kube-scheduler-master            1/1     Running   15 (5h10m ago)    5d      192.168.68.150   master    <none>           <none>            component=kube-scheduler,tier=control-plane
weave-net-6bpzq                  2/2     Running   23 (5h10m ago)    5d      192.168.68.150   master    <none>           <none>            controller-revision-hash=d6dc745f,name=weave-net,pod-template-generation=1
weave-net-8gkx4                  2/2     Running   17 (5h11m ago)    4d11h   192.168.68.151   worker2   <none>           <none>            controller-revision-hash=d6dc745f,name=weave-net,pod-template-generation=1
weave-net-f27sl                  2/2     Running   15 (5h11m ago)    4d10h   192.168.68.145   worker1   <none>           <none>            controller-revision-hash=d6dc745f,name=weave-net,pod-template-generation=1"
===================================
hai@master:~$ kubectl get all  --show-labels  -o wide
NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE     SELECTOR        LABELS
service/"kubernetes"   ClusterIP   "10.96.0.1 "       <none>        443/TCP   5d      <none>          "component=apiserver,provider=kubernetes""


hai@master:~$ kubectl describe service kubernetes
Name:              kubernetes
Namespace:         default
Labels:            component=apiserver
                   provider=kubernetes
Annotations:       <none>
Selector:          <none>
Type:              "ClusterIP"
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                "10.96.0.1"
IPs:               10.96.0.1
Port:              https  "443/"TCP
TargetPort:       " 6443/TC"P
Endpoints:        " 192.168.68.150:6443"
Session Affinity:  None
Events:            <none> 

######################################################################################################################"
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: webapp
spec:
  replicas: 1
  strategy:
     type: Recreate
  selector: 
     matchLabels:
        app: webserver
  template:
    metadata:
      labels: 
         app: webserver
    spec: 
      containers:
        - name: nginx
          image: nginx:1.7.1          
          ports:
            - containerPort: 80
========="               
hai@master:~$ kubectl get all --show-labels  -o  wide
NAME                          READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/webapp-744d6b7964-7v5jj   1/1     Running   0          5m36s   10.44.0.1   worker2   <none>           <none>            app=webserver,pod-template-hash=744d6b7964
pod/webapp-744d6b7964-gmdmg   1/1     Running   0          3s      10.36.0.1   worker1   <none>           <none>            app=webserver,pod-template-hash=744d6b7964

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE   SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   40m   <none>     component=apiserver,provider=kubernetes

NAME                     READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS   IMAGES        SELECTOR        LABELS
deployment.apps/webapp   2/2     2            2           5m36s   nginx        nginx:1.7.1   app=webserver   <none>

NAME                                DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES        SELECTOR                                     LABELS
replicaset.apps/webapp-744d6b7964   2         2         2       5m36s   nginx        nginx:1.7.1   app=webserver,pod-template-hash=744d6b7964   app=webserver,pod-template-hash=744d6b7964"
==============///////////////\\\\\\\\\\\\\\\////////
hai@master:~$ kubectl expose deployment  webserver   --type=ClusterIP  --port  80 --target-port 8089
service/webserver exposed
==============
hai@master:~$ kubectl get svc  --show-labels  -o wide
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE    SELECTOR        LABELS
kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP   63m    <none>          component=apiserver,provider=kubernetes
"webserver "   "ClusterIP "  10.105.28.230   <none>        80/TCP    113s   app=webserver   <none>"

hai@master:~$ kubectl expose deployment webserver   "--type=NodePort"  --port  80  --target-port 8087
Error from server "(AlreadyExists)": services "webserver" already exists"

hai@master:~$ kubectl delete service webserver
service "webserver" deleted"

hai@master:~$ kubectl expose deployment  webserver --type=NodePort  --port  80  --target-port 8086
service/webserver exposed"

hai@master:~$ kubectl get svc  --show-labels  -o wide
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE   SELECTOR        LABELS
kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP        73m   <none>          component=apiserver,provider=kubernetes
"webserver "   "NodePort"    10.109.110.80   <none>        80:30117/TCP   93s   app=webserver   <none>"

hai@master:~$ kubectl get all --show-labels  -o wide
NAME                             READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/webserver-744d6b7964-pqnws   1/1     Running   0          28m   10.44.0.1   worker2   <none>           <none>            app=webserver,pod-template-hash=744d6b7964
pod/webserver-744d6b7964-x2kpw   1/1     Running   0          28m   10.36.0.1   worker1   <none>           <none>            app=webserver,pod-template-hash=744d6b7964

NAME                 TYPE       " CLUSTER-IP   "   EXTERNAL-IP   PORT(S)        AGE   SELECTOR        LABELS
service/kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP        83m   <none>          component=apiserver,provider=kubernetes
service/webserver    "NodePort" 10.109.110.80   <none>        "80:30117/TCP "  11m   app=webserver   <none>

NAME                        READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR        LABELS
deployment.apps/webserver   2/2     2            2           28m   nginx        nginx:1.7.1   app=webserver   <none>

NAME                                   DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                     LABELS
replicaset.apps/webserver-744d6b7964   2         2         2       28m   nginx        nginx:1.7.1   app=webserver,pod-template-hash=744d6b7964   app=webserver,pod-template-hash=744d6b7964"

hai@master:~$ kubectl describe service webserver
Name:                     webserver
Namespace:                default
Labels:                   <none>
Annotations:              <none>
Selector:                 app=webserver
Type:                    " NodePort"
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                      " 10.109.110.80"
IPs:                      10.109.110.80
Port:                     <unset>  80/TCP
TargetPort:               8086/TCP
NodePort:                 <unset>  /"30117TCP"
Endpoints:                10.36.0.1:"8086",10.44.0.1:8086
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>:"

*Nodeport range: 30,000 TO 32767

apiVersion: v1
kind: Service
metadata:
  name: webserver 
  labels:
    name: webserver 
spec:
  type: NodePort
  ports:
    - port: 80
      nodePort: 30080
      name: http
    - port: 443
      nodePort: 30443
      name: https
  selector:
    name: webserver 
"
hai@master:~$ kubectl get all --show-labels  -o wide
NAME                             READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/webserver-744d6b7964-pqnws   1/1     Running   0          61m   10.44.0.1   worker2   <none>           <none>            app=webserver,pod-template-hash=744d6b7964
pod/webserver-744d6b7964-x2kpw   1/1     Running   0          61m   10.36.0.1   worker1   <none>           <none>            app=webserver,pod-template-hash=744d6b7964

NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE    SELECTOR         LABELS
service/kubernetes   ClusterIP   10.96.0.1        <none>        443/TCP                      116m   <none>           component=apiserver,provider=kubernetes
service/webserver    "NodePort "   10.109.169.104   <none>        "80:30080"/TCP,443:30443/TCP   15s    name=webserver   name=webserver

NAME                        READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR        LABELS
deployment.apps/webserver   2/2     2            2           61m   nginx        nginx:1.7.1   app=webserver   <none>

NAME                                   DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                     LABELS
replicaset.apps/webserver-744d6b7964   2         2         2       61m   nginx        nginx:1.7.1   app=webserver,pod-template-hash=744d6b7964   app=webserver,pod-template-hash=744d6b7964
====================================

hai@master:~$ kubectl port-forward    webserver-744d6b7964-pqnws    8089:80  --address 0.0.0.0
Forwarding from 0.0.0.0:8089 -> 80
Handling connection for 8089
^Z
[2]+  Stopped                 kubectl port-forward webserver-744d6b7964-pqnws 8089:80 --address 0.0.0.0 "
============================
hai@master:~/test$ kubectl expose deployment webserver  --type=LoadBalancer
Error from server (AlreadyExists): services "webserver" already exists"

hai@master:~/test$ kubectl delete services webserver
service "webserver" deleted"

hai@master:~/test$ kubectl expose deployment webserver  --type=LoadBalancer
service/webserver exposed

hai@master:~$ kubectl get all --show-labels  -o wide
NAME                             READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/webserver-744d6b7964-pqnws   1/1     Running   0          99m   10.44.0.1   worker2   <none>           <none>            app=webserver,pod-template-hash=744d6b7964
pod/webserver-744d6b7964-x2kpw   1/1     Running   0          99m   10.36.0.1   worker1   <none>           <none>            app=webserver,pod-template-hash=744d6b7964

NAME                 TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE    SELECTOR        LABELS
service/kubernetes   ClusterIP      10.96.0.1       <none>        443/TCP        155m   <none>          component=apiserver,provider=kubernetes
service/webserver   " LoadBalancer"   10.105.244.86  " <pending> "    "80:30677/TCP"   21s    app=webserver   <none>

NAME                        READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR        LABELS
deployment.apps/webserver   2/2     2            2           99m   nginx        nginx:1.7.1   app=webserver   <none>

NAME                                   DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR                                     LABELS
replicaset.apps/webserver-744d6b7964   2         2         2       99m   nginx        nginx:1.7.1   app=webserver,pod-template-hash=744d6b7964   app=webserver,pod-template-hash=
########################################################################################################"
          Init Containers    
https://kubernetes.io/docs/concepts/workloads/pods/init-containers/

A Pod can have multiple containers running apps within it, but it can also have one or more init containers, which are run before the app containers are started.
Init containers are exactly like regular containers, except:
Init containers always run to completion.
Each init container must complete successfully before the next one starts.

apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app.kubernetes.io/name: MyApp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: ['sh', '-c', 'echo The app is running! && sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox:1.28
    command: ['sh', '-c', "until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done"]
  - name: init-mydb
    image: busybox:1.28
    command: ['sh', '-c', "until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done"]
----------------------"
hai@master:~$ kubectl get all --show-labels  -o wide
NAME            READY   STATUS     RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/myapp-pod   0/1     Init:0/2   0          10s   10.44.0.1   worker2   <none>           <none>            app.kubernetes.io/name=MyApp

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE   SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   62s   <none>     component=apiserver,provider=kubernetes"

hai@master:~$ kubectl get pod --show-labels  -o wide
NAME        READY   STATUS     RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
myapp-pod   0/1     Init:0/2   0          60s   10.44.0.1   worker2   <none>           <none>            app.kubernetes.io/name=MyApp"
======----------
hai@master:~/test$ kubectl  describe  pod myapp-pod
Name:             myapp-pod
Namespace:        "default"
Priority:         0
Labels:           app.kubernetes.io/name=MyApp
Status:           "Pending"
Init Containers:
  "init-myservice:"
    Container ID:  containerd://a6eee39408a02e575e0aa71bd0f0c625cb23f579e9fc3ec5d148b134b077012c
    "Image:         busybox:1.28"
    Command:
      sh
      -c
      until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done
    "State:          Running"
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lsrbh (ro)
  "init-mydb:"
    Container ID:
    "Image:         busybox:1.28"
    Image ID:
     Command:
      sh
      -c
      until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done
   " State:          Waiting"
    "Reason:       PodInitializing"
    Ready:          False
     Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lsrbh (ro)
      
"Containers:"
  myapp-conta"iner:"
    Container ID:
   " Image:         busybox:1.28"
    Image ID:
     Command:
      sh
      -c
      echo The app is running! && sleep 3600
   " State:          Waiting"
      "Reason:       PodInitializing"
  "  Ready:          False"
       Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lsrbh (ro)
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  4m47s  default-scheduler  Successfully assigned default/myapp-pod to worker2
  Normal  Pulled     4m46s  kubelet            Container image "busybox:1.28" already present on machine
  Normal  Created    4m46s  kubelet            Created container init-myservice
  Normal  Started    4m46s  kubelet            Started container init-myservice
==================++++++++++++++++++++"
hai@master:~/test$ kubectl logs "myapp-pod" -c "init-myservice"
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

nslookup: can't resolve 'myservice.default.svc.cluster.local'
waiting for myservice
nslookup: can't resolve 'myservice.default.svc.cluster.local'
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

waiting for myservice
nslookup: can't resolve 'myservice.default.svc.cluster.local'
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

waiting for myservice
nslookup: can't resolve 'myservice.default.svc.cluster.local'
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local
========================+++++++++"
hai@master:~/test$ kubectl logs myapp-pod -c init-mydb
Error from server (BadRequest): container "init-mydb" in pod "myapp-pod" is waiting to start: PodInitializing

https://kubernetes.io/docs/concepts/workloads/pods/init-containers/#understanding-init-containers
------------------
hai@master:~$ kubectl get pod --show-labels  -o wide
NAME        READY   STATUS     RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
myapp-pod   0/1     Init:0/2   0          91m   10.44.0.1   worker2   <none>           <none>            app.kubernetes.io/name=MyApp
--------------------"
---
apiVersion: v1
kind: Service
metadata:
  name: myservice
spec:
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9376
---
apiVersion: v1
kind: Service
metadata:
  name: mydb
spec:
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9377

====+++==="
hai@master:~$ kubectl get all --show-labels  -o wide
NAME            READY   STATUS     RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/myapp-pod   0/1     "Init:0/2   0 "         10s   10.44.0.1   worker2   <none>           <none>            app.kubernetes.io/name=MyApp
NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE   SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   62s   <none>     component=apiserver,provider=kubernetes"

hai@master:~$ kubectl get all --show-labels  -o wide
NAME            READY   STATUS     RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/myapp-pod   0/1    " Init:0/2 "  0          95m   10.44.0.1   worker2   <none>           <none>            app.kubernetes.io/name=MyApp
NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE   SELECTOR   LABELS
service/kubernetes "  ClusterIP "  10.96.0.1       <none>        443/TCP   96m   <none>     component=apiserver,provider=kubernetes
service/mydb         C"lusterIP "  10.98.174.26    <none>        80/TCP    1s    <none>     <none>
service/myservice    "ClusterIP "  10.107.72.183   <none>        80/TCP    1s    <none>     <none>"

hai@master:~$ kubectl get all --show-labels  -o wide
NAME            READY   STATUS            RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/myapp-pod   0/1    " PodInitializing "  0          95m   10.44.0.1   worker2   <none>           <none>            app.kubernetes.io/name=MyApp

NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE   SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP   96m   <none>     component=apiserver,provider=kubernetes
service/mydb         ClusterIP   10.98.174.26    <none>        80/TCP    3s    <none>     <none>
service/myservice    ClusterIP   10.107.72.183   <none>        80/TCP    3s    <none>     <none>"

hai@master:~$ kubectl get all --show-labels  -o wide
NAME            READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/myapp-pod   1/1     "Running"   0          95m   10.44.0.1   worker2   <none>           <none>            app.kubernetes.io/name=MyApp
NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE   SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP   96m   <none>     component=apiserver,provider=kubernetes
service/mydb         ClusterIP   10.98.174.26    <none>        80/TCP    10s   <none>     <none>
service/myservice    ClusterIP   10.107.72.183   <none>        80/TCP    10s   <none>     <none>
===----

hai@master:~$ kubectl describe  pod myapp-pod
Name:             myapp-pod
Namespace:        default

"Status:           Running"
Init Containers:
 " init-myservice:"
    Container ID:  containerd://a6eee39408a02e575e0aa71bd0f0c625cb23f579e9fc3ec5d148b134b077012c
    Image:         busybox:1.28
        Command:
      sh
      -c
      until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done
    State:          "Terminated"
      Reason:       Completed
     Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lsrbh (ro)
 " init-mydb:"
    Container ID:  containerd://f0c35385036543cdb7a060a048e6c13b22151c9d6adcc326481ca30f33ed41aa
    Image:         busybox:1.28
       sh
      -c
      until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done
    State:          "Terminated"
      Reason:       Completed
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lsrbh (ro)
"Containers:"
 " myapp-container:"
    Container ID:  containerd://94f7f17085c5209c3a3026ec053a19167e93855c006131d345c29ceb685a96bf
    Image:         busybox:1.28
    Command:
      sh
      -c
      echo The app is running! && sleep 3600
    State:        "  Running"
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lsrbh (ro)
############################################################################

init cobtainer and MULTICONTAINER POD

https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-initialization/#create-a-pod-that-has-an-init-container

apiVersion: v1
kind: Pod
metadata:
  name: init-demo
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
    volumeMounts:
    - name: workdir
      mountPath: /usr/share/nginx/html
  # These containers are run during pod initialization
  initContainers:
  - name: install
    image: busybox:1.28
    command:
    - wget
    - "-O"
    - "/work-dir/index.html"
    - http://info.cern.ch
    volumeMounts:
    - name: workdir
      mountPath: "/work-dir"
  dnsPolicy: Default
  volumes:
  - name: workdir
    emptyDir: {}
------========"
hai@master:~$ kubectl get all --show-labels  -o wide
NAME            READY   STATUS     RESTARTS   AGE   IP       NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/init-demo   0/1     "Init:0/1 "  0          2s    <none>   worker2   <none>           <none>            <none>

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3h39m   <none>     component=apiserver,provider=kubernetes"

hai@master:~$ kubectl get all --show-labels  -o wide
NAME            READY   STATUS            RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/init-demo   0/1     "PodInitializing "  0          4s    10.44.0.1   worker2   <none>           <none>            <none>

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3h39m   <none>     component=apiserver,provider=kubernetes"

hai@master:~$ kubectl get all --show-labels  -o wide
NAME            READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/init-demo   1/1     "Running  " 0          7s    10.44.0.1   worker2   <none>           <none>            <none>

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE     SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3h39m   <none>     component=apiserver,provider=kubernetes
===========================++++++++++++++++++++"
hai@master:~$ kubectl describe pod init-demo
Name:             init-demo
Namespace:        default
Status:           Running
"Init Containers:"
  install:
    Image:         busybox:1.28
    Command:
      wget
      -O
      /work-dir/index.html
      http://info.cern.ch
    State:          Terminated
      Reason:       Completed
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-99pcd (ro)
      /work-dir from workdir (rw)
Containers:
  nginx:
    Image:          nginx
    State:          Running
    Mounts:
      /usr/share/nginx/html from workdir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-99pcd (ro)
Volumes:
  workdir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)

#######################################################################################

apiVersion: v1
kind: Pod
metadata: 
   name: onein2
spec:
  containers: 
     - name: cont1
       image: nginx

     -  name:  cont2
        image: nginx

hai@master:~$ kubectl get po -o wide
NAME     READY   STATUS             RESTARTS       AGE     IP          NODE      NOMINATED NODE   READINESS GATES
onein2   1/2     CrashLoopBackOff   6 (2m6s ago)   8m32s   10.44.0.1   worker2   <none>           <none>

hai@master:~/test$ kubectl describe pod onein2
Name:             onein2
Namespace:        default
Annotations:      <none>
Status:           Running

Containers:
  cont1:
    Image:          nginx
    State:          Running
    Ready:          True
    Restart Count:  0
  
  cont2:
       Image:          nginx
    State:          Terminated
      Reason:       Error
      Exit Code:    1
    Last State:     Terminated
     Reason:       Error
    Ready:          False
    Restart Count:  3
 ==========----------  "small  modifications"" 

apiVersion: v1
kind: Pod
metadata: 
   name: onein2
spec:
  containers: 
     - name: cont1
       image: nginx
       args: ["sleep","3600"]      "## see look   "
     - name:  cont2
       image: nginx

hai@master:~/test$ kubectl get po  -o  wide
NAME     READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES
onein2   "2/2     Running  " 0          40s   10.44.0.1   worker2   <none>           <none>"
=====
hai@master:~/test$ kubectl describe pod onein2
Name:             onein2
Status:           Running
Containers:
 " cont1:"
    Image:         nginx
    Args:
      sleep                      "## look see " 
      3600                     "" ## look see 
    State:          Running    
  "cont2:"
    Image:          nginx
    State:          Running
    Ready:          True
    Restart Count:  0
======---------"
hai@master:~$ kubectl  exec  onein2 -it   --  /bin/bash
Defaulted container "cont1" out of: cont1, cont2
root@onein2:/# ls
bin   dev                  docker-entrypoint.sh  home  lib64  mnt  proc  run   srv  tmp  var
boot  docker-entrypoint.d  etc                   lib   media  opt  root  sbin  sys  usr

"apt update -y 
root@onein2:/# apt install net-tools
root@onein2:/# apt install netcat
root@onein2:/# netcat -l -p 3306  
root@onein2:/# apt  install telnet


root@onein2:/# netstat -tupln
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      -
tcp6       0      0 :::80                   :::*                    LISTEN      -"


root@onein2:/# netcat -l -p 3306          # "second container "
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      "0 0.0.0.0:3306  "          0.0.0.0:*               LISTEN      -
tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      -
tcp6       0      0 :::80                   :::*                    LISTEN      -

root@onein2:/# telnet localhost 3306
Trying ::1...
Trying 127.0.0.1...
Connected to localhost.
Escape character is '^]'.
------"


root@onein2:/# telnet localhost 3306
Trying ::1...
Trying 127.0.0.1...
Connected to localhost.
Escape character is '^]'.
hello  i am from 1st  pod
 ok
how  are you
 iam  good  u
i am also  good"
--
root@onein2:/# netcat -l -p 3306
hello  i am from 1st  pod
 ok
how  are you
 iam  good  u
i am also  good

https://www.mirantis.com/blog/multi-container-pods-and-container-communication-in-kubernetes/      "

apiVersion: v1
kind: Pod
metadata:
  name: mc1
spec:
  volumes:
  - name: html
    emptyDir: {}
  containers:
  - name: 1st
    image: nginx
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html
  - name: 2nd
    image: debian
    volumeMounts:
    - name: html
      mountPath: /html
    command: ["/bin/sh", "-c"]
    args:
      - while true; do
          date >> /html/index.html;
          sleep 1;
        done
"----
hai@master:~$ kubectl get pods -o wide --show-labels
NAME   READY   STATUS              RESTARTS   AGE   IP       NODE      NOMINATED NODE   READINESS GATES   LABELS
mc1    "0/2     ContainerCreating  " 0          28s   <none>   worker2   <none>           <none>            <none>

hai@master:~$ kubectl get pods -o wide --show-labels
NAME   READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
mc1   " 2/2     Running  " 0          37s   10.44.0.1   worker2   <none>           <none>            <none>
==================+++++++++++++++++++++++++++++++++++++++++++"
hai@master:~$ kubectl describe pod mc1
Name:             mc1
Status:           Running
Containers:
 " 1st:"
     Image:          nginx
    State:          Running
    Mounts:
      /usr/share/nginx/html from html (rw)      ##  check data  see  look 
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fqw7f (ro)
 " 2nd:"
    Image:         debian
    Command:
      /bin/sh
      -c
    Args:
      while true; do date >> /html/index.html; sleep 1; done    " ## see look path data"
     Mounts:
      /html from html (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fqw7f (ro)
Volumes:
  html:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
---"
hai@master:~$ kubectl exec    mc1 -c 2nd -it -- /bin/bash        "## see look  2nd container "
root@mc1:/# ls
bin  boot  dev  etc  home  html  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var
root@mc1:/# ls /html
index.html  "test"

root@mc1:/# cd html/
root@mc1:/html# cat index.html
Sun Dec 25 07:47:09 UTC 2022
Sun Dec 25 07:47:10 UTC 2022
Sun Dec 25 07:47:11 UTC 2022
Sun Dec 25 07:47:12 UTC 2022
Sun Dec 25 07:47:13 UTC 2022
Sun Dec 25 07:47:14 UTC 2022
Sun Dec 25 07:47:15 UTC 2022"


hai@master:~$ kubectl exec  mc1 -it -- /bin/bash              " ### see look 1st container"
Defaulted container "1st" out of: 1st, 2nd
root@mc1:/# ls
bin  boot  dev  docker-entrypoint.d  docker-entrypoint.sh  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var

root@mc1:/# cd /usr/share/
root@mc1:/usr/share# ls
"X11         base-passwd      ca-certificates  debianutils  doc-base    fonts  info      libc-bin  man         misc   pam-confi
adduser     bash-completion  common-licenses  dict         dpkg        gcc    java      lintian   maven-repo  nginx  perl5
base-files  bug              debconf          doc          fontconfig  gdb    keyrings  locale    menu        pam    pixmaps"
root@mc1:/usr/share# cd nginx/html/
root@mc1:/usr/share/nginx/html# ls
index.html  "test"
root@mc1:/usr/share/nginx/html# cat index.html
Sun Dec 25 07:47:09 UTC 2022
Sun Dec 25 07:47:10 UTC 2022
Sun Dec 25 07:47:11 UTC 2022
Sun Dec 25 07:47:12 UTC 2022
Sun Dec 25 07:47:13 UTC 2022
Sun Dec 25 07:47:14 UTC 2022
Sun Dec 25 07:47:15 UTC 2022
Sun Dec 25 07:47:16 UTC 2022

root@mc1:/usr/share/nginx/html# tail -f index.html
Sun Dec 25 09:00:40 UTC 2022
Sun Dec 25 09:00:41 UTC 2022
Sun Dec 25 09:00:42 UTC 2022
Sun Dec 25 09:00:43 UTC 2022
############################################################################################

https://www.youtube.com/watch?v=OMzVbeTKN-o

Kubernetes-Day-15
Explanation on DaemonSet, Affinity & AntiAffinity, Scheduling Topologies,Static pods"

apiVersion: apps/v1
kind: DaemonSet
metadata: 
  name: dmset
  labels:
    app: nginx
spec:
  selector:
    matchLabels:
      app: nginx
  template: 
    metadata:
       labels: 
          app: nginx
    spec: 
      containers:
         - name: nginx
           image: nginx:1.7.1
           ports:
             - containerPort: 80   "

hai@master:~$ kubectl get pods --show-labels  -o wide
NAME          READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
dmset-5gp84   1/1     Running   0          21s   10.36.0.1   "worker1  " <none>           <none>            app=nginx,controller-revision-hash=788b5bb875,pod-template-generation=1
dmset-pb8pw   1/1     Running   0          21s   10.44.0.1   "worker2"   <none>           <none>            app=nginx,controller-revision-hash=788b5bb875,pod-template-generation=1"

hai@master:~$ kubectl get all -o wide --show-labels
NAME              READY   STATUS    RESTARTS   AGE    IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/dmset-5gp84   1/1     Running   0          2m4s   10.36.0.1   worker1   <none>           <none>            app=nginx,controller-revision-hash=788b5bb875,pod-template-generation=1
pod/dmset-pb8pw   1/1     Running   0          2m4s   10.44.0.1   worker2   <none>           <none>            app=nginx,controller-revision-hash=788b5bb875,pod-template-generation=1

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE   SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   26h   <none>     component=apiserver,provider=kubernetes

NAME                   DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE    CONTAINERS   IMAGES        SELECTOR    LABELS
daemonset.apps/"dmset   2         2         2       2            2  "         <none>          2m4s   nginx        nginx:1.7.1   app=nginx   app=nginx "

hai@master:~$ kubectl get pods -n  kube-system -o wide --show-labels
NAME                              READY   STATUS    RESTARTS          AGE     IP               NODE      NOMINATED NODE   READINESS GATES   LABELS
coredns-565d847f94-q5zw9         1/1     Running   467 (6h44m ago)   6d1h    10.32.0.3        master    <none>           <none>            k8s-app=kube-dns,pod-template-hash=565d847f94
coredns-565d847f94-rg789         1/1     Running   456 (6h44m ago)   6d1h    10.32.0.2        master    <none>           <none>            k8s-app=kube-dns,pod-template-hash=565d847f94
etcd-master                      1/1     Running   12 (6h44m ago)    6d18h   192.168.68.150   master    <none>           <none>            component=etcd,tier=control-plane
kube-apiserver-master            1/1     Running   8 (6h44m ago)     6d      192.168.68.150   master    <none>           <none>            component=kube-apiserver,tier=control-plane
kube-controller-manager-master   1/1     Running   18 (6h44m ago)    6d18h   192.168.68.150   master    <none>           <none>            component=kube-controller-manager,tier=control-plane
kube-proxy-6t6qp                 1/1     Running   12 (6h44m ago)    6d18h   192.168.68.150   master    <none>           <none>            controller-revision-hash=cc5dd58d4,k8s-app=kube-proxy,pod-template-generation=1
kube-proxy-ffdgc                 1/1     Running   10 (6h44m ago)    6d5h    192.168.68.151   worker2   <none>           <none>            controller-revision-hash=cc5dd58d4,k8s-app=kube-proxy,pod-template-generation=1
kube-proxy-gq4v9                 1/1     Running   9 (6h44m ago)     6d4h    192.168.68.145   worker1   <none>           <none>            controller-revision-hash=cc5dd58d4,k8s-app=kube-proxy,pod-template-generation=1
kube-scheduler-master            1/1     Running   17 (6h44m ago)    6d18h   192.168.68.150   master    <none>           <none>            component=kube-scheduler,tier=control-plane
weave-net-6bpzq                  2/2     Running   28 (6h44m ago)    6d18h   192.168.68.150   master    <none>           <none>            controller-revision-hash=d6dc745f,name=weave-net,pod-template-generation=1
weave-net-8gkx4                  2/2     Running   21 (6h44m ago)    6d5h    192.168.68.151   worker2   <none>           <none>            controller-revision-hash=d6dc745f,name=weave-net,pod-template-generation=1
weave-net-f27sl                  2/2     Running   19 (6h44m ago)    6d4h    192.168.68.145   worker1   <none>           <none>            controller-revision-hash=d6dc745f,name=weave-net,pod-template-generation=1 "

hai@master:~$ hai@master:~$ kubectl get all -n  kube-system -o wide --show-labels
hai@master:~$: command not found
hai@master:~$  kubectl get all -n  kube-system -o wide --show-labels
NAME                                 READY   STATUS    RESTARTS         AGE     IP               NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/coredns-565d847f94-q5zw9         1/1     Running   467 (7h5m ago)   6d1h    10.32.0.3        master    <none>           <none>            k8s-app=kube-dns,pod-template-hash=565d847f94
pod/coredns-565d847f94-rg789         1/1     Running   456 (7h5m ago)   6d1h    10.32.0.2        master    <none>           <none>            k8s-app=kube-dns,pod-template-hash=565d847f94
pod/etcd-master                      1/1     Running   12 (7h5m ago)    6d19h   192.168.68.150   master    <none>           <none>            component=etcd,tier=control-plane
pod/kube-apiserver-master            1/1     Running   8 (7h5m ago)     6d1h    192.168.68.150   master    <none>           <none>            component=kube-apiserver,tier=control-plane
pod/kube-controller-manager-master   1/1     Running   18 (7h5m ago)    6d19h   192.168.68.150   master    <none>           <none>            component=kube-controller-manager,tier=control-plane
pod/kube-proxy-6t6qp                 1/1     Running   12 (7h5m ago)    6d18h   192.168.68.150   master    <none>           <none>            controller-revision-hash=cc5dd58d4,k8s-app=kube-proxy,pod-template-generation=1
pod/kube-proxy-ffdgc                 1/1     Running   10 (7h5m ago)    6d5h    192.168.68.151   worker2   <none>           <none>            controller-revision-hash=cc5dd58d4,k8s-app=kube-proxy,pod-template-generation=1
pod/kube-proxy-gq4v9                 1/1     Running   9 (7h5m ago)     6d4h    192.168.68.145   worker1   <none>           <none>            controller-revision-hash=cc5dd58d4,k8s-app=kube-proxy,pod-template-generation=1
pod/kube-scheduler-master            1/1     Running   17 (7h5m ago)    6d19h   192.168.68.150   master    <none>           <none>            component=kube-scheduler,tier=control-plane
pod/weave-net-6bpzq                  2/2     Running   28 (7h5m ago)    6d18h   192.168.68.150   master    <none>           <none>            controller-revision-hash=d6dc745f,name=weave-net,pod-template-generation=1
pod/weave-net-8gkx4                  2/2     Running   21 (7h5m ago)    6d5h    192.168.68.151   worker2   <none>           <none>            controller-revision-hash=d6dc745f,name=weave-net,pod-template-generation=1
pod/weave-net-f27sl                  2/2     Running   19 (7h5m ago)    6d4h    192.168.68.145   worker1   <none>           <none>            controller-revision-hash=d6dc745f,name=weave-net,pod-template-generation=1

NAME               TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE     SELECTOR           LABELS
service/kube-dns   ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP   6d19h   k8s-app=kube-dns   k8s-app=kube-dns,kubernetes.io/cluster-service=true,kubernetes.io/name=CoreDNS

NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE     CONTAINERS        IMAGES                                                     SELECTOR             LABELS
"daemonset.apps/kube-proxy   3         3         3       3            3  "         kubernetes.io/os=linux   6d19h   kube-proxy        registry.k8s.io/kube-proxy:v1.25.5                         k8s-app=kube-proxy   k8s-app=kube-proxy
"daemonset.apps/weave-net    3         3         3       3            3  "         <none>                   6d18h   weave,weave-npc   weaveworks/weave-kube:latest,weaveworks/weave-npc:latest   name=weave-net       name=weave-net

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS   IMAGES                                   SELECTOR           LABELS
deployment.apps/coredns   2/2     2            2           6d19h   coredns      registry.k8s.io/coredns/coredns:v1.9.3   k8s-app=kube-dns   k8s-app=kube-dns

NAME                                 DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES                                   SELECTOR                                        LABELS
replicaset.apps/coredns-565d847f94   2         2         2       6d18h   coredns      registry.k8s.io/coredns/coredns:v1.9.3   k8s-app=kube-dns,pod-template-hash=565d847f94   k8s-app=kube-dns,pod-template-hash=565d847f94"

https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/
DaemonSet
A DaemonSet ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them. 
As nodes are removed from the cluster, those Pods are garbage collected. Deleting a DaemonSet will clean up the Pods it created.
Some typical uses of a DaemonSet are:

running a cluster "storage" daemon on every node
running a "logs" collection daemon on every node
running a "node monitoring" daemon on every node

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd-elasticsearch
  namespace: kube-system
  labels:
    k8s-app: fluentd-logging
spec:
  selector:
    matchLabels:
      name: fluentd-elasticsearch
  template:
    metadata:
      labels:
        name: fluentd-elasticsearch
    spec:
      tolerations:
      # these tolerations are to have the daemonset runnable on control plane nodes
      # remove them if your control plane nodes should not run pods
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      containers:
      - name: fluentd-elasticsearch
        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2
        resources:
          limits:
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - name: varlog
          mountPath: /var/log
      terminationGracePeriodSeconds: 30
      volumes:
      - name: varlog
        hostPath:
          path: /var/log          "
##################################################################################################
scheduling Topologies 

"1. nodeName:
2. nodeSelector:
3. cordon and uncordon "

apiVersion: apps/v1
kind: Deployment
metadata:
  name: topology
  labels: 
     app: stopology
spec: 
  replicas: 4
  selector: 
     matchLabels: 
        app: stopology
  template:
    metadata:
       labels: 
         app: stopology
    spec: 
      nodeName: worker1           "# see look "
      containers:
         - name: nginx
           image: nginx
           ports:
             - containerPort: 80   "
hai@master:~$ kubectl get nodes -o wide --show-labels
NAME      STATUS   ROLES           AGE     VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME     LABELS
master    Ready    control-plane   6d19h   v1.25.4   192.168.68.150   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=master,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node.kubernetes.io/exclude-from-external-load-balancers=
worker1   Ready    <none>          6d5h    v1.25.4   192.168.68.145   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=worker1,kubernetes.io/os=linux
"worker2"   Ready    <none>          6d6h    v1.25.4   192.168.68.151   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=worker2,kubernetes.io/os=linux

hai@master:~$ kubectl get pods --show-labels -o  wide
NAME                        READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
topology-85649bdbc9-966rq   1/1     Running   0          18s   10.36.0.2   "worker1 "  <none>           <none>            app=stopology,pod-template-hash=85649bdbc9
topology-85649bdbc9-9jd68   1/1     Running   0          19s   10.36.0.4   '"worker1" '  <none>           <none>            app=stopology,pod-template-hash=85649bdbc9
topology-85649bdbc9-bhpj8   1/1     Running   0          19s   10.36.0.3  " worker1 "  <none>           <none>            app=stopology,pod-template-hash=85649bdbc9
topology-85649bdbc9-f8krb   1/1     Running   0          19s   10.36.0.1   "worker1  " <none>           <none>            app=stopology,pod-template-hash=85649bdbc9
+++++++++++++++++++++++++++++++++++++++++++++++++
hai@master:~/test$ kubectl label node worker1  cpu=intel
node/worker1 labeled "

hai@master:~/test$ kubectl label node worker2 "cpu=amd"
node/worker2 labeled

apiVersion: apps/v1
kind: Deployment
metadata: 
  name: ndselector
  labels:
    app: ndselector
spec:
   replicas: 4
   selector:
     matchLabels:
       app: ndselector
   template:    
     metadata: 
       labels:
         app: ndselector
     spec:
       nodeSelector:        " # see look  "
          cpu: amd            # see look 
       containers:      
        - name: nginx
          image: nginx
          ports: 
           - containerPort: 80             
++++++++++++++++++++"
hai@master:~$ kubectl get pods --show-labels -o  wide
NAME                          READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
ndselector-5fc7d6d75d-bdcd9   1/1     Running   0          83s   10.44.0.3   "worker2"   <none>           <none>            app=ndselector,pod-template-hash=5fc7d6d75d
ndselector-5fc7d6d75d-gwplp   1/1     Running   0          83s   10.44.0.4   "worker2 "  <none>           <none>            app=ndselector,pod-template-hash=5fc7d6d75d
ndselector-5fc7d6d75d-r4hdm   1/1     Running   0          83s   10.44.0.1   "worker2"   <none>           <none>            app=ndselector,pod-template-hash=5fc7d6d75d
ndselector-5fc7d6d75d-tpzpb   1/1     Running   0          83s   10.44.0.2   "worker2 "  <none>           <none>            app=ndselector,pod-template-hash=5fc7d6d75d
++++++++++++++======================###########"

3. cordon and uncordon

hai@master:~$ kubectl get nodes -o wide --show-labels
NAME      STATUS   ROLES           AGE     VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME     LABELS
master    Ready    control-plane   6d21h   v1.25.4   192.168.68.150   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hos
worker1   Ready    <none>          6d6h    v1.25.4   192.168.68.145   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,cpu=intel,kubernetes.io/arch=amd64,kuberne
worker2   Ready    <none>          6d8h    v1.25.4   192.168.68.151   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,cpu=amd,kubernetes.io/arch=amd64,kubernete"

hai@master:~$ kubectl cordon worker1
node/worker1 cordoned   "

hai@master:~$ kubectl get nodes -o wide
NAME      STATUS                     ROLES           AGE     VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
master    Ready                      control-plane   6d21h   v1.25.4   192.168.68.150   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14
"worker1   Ready,SchedulingDisabled"   <none>          6d6h    v1.25.4   192.168.68.145   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14
worker2   Ready                      <none>          6d8h    v1.25.4   192.168.68.151   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14 "

apiVersion: apps/v1
kind: Deployment
metadata:
  name: ndselector
  labels:
    app: ndselector
spec:
   replicas: 4
   selector:
     matchLabels:
       app: ndselector
   template:
     metadata:
       labels:
         app: ndselector
     spec:
       containers:
        - name: nginx
          image: nginx
          ports:
           - containerPort: 80
                                  "
hai@master:~$ kubectl get pods -o wide
NAME                          READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES
ndselector-74ff8bbfb5-58czk   1/1     Running   0          24s   10.44.0.4   "worker2  " <none>           <none>
ndselector-74ff8bbfb5-8rfkl   1/1     Running   0          24s   10.44.0.2   "worker2 "  <none>           <none>
ndselector-74ff8bbfb5-bzdn5   1/1     Running   0          24s   10.44.0.3   "worker2 "  <none>           <none>
ndselector-74ff8bbfb5-hm9t7   1/1     Running   0          24s   10.44.0.1   "worker2 "  <none>           <none>

========================+++++++++++++"

hai@master:~$ kubectl get nodes -o wide
NAME      STATUS                     ROLES           AGE     VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
master    Ready                      control-plane   6d21h   v1.25.4   192.168.68.150   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14
"worker1   Ready,SchedulingDisabled "  <none>          6d7h    v1.25.4   192.168.68.145   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14
worker2   Ready                      <none>          6d8h    v1.25.4   192.168.68.151   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14

apiVersion: apps/v1
kind: Deployment
metadata:
  name: topology
  labels: 
     app: stopology
spec: 
  replicas: 4
  selector: 
     matchLabels: 
        app: stopology
  template:
    metadata:
       labels: 
         app: stopology
    spec: 
      nodeName: worker1           " ### importent ok best  "
      containers:                  "## see look "
         - name: nginx
           image: nginx
           ports:
             - containerPort: 80 
======"
hai@master:~$ kubectl get pods -o wide
No resources found in default namespace.

hai@master:~$ kubectl get pods -o wide
NAME                        READY   STATUS              RESTARTS   AGE   IP       NODE      NOMINATED NODE   READINESS GATES
topology-85649bdbc9-krvqb   0/1     ContainerCreating   0          3s    <none>   worker1   <none>           <none>
topology-85649bdbc9-njv97   0/1     ContainerCreating   0          3s    <none>   worker1   <none>           <none>
topology-85649bdbc9-scbnx   0/1     ContainerCreating   0          3s    <none>   worker1   <none>           <none>
topology-85649bdbc9-vf6w9   0/1     ContainerCreating   0          3s    <none>   worker1   <none>           <none>

hai@master:~$ kubectl get pods -o wide
NAME                        READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES
topology-85649bdbc9-krvqb   1/1     Running   0          88s   10.36.0.4   worker1   <none>           <none>
topology-85649bdbc9-njv97   1/1     Running   0          88s   10.36.0.2   worker1   <none>           <none>
topology-85649bdbc9-scbnx   1/1     Running   0          88s   10.36.0.3   worker1   <none>           <none>
topology-85649bdbc9-vf6w9   1/1     Running   0          88s   10.36.0.1   worker1   <none>           <none>

====================================##############################"

hai@master:~$ kubectl get nodes -o wide
NAME      STATUS                     ROLES           AGE     VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
master    Ready                      control-plane   6d21h   v1.25.4   192.168.68.150   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14
"worker1   Ready,SchedulingDisabled "  <none>          6d7h    v1.25.4   192.168.68.145   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14
worker2   Ready                      <none>          6d8h    v1.25.4   192.168.68.151   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14 "

apiVersion: apps/v1
kind: Deployment
metadata: 
  name: ndselector
  labels:
    app: ndselector
spec:
   replicas: 4
   selector:
     matchLabels:
       app: ndselector
   template:    
     metadata: 
       labels:
         app: ndselector
     spec:
       nodeSelector:      ## see look 
          cpu: intel      ## importent  see look  ok best 
       containers:      
        - name: nginx
          image: nginx
          ports: 
           - containerPort: 80           "    

No resources found in default namespace.
hai@master:~$ kubectl get pods -o wide
NAME                          READY   STATUS    RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES
ndselector-6cffc794bd-44ffx   0/1     "Pending  " 0          1s    <none>   <none>   <none>           <none>
ndselector-6cffc794bd-46ggk   0/1     "Pending "  0          1s    <none>   <none>   <none>           <none>
ndselector-6cffc794bd-ngn2z   0/1   "  Pending  " 0          1s    <none>   <none>   <none>           <none>
ndselector-6cffc794bd-xfdsd   0/1     "Pending"   0          1s    <none>   <none>   <none>           <none>"

hai@master:~$ kubectl get nodes -o wide
NAME      STATUS                     ROLES           AGE     VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
master    Ready                      control-plane   6d21h   v1.25.4   192.168.68.150   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14
"worker1   Ready,SchedulingDisabled"   <none>          6d7h    v1.25.4   192.168.68.145   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14
worker2   Ready                      <none>          6d8h    v1.25.4   192.168.68.151   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14 "

hai@master:~/test$ kubectl uncordon worker1
node/worker1 uncordoned

hai@master:~$ kubectl get nodes -o wide
NAME      STATUS   ROLES           AGE     VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
master    Ready    control-plane   6d21h   v1.25.4   192.168.68.150   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14
"worker1   Ready "   <none>          6d7h    v1.25.4   192.168.68.145   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14
worker2   Ready    <none>          6d8h    v1.25.4   192.168.68.151   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14

hai@master:~$ kubectl get pods -o wide
NAME                          READY   STATUS    RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES
ndselector-6cffc794bd-44ffx   0/1     Pending   0          10m   <none>   <none>   <none>           <none>
ndselector-6cffc794bd-46ggk   0/1     Pending   0          10m   <none>   <none>   <none>           <none>
ndselector-6cffc794bd-ngn2z   0/1     Pending   0          10m   <none>   <none>   <none>           <none>
ndselector-6cffc794bd-xfdsd   0/1     Pending   0          10m   <none>   <none>   <none>           <none>"
hai@master:~$
hai@master:~$ kubectl get pods -o wide
NAME                          READY   STATUS              RESTARTS   AGE   IP       NODE      NOMINATED NODE   READINESS GATES
ndselector-6cffc794bd-44ffx   0/1     "ContainerCreating "  0          11m   <none>   worker1   <none>           <none>
ndselector-6cffc794bd-46ggk   0/1     ContainerCreating   0          11m   <none>   worker1   <none>           <none>
ndselector-6cffc794bd-ngn2z   0/1     ContainerCreating   0          11m   <none>   worker1   <none>           <none>
ndselector-6cffc794bd-xfdsd   0/1    " ContainerCreating "  0          11m   <none>   worker1   <none>           <none>"

hai@master:~$ kubectl get pods -o wide
NAME                          READY   STATUS              RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES
ndselector-6cffc794bd-44ffx   0/1     ContainerCreating   0          11m   <none>      worker1   <none>           <none>
ndselector-6cffc794bd-46ggk   0/1     ContainerCreating   0          11m   <none>      worker1   <none>           <none>
ndselector-6cffc794bd-ngn2z   1/1     "Running "            0          11m   10.36.0.1   worker1   <none>           <none>
ndselector-6cffc794bd-xfdsd   0/1     ContainerCreating   0          11m   <none>      worker1   <none>           <none>"

hai@master:~$ kubectl get pods -o wide
NAME                          READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES
ndselector-6cffc794bd-44ffx   1/1     Running   0          11m   10.36.0.4   worker1   <none>           <none>
ndselector-6cffc794bd-46ggk   1/1     Running   0          11m   10.36.0.3   worker1   <none>           <none>
ndselector-6cffc794bd-ngn2z   1/1     Running   0          11m   10.36.0.1   worker1   <none>           <none>
ndselector-6cffc794bd-xfdsd   1/1     Running   0          11m   10.36.0.2   worker1   <none>           <none>

############################################################ "

affinity and anti affinity kubernetes 

https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
https://www.densify.com/kubernetes-autoscaling/kubernetes-affinity
https://blog.kubecost.com/blog/kubernetes-node-affinity/
https://www.howtogeek.com/devops/what-is-pod-affinity-and-anti-affinity-in-kubernetes/


apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-cache
spec:
  selector:
    matchLabels:
      app: store
  replicas: 3
  template:
    metadata:
      labels:
        app: store
    spec:
      affinity:                      ##  importent  see look  
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - store
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: redis-server
        image: redis:3.2-alpine
--"
hai@master:~$ kubectl get nodes -o wide
NAME      STATUS   ROLES           AGE     VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
master    Ready    control-plane   6d23h   v1.25.4   192.168.68.150   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14
"worker1  " Ready    <none>          6d9h    v1.25.4   192.168.68.145   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14
"worker2 "  Ready    <none>          6d10h   v1.25.4   192.168.68.151   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.6.14"
---
hai@master:~$ kubectl get pods -o wide
NAME                          READY   STATUS              RESTARTS   AGE   IP       NODE      NOMINATED NODE   READINESS GATES
redis-cache-7c5dd87dd-8kdk5   0/1     Pending             0          0s    <none>   <none>    <none>           <none>
redis-cache-7c5dd87dd-kmbdn   0/1     "ContainerCreating   0          0s    <none>   worker1"   <none>           <none>
redis-cache-7c5dd87dd-pxw7f   0/1     "ContainerCreating   0          0s    <none>   worker2"   <non"

hai@master:~$ kubectl get pods -o wide
NAME                          READY   STATUS    RESTARTS   AGE    IP          NODE      NOMINATED NODE   READINESS GATES
redis-cache-7c5dd87dd-8kdk5   0/1    " Pending   0          5m3s   <none>   "   <none>    <none>           <none>
redis-cache-7c5dd87dd-kmbdn   1/1     Running   0          5m3s   10.36.0.1   worker1   <none>           <none>
redis-cache-7c5dd87dd-pxw7f   1/1     Running   0          5m3s   10.44.0.1   worker2   <none>           <none>"

hai@master:~$ kubectl get all -o wide
NAME                              READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES
pod/redis-cache-7c5dd87dd-8kdk5   "0/1     Pending   0          11m   <none>  "    <none>    <none>           <none>
pod/redis-cache-7c5dd87dd-kmbdn   1/1     Running   0          11m   10.36.0.1   worker1   <none>           <none>
pod/redis-cache-7c5dd87dd-pxw7f   1/1     Running   0          11m   10.44.0.1   worker2   <none>           <none>

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE   SELECTOR
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   32h   <none>

NAME                          READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS     IMAGES             SELECTOR
deployment.apps/redis-cache  " 2/3     3            2      "     11m   redis-server   redis:3.2-alpine   app=store

NAME                                    DESIRED   CURRENT   READY   AGE   CONTAINERS     IMAGES             SELECTOR
replicaset.apps/redis-cache-7c5dd87dd   "3         3         2  "     11m   redis-server   redis:3.2-alpine   app=store,pod-template-hash=7c5dd87dd
========+++++++++++++++++++++++++++++
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-server
spec:
  selector:
    matchLabels:
      app: web-store
  replicas: 3
  template:
    metadata:
      labels:
        app: web-store
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - web-store
            topologyKey: "kubernetes.io/hostname"
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - store
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: web-app
        image: nginx:1.16-alpine

hai@master:~$ kubectl get pods -o wide
NAME                          READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES
redis-cache-7c5dd87dd-bvcmf   1/1     Running   0          2m31s   10.36.0.1   worker1   <none>           <none>
redis-cache-7c5dd87dd-lmmft   0/1     Pending   0          2m31s   <none>      <none>    <none>           <none>
redis-cache-7c5dd87dd-m55cv   1/1     Running   0          2m31s   10.44.0.1   worker2   <none>           <none>
web-server-54d4dcb578-4rv4z   1/1     Running   0          2m      10.44.0.2   worker2   <none>           <none>
web-server-54d4dcb578-4tctl   0/1     Pending   0          2m      <none>      <none>    <none>           <none>
web-server-54d4dcb578-h6nzp   1/1     Running   0          2m      10.36.0.2   worker1   <none>           <none>
############################################################################ "
       "  #%%#$%#$^______________  staticpod _______+============#%#$"
https://kubernetes.io/docs/tasks/configure-pod-container/static-pod/   

hai@master:~$ kubectl get pods -n kube-system  ## "static pods "
NAME                             READY   STATUS    RESTARTS        AGE
coredns-565d847f94-q5zw9         1/1     Running   471 (23m ago)   6d20h
coredns-565d847f94-rg789         1/1     Running   460 (23m ago)   6d20h
etcd-master                      1/1     Running   16 (23m ago)    7d13h
kube-apiserver-master            1/1     Running   12 (23m ago)    6d19h
kube-controller-manager-master   1/1     Running   22 (23m ago)    7d13h
kube-proxy-6t6qp                 1/1     Running   16 (23m ago)    7d13h
kube-proxy-ffdgc                 1/1     Running   14 (24m ago)    7d
kube-proxy-gq4v9                 1/1     Running   13 (24m ago)    6d23h
kube-scheduler-master            1/1     Running   21 (23m ago)    7d13h
weave-net-6bpzq                  2/2     Running   36 (23m ago)    7d13h
weave-net-8gkx4                  2/2     Running   32 (22m ago)    7d
weave-net-f27sl                  2/2     Running   27 (24m ago)    6d23h "

hai@master:~$ kubectl get ns     ## " static pods "
NAME              STATUS   AGE
default           Active   7d13h
kube-node-lease   Active   7d13h
kube-public       Active   7d13h
kube-system       Active   7d13h
project1          Active   6d22h
project2          Active   6d22h

static pods  are always managed by the " KUBELETS "
static pods are never managed by the "MASTER NODE "

scheduler  is not  aware of any of the static pods 

static pods  are  always created and  managed by kubelet  agent running in  each and every node 


hai@ubuntu:~/test$ kubectl get pods  -o wide --show-labels
No resources found in default namespace "

root@ubuntu:/etc/kubernetes/manifests# ll
total 24
drwxr-xr-x 2 root root 4096 Dec 25 23:49 ./
drwxr-xr-x 4 root root 4096 Dec 25 23:49 ../
-rw------- 1 root root 2382 Dec 25 23:49 etcd.yaml
-rw------- 1 root root 4019 Dec 25 23:49 kube-apiserver.yaml
-rw------- 1 root root 3520 Dec 25 23:49 kube-controller-manager.yaml
-rw------- 1 root root 1440 Dec 25 23:49 kube-scheduler.yaml "

root@ubuntu:~# cd /var/lib/kubelet/
root@ubuntu:/var/lib/kubelet# ll
total 48
drwx------  8 root root 4096 Dec 25 23:49 ./
drwxr-xr-x 74 root root 4096 Dec 26 00:00 ../
-rw-r--r--  1 root root 1015 Dec 25 23:49 config.yaml
-rw-------  1 root root   62 Dec 25 23:49 cpu_manager_state
drwxr-xr-x  2 root root 4096 Dec 25 23:50 device-plugins/
-rw-r--r--  1 root root  176 Dec 25 23:49 kubeadm-flags.env
-rw-------  1 root root   61 Dec 25 23:49 memory_manager_state
drwxr-xr-x  2 root root 4096 Dec 25 23:49 pki/
drwxr-x---  2 root root 4096 Dec 25 23:49 plugins/
drwxr-x---  2 root root 4096 Dec 25 23:49 plugins_registry/
drwxr-x---  2 root root 4096 Dec 25 23:50 pod-resources/
drwxr-x--- 10 root root 4096 Dec 26 00:00 pods/ "

root@ubuntu:/var/lib/kubelet# cat config.yaml
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
shutdownGracePeriod: 0s
shutdownGracePeriodCriticalPods: 0s
"staticPodPath: /etc/kubernetes/manifests"    ## importent  see look 
streamingConnectionIdleTimeout: 0s
syncFrequency: 0s
volumeStatsAggPeriod: 0s "

hai@ubuntu:~/test$ cd /etc/kubernetes/manifests/
hai@ubuntu:/etc/kubernetes/manifests$ ll
total 24
drwxr-xr-x 2 root root 4096 Dec 25 23:49 ./
drwxr-xr-x 4 root root 4096 Dec 25 23:49 ../
-rw------- 1 root root 2382 Dec 25 23:49 etcd.yaml
-rw------- 1 root root 4019 Dec 25 23:49 kube-apiserver.yaml
-rw------- 1 root root 3520 Dec 25 23:49 kube-controller-manager.yaml
-rw------- 1 root root 1440 Dec 25 23:49 kube-scheduler.yaml "

hai@ubuntu:/etc/kubernetes/manifests$ kubectl get pods -n
error: flag needs an argument: 'n' in -n
See 'kubectl get --help' for usage.
hai@ubuntu:/etc/kubernetes/manifests$ kubectl get  namespace
NAME              STATUS   AGE
default           Active   72m
kube-node-lease   Active   72m
kube-public       Active   72m
kube-system       Active   72m  "

hai@ubuntu:/etc/kubernetes/manifests$ kubectl get pods  -n kube-system  
NAME                             READY   STATUS    RESTARTS      AGE
coredns-565d847f94-qpcn7         1/1     Running   0             73m
coredns-565d847f94-x2kzb         1/1     Running   0             73m
etcd-ubuntu                      1/1     Running   0             73m
kube-apiserver-ubuntu            1/1     Running   0             73m
kube-controller-manager-ubuntu   1/1     Running   0             73m
kube-proxy-c9jk5                 1/1     Running   0             30m
kube-proxy-g69ms                 1/1     Running   0             73m
kube-proxy-hd7k2                 1/1     Running   0             41m
kube-scheduler-ubuntu            1/1     Running   0             73m
weave-net-m9pzz                  2/2     Running   1 (63m ago)   63m
weave-net-mh2f8                  2/2     Running   0             41m
weave-net-t459v                  2/2     Running   0             30m "

root@ubuntu:/etc/kubernetes/manifests# ll
total 28
drwxr-xr-x 2 root root 4096 Dec 26 01:26 ./
drwxr-xr-x 4 root root 4096 Dec 25 23:49 ../
-rw------- 1 root root 2382 Dec 25 23:49 etcd.yaml
-rw------- 1 root root 4019 Dec 25 23:49 kube-apiserver.yaml
-rw------- 1 root root 3520 Dec 25 23:49 kube-controller-manager.yaml
-rw------- 1 root root 1440 Dec 25 23:49 kube-scheduler.yaml
-rw-r--r-- 1 root root  123 Dec 26 01:25 "withoutcommand.yaml" "  ## importent look see "

"withoutcommand.yaml"
apiVersion: v1      
kind: Pod
metadata:
  name:" kubelet-automated-test"
spec:
  containers:
    - name: web
      image: nginx  "

hai@ubuntu:~$ kubectl get pods -o wide
No resources found in default namespace.

hai@ubuntu:~$ kubectl get pods -o wide
NAME                            READY   STATUS              RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATE                                                   S
kubelet-automated-test-ubuntu   0/1     ContainerCreating   0          3s    <none>  " ubuntu "  <none>           <none>

hai@ubuntu:~$ kubectl get pods -o wide
NAME                            READY   STATUS    RESTARTS   AGE    IP          NODE     NOMINATED NODE   READINESS GATES
"kubelet-automated-test-ubuntu "  1/1     Running   0          112s   10.32.0.4  " ubuntu "  <none>           <none>"

root@ubuntu:/etc/kubernetes/manifests# mv withoutcommand.yaml /

root@ubuntu:/etc/kubernetes/manifests# ll
total 24
drwxr-xr-x 2 root root 4096 Dec 26 01:42 ./
drwxr-xr-x 4 root root 4096 Dec 25 23:49 ../
-rw------- 1 root root 2382 Dec 25 23:49 etcd.yaml
-rw------- 1 root root 4019 Dec 25 23:49 kube-apiserver.yaml
-rw------- 1 root root 3520 Dec 25 23:49 kube-controller-manager.yaml
-rw------- 1 root root 1440 Dec 25 23:49 kube-scheduler.yaml

hai@ubuntu:~$ kubectl get pods -o wide
No resources found in default namespace."

root@ubuntu:/etc/kubernetes/manifests# mv /withoutcommand.yaml .
root@ubuntu:/etc/kubernetes/manifests# ll
total 28
drwxr-xr-x 2 root root 4096 Dec 26 01:45 ./
drwxr-xr-x 4 root root 4096 Dec 25 23:49 ../
-rw------- 1 root root 2382 Dec 25 23:49 etcd.yaml
-rw------- 1 root root 4019 Dec 25 23:49 kube-apiserver.yaml
-rw------- 1 root root 3520 Dec 25 23:49 kube-controller-manager.yaml
-rw------- 1 root root 1440 Dec 25 23:49 kube-scheduler.yaml
-rw-r--r-- 1 root root  123 Dec 26 01:25 withoutcommand.yaml"

hai@ubuntu:~$ kubectl get pods -o wide
NAME                            READY   STATUS    RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES
kubelet-automated-test-ubuntu   0/1     Pending   0          2s    <none>   "ubuntu  " <none>           <none>
hai@ubuntu:~$ kubectl get pods -o wide
NAME                            READY   STATUS    RESTARTS   AGE   IP          NODE     NOMINATED NODE   READINESS GATES
"kubelet-automated-test-ubuntu"   1/1     Running   0          4s    10.32.0.4   '"ubuntu  "' <none>           <none>"

hai@ubuntu:~/test$ kubectl delete pod  kubelet-automated-test-ubuntu
pod "kubelet-automated-test-ubuntu" deleted

hai@ubuntu:~$ kubectl get pods -o wide
NAME                            READY   STATUS    RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES
kubelet-automated-test-ubuntu   0/1     Pending   0          6s    <none>   ubuntu   <none>           <none>"

hai@ubuntu:~$ kubectl get pods -o wide
NAME                            READY   STATUS    RESTARTS   AGE   IP          NODE     NOMINATED NODE   READINESS GATES
kubelet-automated-test-ubuntu   1/1     Running   0          8s    10.32.0.4   ubuntu   <none>           <none>

###############################################################"

hai@ubuntu:~/test$ kubectl get pods -o wide
NAME                            READY   STATUS    RESTARTS   AGE   IP          NODE     NOMINATED NODE   READINESS GATES
"kubelet-automated-test-ubuntu  " 1/1     Running   0          77m   10.32.0.4   ubuntu   <none>           <none>

root@worker1:/etc/kubernetes# ll
total 24
drwxr-xr-x   3 root root  4096 Dec 26 00:22 ./
drwxr-xr-x 138 root root 12288 Dec 26 00:21 ../
-rw-------   1 root root  1955 Dec 26 00:22 kubelet.conf
drwxr-xr-x   2 root root  4096 Dec 26 00:22 pki/ "

root@worker1:/etc/kubernetes# ll
total 28
drwxr-xr-x   3 root root  4096 Dec 26 03:18 ./
drwxr-xr-x 138 root root 12288 Dec 26 00:21 ../
-rw-------   1 root root  1955 Dec 26 00:22 kubelet.conf
drwxr-xr-x   2 root root  4096 Dec 26 00:22 pki/ "

root@worker1:/etc/kubernetes# mkdir manifests
root@worker1:/etc/kubernetes# ll
total 32
drwxr-xr-x   4 root root  4096 Dec 26 03:31 ./
drwxr-xr-x 138 root root 12288 Dec 26 00:21 ../
-rw-------   1 root root  1955 Dec 26 00:22 kubelet.conf
drwxr-xr-x   2 root root  4096 Dec 26 03:32 manifests/
drwxr-xr-x   2 root root  4096 Dec 26 00:22 pki/
-rw-r--r--   1 root root   116 Dec 26 03:28 pod.yaml
root@worker1:/etc/kubernetes# cd manifests/
root@worker1:/etc/kubernetes/manifests# nano pod.yaml  "

apiVersion: v1
kind: Pod
metadata:
  name: worker1pod
spec:
  containers:
    - name: worker1pod
      image: nginx      "
hai@ubuntu:~$ kubectl get pods -o wide
NAME                            READY   STATUS    RESTARTS   AGE   IP          NODE     NOMINATED NODE   READINESS GATES
kubelet-automated-test-ubuntu   1/1     Running   0          94m   10.32.0.4   ubuntu   <none>           <none> "

hai@ubuntu:~$ kubectl get pods -o wide
NAME                            READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES
kubelet-automated-test-ubuntu   1/1     Running   0          98m   10.32.0.4   ubuntu    <none>           <none>
"worker1pod-worker1 "             0/1     Pending   0          6s    <none>      worker1   <none>           <none>"

hai@ubuntu:~$ kubectl get pods -o wide
NAME                            READY   STATUS              RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES
kubelet-automated-test-ubuntu   1/1     Running             0          98m   10.32.0.4   ubuntu    <none>           <none>
"worker1pod-worker1"              0/1     ContainerCreating   0          7s    <none>      "worker1 "  <none>           <none>

hai@ubuntu:~$ kubectl get pods -o wide
NAME                            READY   STATUS    RESTARTS   AGE    IP          NODE      NOMINATED NODE   READINESS GATES
kubelet-automated-test-ubuntu   1/1     Running   0          109m   10.32.0.4   ubuntu    <none>           <none>
"worker1pod-worker1 "             1/1     Running   0          11m    10.44.0.1   "worker1  " <none>           <none>

====+++++++++++"

root@worker2:~# cd /etc/kubernetes/
root@worker2:/etc/kubernetes# mkdir manifests
root@worker2:/etc/kubernetes# ll
total 28
drwxr-xr-x   4 root root  4096 Dec 26 03:48 ./
drwxr-xr-x 137 root root 12288 Dec 26 00:30 ../
-rw-------   1 root root  1955 Dec 26 00:32 kubelet.conf
drwxr-xr-x   2 root root  4096 Dec 26 03:48 manifests/
drwxr-xr-x   2 root root  4096 Dec 26 00:32 pki/
root@worker2:/etc/kubernetes# cd manifests/
root@worker2:/etc/kubernetes/manifests# ll
total 8
drwxr-xr-x 2 root root 4096 Dec 26 03:48 ./
drwxr-xr-x 4 root root 4096 Dec 26 03:48 ../
root@worker2:/etc/kubernetes/manifests# nano worker2-to-master.yaml
root@worker2:/etc/kubernetes/manifests# ll
total 12
drwxr-xr-x 2 root root 4096 Dec 26 03:50 ./
drwxr-xr-x 4 root root 4096 Dec 26 03:48 ../
-rw-r--r-- 1 root root  122 Dec 26 03:50 worker2-to-master.yaml  "

apiVersion: v1
kind: Pod
metadata:
  name: from-worker2pod
spec:
  containers:
    - name: worker2pod
      image: nginx   "

hai@ubuntu:~$ kubectl get pods -o wide
NAME                            READY   STATUS    RESTARTS   AGE    IP          NODE      NOMINATED NODE   READINESS GATES
"from-worker2pod-worker2  "       1/1     Running   0          5s     10.47.0.1   "worker2 "  <none>           <none>
kubelet-automated-test-ubuntu   1/1     Running   0          115m   10.32.0.4   ubuntu    <none>           <none>
worker1pod-worker1              1/1     Running   0          17m    10.44.0.1   worker1   <none>           <none>

#################################################"

Persistent Volumes 

 PersistentVolume (PV) := is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using Storage Classes.

PersistentVolumeClaim (PVC) =:  is a request for storage by a user. It is similar to a Pod. Pods consume node resources and PVCs consume PV resources.

Types of Persistent Volumes  : = 
PersistentVolume types are implemented as plugins. Kubernetes currently supports the following plugins:

cephfs - CephFS volume
csi - Container Storage Interface (CSI)
fc - Fibre Channel (FC) storage
hostPath - HostPath volume (for single node testing only; WILL NOT WORK in a multi-node cluster; consider using local volume instead)
iscsi - iSCSI (SCSI over IP) storage
local - local storage devices mounted on nodes.
nfs - Network File System (NFS) storage
rbd - Rados Block Device (RBD) volume "

The following types of PersistentVolume are deprecated. This means that support is still available but will be removed in a future Kubernetes release.
awsElasticBlockStore - AWS Elastic Block Store (EBS) (deprecated in v1.17)
azureDisk - Azure Disk (deprecated in v1.19)
azureFile - Azure File (deprecated in v1.21)
cinder - Cinder (OpenStack block storage) (deprecated in v1.18)
flexVolume - FlexVolume (deprecated in v1.23)
gcePersistentDisk - GCE Persistent Disk (deprecated in v1.17)
portworxVolume - Portworx volume (deprecated in v1.25)
vsphereVolume - vSphere VMDK volume (deprecated in v1.19) "

Older versions of Kubernetes also supported the following in-tree PersistentVolume types:
photonPersistentDisk - Photon controller persistent disk. (not available starting v1.15)
scaleIO - ScaleIO volume (not available starting v1.21)
flocker - Flocker storage (not available starting v1.25)
quobyte - Quobyte volume (not available starting v1.25)
storageos - StorageOS volume (not available starting v1.25)
Persistent Volumes

RWO - ReadWriteOnce
ROX - ReadOnlyMany
RWX - ReadWriteMany
RWOP - ReadWriteOncePod

awsElasticBlockStore
azureDisk
azureFile
cephfs
cinder (deprecated in v1.18)
gcePersistentDisk
iscsi
nfs
rbd
vsphereVolume

======++++++++++++++++++++++++++++++++++++++++++//////\\\\\\\\
How To Set Up an NFS Mount on Ubuntu 20.04

https://www.tecmint.com/install-nfs-server-on-ubuntu/       best

sudo apt install nfs-kernel-server
 sudo mkdir -p /mnt/nfs_share

root@ubuntu:/mnt# ll
total 12
drwxr-xr-x  3 root root 4096 Dec 26 21:34 ./
drwxr-xr-x 20 root root 4096 Nov 24 21:53 ../
drwxr-xr-x  2 root root 4096 Dec 26 21:34 nfs_share/

root@ubuntu:~#  sudo chown -R nobody:nogroup /mnt/nfs_share/
root@ubuntu:/mnt# ll
total 12
drwxr-xr-x  3 root   root    4096 Dec 26 21:34 ./
drwxr-xr-x 20 root   root    4096 Nov 24 21:53 ../
drwxr-xr-x  2 "nobody nogroup" 4096 Dec 26 21:34 nfs_share/

root@ubuntu:/mnt# sudo chmod 777 /mnt/nfs_share/
root@ubuntu:/mnt# ll
total 12
drwxr-xr-x  3 root   root    4096 Dec 26 21:34 ./
drwxr-xr-x 20 root   root    4096 Nov 24 21:53 ../
drwxrwxrwx  2 nobody nogroup 4096 Dec 26 21:34 nfs_share/

sudo nano  /etc/exports
/mnt/nfs_share  192.168.68.133/24(rw,sync,no_subtree_check)
   #// OR  OR  OR 
/mnt/nfs_share *(rw,sync,no_root_squash,no_subtree_check)
  

root@ubuntu:~# cat /etc/exports
# /etc/exports: the access control list for filesystems which may be exported
#               to NFS clients.  See exports(5).
#
# Example for NFSv2 and NFSv3:
# /srv/homes       hostname1(rw,sync,no_subtree_check) hostname2(ro,sync,no_subtree_check)
#(rw,sync,no_root_squash,no_subtree_check)
#  
# Example for NFSv4:
# /srv/nfs4        gss/krb5i(rw,sync,fsid=0,crossmnt,no_subtree_check)
# /srv/nfs4/homes  gss/krb5i(rw,sync,no_subtree_check)
#
/mnt/nfs_share  192.168.68.133/24(rw,sync,no_subtree_check)
   # OR  OR  
   /mnt/nfs_share *(rw,sync,no_root_squash,no_subtree_check)


     rw: Stands for Read/Write.
sync: Requires changes to be written to the disk before they are applied.
No_subtree_check: Eliminates subtree checking.
-----
sudo systemctl restart nfs-kernel-server

root@nfs:~# sudo systemctl restart nfs-kernel-server
root@nfs:~# sudo systemctl enable  nfs-kernel-server
Synchronizing state of nfs-kernel-server.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable nfs-kernel-server

root@ubuntu:~#  sudo exportfs -a

root@ubuntu:~#  sudo systemctl restart nfs-kernel-server

root@nfs:~# exportfs -v
/mnt/nfs_share	<world>(rw,wdelay,no_root_squash,no_subtree_check,sec=sys,rw,secure,no_root_squash,no_all_squash)

root@ubuntu:~# sudo ufw allow from 192.168.68.133/24  to any port nfs
WARN: Rule changed after normalization
Rules updated
---
root@ubuntu:~# sudo ufw status
Status: active
To                         Action      From
--                         ------      ----
2049                       ALLOW       192.168.68.0/24
--
sudo apt update -y
sudo apt install nfs-common

root@ubuntu:/mnt# sudo mkdir -p /mnt/nfs_clientshare
root@ubuntu:/mnt# ll
total 16
drwxr-xr-x  4 root   root    4096 Dec 26 22:03 ./
drwxr-xr-x 20 root   root    4096 Nov 24 21:53 ../
drwxr-xr-x  2 root   root    4096 Dec 26 22:03 nfs_clientshare/
drwxrwxrwx  2 nobody nogroup 4096 Dec 26 21:34 nfs_share/

root@ubuntu:/mnt# ifconfig
ens33: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 192.168.68.153  netmask 255.255.255.0  broadcast 192.168.68.255

sudo mount 192.168.68.153:/mnt/nfs_share  /mnt/nfs_clientshare

root@ubuntu:/mnt/nfs_share# touch fromnfsshare  a  b  c d 
root@ubuntu:/mnt/nfs_share# ll
total 8
drwxrwxrwx 2 nobody nogroup 4096 Dec 26 22:16 ./
drwxr-xr-x 4 root   root    4096 Dec 26 22:03 ../
-rw-r--r-- 1 root   root       0 Dec 26 22:08 a
-rw-r--r-- 1 root   root       0 Dec 26 22:08 b
-rw-r--r-- 1 root   root       0 Dec 26 22:08 c
-rw-r--r-- 1 root   root       0 Dec 26 22:08 d
-rw-r--r-- 1 root   root       0 Dec 26 22:16 fromnfsshare

root@ubuntu:/mnt/nfs_clientshare# ll
total 8
drwxrwxrwx 2 nobody nogroup 4096 Dec 26 22:16 ./
drwxr-xr-x 4 root   root    4096 Dec 26 22:03 ../
-rw-r--r-- 1 root   root       0 Dec 26 22:08 a
-rw-r--r-- 1 root   root       0 Dec 26 22:08 b
-rw-r--r-- 1 root   root       0 Dec 26 22:08 c
-rw-r--r-- 1 root   root       0 Dec 26 22:08 d
-rw-r--r-- 1 root   root       0 Dec 26 22:16 fromnfsshare

############################################################

https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#persistentvolume-v1-core

PersistentVolume v1 core
Group    	Version   	Kind
core     	v1	          PersistentVolume

PersistentVolumeSpec v1 core
 Appears In:
PersistentVolume [core/v1]
VolumeAttachmentSource [storage/v1]
Field	Description
accessModes
string array	accessModes contains all ways the volume can be mounted. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#access-modes

awsElasticBlockStore
AWSElasticBlockStoreVolumeSource	awsElasticBlockStore represents an AWS Disk resource that is attached to a kubelet's host machine and then exposed to the pod. More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore

azureDisk
AzureDiskVolumeSource	azureDisk represents an Azure Data Disk mount on the host and bind mount to the pod.

azureFile
AzureFilePersistentVolumeSource	azureFile represents an Azure File Service mount on the host and bind mount to the pod.

capacity
object	capacity is the description of the persistent volume's resources and capacity. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#capacity

cephfs
CephFSPersistentVolumeSource	cephFS represents a Ceph FS mount on the host that shares a pod's lifetime

cinder
CinderPersistentVolumeSource	cinder represents a cinder volume attached and mounted on kubelets host machine. More info: https://examples.k8s.io/mysql-cinder-pd/README.md

claimRef
ObjectReference	claimRef is part of a bi-directional binding between PersistentVolume and PersistentVolumeClaim. Expected to be non-nil when bound. claim.VolumeName is the authoritative bind between PV and PVC. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#binding

csi
CSIPersistentVolumeSource	csi represents storage that is handled by an external CSI driver (Beta feature).

fc
FCVolumeSource	fc represents a Fibre Channel resource that is attached to a kubelet's host machine and then exposed to the pod.

flexVolume
FlexPersistentVolumeSource	flexVolume represents a generic volume resource that is provisioned/attached using an exec based plugin.

flocker
FlockerVolumeSource	flocker represents a Flocker volume attached to a kubelet's host machine and exposed to the pod for its usage. This depends on the Flocker control service being running

gcePersistentDisk
GCEPersistentDiskVolumeSource	gcePersistentDisk represents a GCE Disk resource that is attached to a kubelet's host machine and then exposed to the pod. Provisioned by an admin. More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk

glusterfs
GlusterfsPersistentVolumeSource	glusterfs represents a Glusterfs volume that is attached to a host and exposed to the pod. Provisioned by an admin. More info: https://examples.k8s.io/volumes/glusterfs/README.md

hostPath
HostPathVolumeSource	hostPath represents a directory on the host. Provisioned by a developer or tester. This is useful for single-node development and testing only! On-host storage is not supported in any way and WILL NOT WORK in a multi-node cluster. More info: https://kubernetes.io/docs/concepts/storage/volumes#hostpath

iscsi
ISCSIPersistentVolumeSource	iscsi represents an ISCSI Disk resource that is attached to a kubelet's host machine and then exposed to the pod. Provisioned by an admin.

local
LocalVolumeSource	local represents directly-attached storage with node affinity

mountOptions
string array	mountOptions is the list of mount options, e.g. ["ro", "soft"]. Not validated - mount will simply fail if one is invalid. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#mount-options

nfs
NFSVolumeSource	nfs represents an NFS mount on the host. Provisioned by an admin. More info: https://kubernetes.io/docs/concepts/storage/volumes#nfs

nodeAffinity
VolumeNodeAffinity	nodeAffinity defines constraints that limit what nodes this volume can be accessed from. This field influences the scheduling of pods that use this volume.

persistentVolumeReclaimPolicy
string	persistentVolumeReclaimPolicy defines what happens to a persistent volume when released from its claim. Valid options are Retain (default for manually created PersistentVolumes), Delete (default for dynamically provisioned PersistentVolumes), and Recycle (deprecated). Recycle must be supported by the volume plugin underlying this PersistentVolume. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#reclaiming

photonPersistentDisk
PhotonPersistentDiskVolumeSource	photonPersistentDisk represents a PhotonController persistent disk attached and mounted on kubelets host machine

portworxVolume
PortworxVolumeSource	portworxVolume represents a portworx volume attached and mounted on kubelets host machine

quobyte
QuobyteVolumeSource	quobyte represents a Quobyte mount on the host that shares a pod's lifetime

rbd
RBDPersistentVolumeSource	rbd represents a Rados Block Device mount on the host that shares a pod's lifetime. More info: https://examples.k8s.io/volumes/rbd/README.md

scaleIO
ScaleIOPersistentVolumeSource	scaleIO represents a ScaleIO persistent volume attached and mounted on Kubernetes nodes.

storageClassName
string	storageClassName is the name of StorageClass to which this persistent volume belongs. Empty value means that this volume does not belong to any StorageClass.

storageos
StorageOSPersistentVolumeSource	storageOS represents a StorageOS volume that is attached to the kubelet's host machine and mounted into the pod More info: https://examples.k8s.io/volumes/storageos/README.md

volumeMode
string	volumeMode defines if a volume is intended to be used with a formatted filesystem or to remain in raw block state. Value of Filesystem is implied when not included in spec.
-----------+++"
Reclaim Policy

Retain -- manual reclamation

Recycle -- basic scrub (rm -rf /thevolume/*)

Delete -- associated storage asset such as AWS EBS, GCE PD, Azure Disk, or OpenStack Cinder volume is deleted

Currently, only NFS and HostPath support recycling. AWS EBS, GCE PD, Azure Disk, and Cinder volumes support deletion. "
-+++++++++++++++++++///\\/\\/\/\/\/\\/\/###############################\\/\/ "
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mypv
spec:
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  storageClassName: slow
  mountOptions:
    - hard
    - nfsvers=4.1
  nfs:
    path: /mnt/nfs_share
    server: 192.168.68.153   
---"
hai@ubuntu:~$ kubectl get pv --show-labels -o wide
NAME   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE   VOLUMEMODE   LABELS
mypv   5Gi        RWO            Recycle          Available           slow                    47s   Filesystem   <none> "

hai@ubuntu:~$ kubectl describe pv
Name:            mypv
Labels:          <none>
Annotations:     <none>
Finalizers:      [kubernetes.io/pv-protection]
StorageClass:    slow
Status:          Available
Claim:
"Reclaim Policy:  Recycle"
Access Modes:   " RWO"
VolumeMode:      Filesystem
Capacity:        5Gi
Node Affinity:   <none>
Message:
Source:
    Type:      NFS (an NFS mount that lasts the lifetime of a pod)
    Server:    192.168.68.153
    Path:      "/mnt/nfs_share"
    ReadOnly:  "false"
Events:        <none>
===---=== "++++++++++++++++++++++++++++"
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
   name: mypvc
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
     requests:
        storage: 1Gi
  storageClassName: slow
 

hai@ubuntu:~/test$ kubectl apply -f pvc.yaml 
error: "when path, url, or stdin is provided as input, you may not specify resource arguments as well"
ERROR  ERRORS  PV ERROR  VOLUME ERROR  {{}}}}/\\\/\/\/\/\/\/\/\/\/{{{{{{{{{{{{{{[[][[[[[[[]]]]]]]]}}}}}}}}}}}}}}

hai@nfs:~$ cat /etc/exports 
# /etc/exports: the access control list for filesystems which may be exported
#		to NFS clients.  See exports(5).           "  ##  SEE LOOK "
#
# Example for NFSv2 and NFSv3:
# /srv/homes       hostname1(rw,sync,no_subtree_check) hostname2(ro,sync,no_subtree_check)
#
# Example for NFSv4:
# /srv/nfs4        gss/krb5i(rw,sync,fsid=0,crossmnt,no_subtree_check)
# /srv/nfs4/homes  gss/krb5i(rw,sync,no_subtree_check)
#

/mnt/nfs_share * (rw,sync,no_root_squash)   "### see look "

## in  MASTER  MASTER 
root@ubuntu:~# systemctl start  firewalld

root@ubuntu:~# ufw enable 
Firewall is active and enabled on system startup

root@ubuntu:~# ufw status 
Status: active  "


hai@ubuntu:~$ kubectl get pv
NAME   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   "CLAIM "          STORAGECLASS   REASON   AGE
mypv   5Gi        RWO            Recycle          Bound    "default/mypvc "  slow                    4h9m

hai@ubuntu:~$ kubectl get pvc -o wide --show-labels
NAME    STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE    VOLUMEMODE   LABELS
mypvc  " Bound"    mypv     5Gi        RWO           " slow  "         107m   Filesystem   <none>

hai@ubuntu:~$ kubectl describe pvc
Name:          mypvc
Namespace:     default
StorageClass:  slow
Status:        "Bound"
Volume:        "mypv"
Labels:        <none>
Annotations:   pv.kubernetes.io/bind-completed: yes
               pv.kubernetes.io/bound-by-controller: yes
Finalizers:    [kubernetes.io/pvc-protection]
Capacity:      5Gi
Access Modes:  RWO
VolumeMode:    Filesystem
Used By:       <none>
Events:        <none>

############################################################ "

https://kubernetes.io/docs/concepts/storage/persistent-volumes/
https://kubernetes.io/search/?q=wordpress
https://kubernetes.io/docs/tutorials/stateful-application/mysql-wordpress-persistent-volume/  

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: wordpress-mysql
  labels:
    app: wordpress
spec:
  selector:
    matchLabels:
      app: wordpress
      tier: mysql
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: wordpress
        tier: mysql
    spec:
      containers:
      - image: mysql:5.6
        name: mysql
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: anji
        ports:
        - containerPort: 3306
          name: mysql
        volumeMounts:
        - name: mysql-persistent-storage
          mountPath: /var/lib/mysql
      volumes:
      - name: mysql-persistent-storage
        persistentVolumeClaim:
          claimName: mypvc
--=========="
hai@ubuntu:~$ kubectl get pods -o wide --show-labels
NAME                               READY   STATUS              RESTARTS   AGE   IP       NODE      NOMINATED NODE   READINESS GATES   LABELS
wordpress-mysql-7664b65dc8-k5b72   0/1     "ContainerCreating  " 0          42s   <none>   worker1   <none>           <none>            app=wordpress,pod-template-hash=7664b65dc8,tier=mysql


hai@ubuntu:~$ kubectl get pods -o wide --show-labels
NAME                               READY   STATUS              RESTARTS   AGE     IP       NODE      NOMINATED NODE   READINESS GATES   LABELS
wordpress-mysql-7664b65dc8-k5b72   0/1     ContainerCreating   0          2m18s   <none>   worker1   <none>           <none>            app=wordpress,pod-template-hash=7664b65dc8,tier=mysql"

apiVersion: v1
kind: PersistentVolume
metadata:
  name: mypv
  labels: 
    type: local
spec:   
  storageClassName: slow
  capacity:
    storage: 10Gi
  accessModes:
     - ReadWriteOnce
  hostPath:
     path: /mnt/data     "
 
anji@master:~$ kubectl get pv -o wide --show-labels
NAME   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE   VOLUMEMODE   LABELS
mypv   10Gi       RWO            Retain           Available           slow                    67s   Filesystem   type=local

anji@master:~/test$ kubectl describe pv
Name:            mypv
Labels:          type=local
Annotations:     pv.kubernetes.io/bound-by-controller: yes
Finalizers:      [kubernetes.io/pv-protection]
S"torageClass:    slow"
"Status:          Bound"
"Claim:           default/mpvc"
Reclaim Policy:  Retain
"Access Modes:    RWO"
VolumeMode:      Filesystem
Capacity:        10Gi
Node Affinity:   <none>
Message:         
Source:
    Type:          HostPath (bare host directory volume)
   " Path:          /mnt/data"
    HostPathType:  
Events:            <none> "
--- ""
apiVersion: v1
kind: PersistentVolumeClaim
metadata: 
  name: mpvc
spec: 
  storageClassName: slow
  accessModes:
     - ReadWriteOnce
  resources:
     requests:
        storage: 3Gi  
==//
anji@master:~/test$ kubectl describe pvc 
Name:          mpvc
Namespace:     default
StorageClass:  slow
Status:        Bound
Volume:        mypv
Labels:        <none>
Annotations:   pv.kubernetes.io/bind-completed: yes
               pv.kubernetes.io/bound-by-controller: yes
Finalizers:    [kubernetes.io/pvc-protection]
Capacity:      10Gi
Access Modes:  RWO
VolumeMode:    Filesystem
Used By:       <none>
Events:        <none>
====================================###################
=## error dailing tcp 10250 ,
 {}{{}{{}{{{{{{{{{}}}}}}}}}}}}}}}}}}}[][[[[\/\/{{}{}{}{}{}}{}}
 https://stackoverflow.com/questions/47140813/error-from-server-error-dialing-backend-dial-tcp-10-9-84-14910250-getsockopt
https://bobcares.com/blog/kubernetes-error-dialing-backend/



anji@master:~$ kubectl exec nginx1 -it  -- /bin/bash
Error from server: error dialing backend: dial tcp 192.168.122.131:10250: connect: no route to host
anji@master:~$ systemctl restart kube-apiserver
Failed to restart kube-apiserver.service: Unit kube-apiserver.service not found.
anji@master:~$ kubectl get pods --namespace kube-system
NAME                             READY   STATUS    RESTARTS       AGE
coredns-565d847f94-7khmz         1/1     Running   0              171m
coredns-565d847f94-rkt8t         1/1     Running   0              171m
etcd-master                      1/1     Running   0              171m
kube-apiserver-master            1/1     Running   0              171m
kube-controller-manager-master   1/1     Running   0              171m
kube-proxy-4hx6h                 1/1     Running   0              171m
kube-proxy-psvxz                 1/1     Running   0              160m
kube-scheduler-master            1/1     Running   0              171m
weave-net-jc99v                  2/2     Running   1 (168m ago)   168m
weave-net-kpjtt                  2/2     Running   0              160m
anji@master:~$ systemctl disable firewalld 
Synchronizing state of firewalld.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install disable firewalld
update-rc.d: error: Permission denied
anji@master:~$ kubectl exec nginx1 -it  -- /bin/bash
Error from server: error dialing backend: dial tcp 192.168.122.131:10250: i/o timeout
anji@master:~$ ss -tnpl  | grep 10250
LISTEN  0       4096                    *:10250                *:*              

root@master:~# systemctl disable firewalld 
Synchronizing state of firewalld.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install disable firewalld
Removed /etc/systemd/system/multi-user.target.wants/firewalld.service.
Removed /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.

root@master:~# lsof -i :10250
COMMAND  PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME
kubelet 3045 root   11u  IPv6  48249      0t0  TCP *:10250 (LISTEN)

https://www.codegrepper.com/tpc/Error+from+server%3A+error+dialing+backend%3A+dial+tcp+%3A10250%3A+connect%3A+no+route+to+host+kubernetes

systemctl stop kubelet
systemctl stop docker
iptables --flush
iptables -tnat --flush
systemctl start kubelet
systemctl start docker


telnet hostname(worker1) 10250

systemctl enable firewalld
systemctl start firewalld
firewall-cmd --permanent --add-port=6443/tcp

firewall-cmd --permanent --add-port=2379-2380/tcp

firewall-cmd --permanent --add-port=10250-10255/tcp

 sudo firewall-cmd --permanent --add-port={6443,2379-2381,10250-10252,80,8080,8001}/tcp

  sudo firewall-cmd --permanent --add-port={6443,2379-2381,10250-10255}/tcp
  
oot@worker1:/mnt/data# systemctl start firewalld
root@worker1:/mnt/data# firewall-cmd --permanent --add-port=6443/tcp
Warning: ALREADY_ENABLED: 6443:tcp
success
root@worker1:/mnt/data# firewall-cmd --permanent --add-port=2379-2380/tcp
success
root@worker1:/mnt/data# firewall-cmd --permanent --add-port=10250-10255/tcp
Warning: ALREADY_ENABLED: 10250-10255:tcp
success
root@worker1:/mnt/data#  sudo firewall-cmd --permanent --add-port={6443,2379-2381,10250-10252,80,8080,8001}/tcp

Warning: ALREADY_ENABLED: 6443:tcp
Warning: ALREADY_ENABLED: 2379-2381:tcp
success
root@worker1:/mnt/data#  
root@worker1:/mnt/data# sudo firewall-cmd --permanent --add-port={6443,2379-2381,10250-10255}/tcp
Warning: ALREADY_ENABLED: 6443:tcp
Warning: ALREADY_ENABLED: 2379-2381:tcp
Warning: ALREADY_ENABLED: 10250-10255:tcp
success
root@worker1:/mnt/data# 

oot@master:~# iptables -tnat --flush
root@master:~# systemctl enable firewalld
Synchronizing state of firewalld.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable firewalld
Created symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service â†’ /lib/systemd/system/firewalld.service.
Created symlink /etc/systemd/system/multi-user.target.wants/firewalld.service â†’ /lib/systemd/system/firewalld.service.

root@master:~# systemctl start firewalld
root@master:~# firewall-cmd --permanent --add-port=6443/tcp
success
root@master:~# firewall-cmd --permanent --add-port=2379-2380/tcp
success
root@master:~# firewall-cmd --permanent --add-port=10250-10255/tcp
success
root@master:~#  sudo firewall-cmd --permanent --add-port={6443,2379-2381,10250-10252,80,8080,8001}/tcp
Warning: ALREADY_ENABLED: 6443:tcp
success

root@master:~#  
root@master:~# sudo firewall-cmd --permanent --add-port={6443,2379-2381,10250-10255}/tcp
Warning: ALREADY_ENABLED: 6443:tcp
Warning: ALREADY_ENABLED: 2379-2381:tcp
Warning: ALREADY_ENABLED: 10250-10255:tcp
success

anji@master:~$ kubectl exec nginx1  -it  -- /bin/bash
root@nginx1:/# ls
bin   dev		   docker-entrypoint.sh  home  lib64  mnt  proc  run   srv  tmp  var
boot  docker-entrypoint.d  etc			 lib   media  opt  root  sbin  sys  usr
root@nginx1:/# 

########################################################################
https://www.containiq.com/post/kubernetes-persistent-volumes

apiVersion: v1
kind: PersistentVolume
metadata:
  name: nginx-pv-volume
spec:
  storageClassName: standard
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/nginx"

---

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nginx-pv-claim
spec:
  storageClassName: standard
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi

---

apiVersion: v1
kind: Pod
metadata:
  name: nginx-pv-pod
spec:
  containers:
    - name: nginx-container
      image: nginx
      ports:
        - containerPort: 80
          name: "http-server"
      volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: nginx-storage
  volumes:
    - name: nginx-storage
      persistentVolumeClaim:
        claimName: nginx-pv-claim  
"
anji@master:~$ kubectl get pv
NAME              CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                    STORAGECLASS   REASON   AGE
nginx-pv-volume   5Gi        RWO            Retain           Bound    default/nginx-pv-claim   standard                7s
anji@master:~$ kubectl get pvc
NAME             STATUS   VOLUME            CAPACITY   ACCESS MODES   STORAGECLASS   AGE
nginx-pv-claim   Bound    nginx-pv-volume   5Gi        RWO            standard       8s
anji@master:~$ kubectl get  po
NAME           READY   STATUS    RESTARTS   AGE
nginx-pv-pod   1/1     Running   0          12s  "

apiVersion: v1
kind: PersistentVolume
metadata:
  name: nginx-pv-volume
spec:
  storageClassName: sony
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/nginx"

---

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mypvc
spec:
  storageClassName: sony
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
        volumeMounts:
         - mountPath: "/usr/share/nginx/html"
           name: nginx
      volumes:
       - name: nginx
         persistentVolumeClaim:
           claimName: mypvc
######################################################################"
" error " master notready  not ready {}{{{{}{][{}{{}{{{{}{{}{{}[[}}}{{}}{{{{}}{}{}}{}}]]}}}}}}]}}}}
# error "   
anji@master:~$ kubectl get nodes --show-labels -o wide 
NAME      STATUS     ROLES           AGE   VERSION   INTERNAL-IP       EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME    LABELS
master    "NotReady  " control-plane   28h   v1.25.5   192.168.122.34    <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.5.9   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=master,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node.kubernetes.io/exclude-from-external-load-balancers=
worker1   Ready      <none>          28h   v1.25.5   192.168.122.131   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.5.9   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=worker1,kubernetes.io/os=linux
====++++"
anji@master:~$ kubectl get all --all-namespaces  --show-labels
NAMESPACE     NAME                                 READY   STATUS        RESTARTS      AGE   LABELS
kube-system   pod/coredns-565d847f94-4cj95         1/1     Running       0             25h   k8s-app=kube-dns,pod-template-hash=565d847f94
kube-system   pod/coredns-565d847f94-5zgp7         1/1     Running       0             25h   k8s-app=kube-dns,pod-template-hash=565d847f94
"kube-system   pod/coredns-565d847f94-7khmz         1/1     Terminating "  0             28h   k8s-app=kube-dns,pod-template-hash=565d847f94
"kube-system   pod/coredns-565d847f94-rkt8t         1/1     Terminating  " 0             28h   k8s-app=kube-dns,pod-template-hash=565d847f94
kube-system   pod/etcd-master                      1/1     Running       0             28h   component=etcd,tier=control-plane
kube-system   pod/kube-apiserver-master            1/1     Running       0             28h   component=kube-apiserver,tier=control-plane
kube-system   pod/kube-controller-manager-master   1/1     Running       0             28h   component=kube-controller-manager,tier=control-plane
kube-system   pod/kube-proxy-4hx6h                 1/1     Running       0             28h   controller-revision-hash=cc5dd58d4,k8s-app=kube-proxy,pod-template-generation=1
kube-system   pod/kube-proxy-psvxz                 1/1     Running       0             28h   controller-revision-hash=cc5dd58d4,k8s-app=kube-proxy,pod-template-generation=1
kube-system   pod/kube-scheduler-master            1/1     Running       0             28h   component=kube-scheduler,tier=control-plane
kube-system   pod/weave-net-jc99v                  2/2     Running       1 (28h ago)   28h   controller-revision-hash=d6dc745f,name=weave-net,pod-template-generation=1
kube-system   pod/weave-net-kpjtt                  2/2     Running       0             28h   controller-revision-hash=d6dc745f,name=weave-net,pod-template-generation=1

NAMESPACE     NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE   LABELS
default       service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP                  28h   component=apiserver,provider=kubernetes
kube-system   service/kube-dns     ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP   28h   k8s-app=kube-dns,kubernetes.io/cluster-service=true,kubernetes.io/name=CoreDNS

NAMESPACE     NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE   LABELS
kube-system   daemonset.apps/kube-proxy   2         2         1       2            1           kubernetes.io/os=linux   28h   k8s-app=kube-proxy
kube-system   daemonset.apps/weave-net    2         2         1       2            1           <none>                   28h   name=weave-net

NAMESPACE     NAME                      READY   UP-TO-DATE   AVAILABLE   AGE   LABELS
kube-system   deployment.apps/coredns   2/2     2            2           28h   k8s-app=kube-dns

NAMESPACE     NAME                                 DESIRED   CURRENT   READY   AGE   LABELS
kube-system   replicaset.apps/coredns-565d847f94   2         2         2       28h   k8s-app=kube-dns,pod-template-hash=565d847f94"

{{{{  rebbot system }{}{{[[[[]]]]}}}}}}
========================================================================
https://www.youtube.com/watch?v=A4svk80wPbA&list=PL8SR-mMnL9f6KKYDnrG3saVnyLqNDz1GP&index=17
day - 16  Kubernetes volume (PVC,PV) and explanation on RBAC

anji@master:~$ ls -a 
.              .bashrc  Documents  .local    .profile                   Templates
..             .cache   Downloads  .mozilla  Public                     test
.bash_history  .config  .gnupg     Music     .ssh                       Videos
.bash_logout   Desktop  .kube      Pictures  .sudo_as_admin_successful
anji@master:~$ cd .kube/
anji@master:~/.kube$ ll
total 20
drwxrwxr-x  3 anji anji 4096 Dec 28 12:12 ./
drwxr-xr-x 18 anji anji 4096 Dec 28 12:51 ../
drwxr-x---  4 anji anji 4096 Dec 28 12:12 cache/
-rw-------  1 anji anji 5642 Dec 28 12:11 config "

ter:~/.kube$ cat configbak 
apiVersion: v1
clusters:
- cluster:
    "certificate-authority-data:" LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMvakNDQWVhZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBRE

    server: https://192.168.122.34:6443
  name: kubernetes
contexts:
- context:
    "cluster: kubernetes"
    user: "kubernetes-admin"
  name: "kubernetes-admin@kubernetes"
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: {}
users:
- name: kubernetes-admin
  user:
   " client-certificate-data: "LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURJVENDQWdtZ0F3SUJBZ0lJZGxKc0FLY20yZTR3RFFZSktvWklodmNOQVFF

client-key-data: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFcFFJQkFBS0NBUUVBcVlKQkVrTUpnaWdLNjUrdHpNM0t5R1FhYzZIbjFXVmoxcjhGTWF
===========++++++++++"
root@master:~# adduser wipro
Adding user `wipro' ...
Adding new group `wipro' (1001) ...

root@master:~# su - wipro 

wipro@master:~$ kubectl get nodes 
The connection to the server localhost:8080 was refused - did you specify the right host or port?
----=="
anji@master:~$ mkdir wipro
anji@master:~$ cd wipro/
anji@master:~/wipro$ cp /etc/kubernetes/pki/ca.
ca.crt  ca.key  
anji@master:~/wipro$ cp /etc/kubernetes/pki/
apiserver.crt                 apiserver-kubelet-client.key  front-proxy-ca.key
apiserver-etcd-client.crt     ca.crt                        front-proxy-client.crt
apiserver-etcd-client.key     ca.key                        front-proxy-client.key
apiserver.key                 etcd/                         sa.key
apiserver-kubelet-client.crt  front-proxy-ca.crt            sa.pub
anji@master:~/wipro$ cp /etc/kubernetes/pki/ca.crt  .

anji@master:~/wipro$ sudo cp /etc/kubernetes/pki/ca.key  . 
[sudo] password for anji: 
anji@master:~/wipro$ ll
total 16
drwxrwxr-x  2 anji anji 4096 Dec 30 11:47 ./
drwxr-xr-x 19 anji anji 4096 Dec 30 11:43 ../
-rw-r--r--  1 anji anji 1099 Dec 30 11:45 ca.crt
-rw-------  1 root root 1679 Dec 30 11:47 ca.key "

anji@master:~/wipro$ sudo openssl genrsa -out wipro.key 2048
[sudo] password for anji: 
Generating RSA private key, 2048 bit long modulus (2 primes)
.............................+++++
..........+++++
e is 65537 (0x010001)
anji@master:~/wipro$ ll
total 20
drwxrwxr-x  2 anji anji 4096 Dec 30 12:02 ./
drwxr-xr-x 19 anji anji 4096 Dec 30 11:43 ../
-rw-r--r--  1 anji anji 1099 Dec 30 11:45 ca.crt
-rw-------  1 root root 1679 Dec 30 11:47 ca.key
-rw-------  1 root root 1679 Dec 30 12:02 wipro.key "

anji@master:~/wipro$ sudo openssl req -new  -key wipro.key -out  wipro.csr
You are about to be asked to enter information that will be incorporated
into your certificate request.
What you are about to enter is what is called a Distinguished Name or a DN.
There are quite a few fields but you can leave some blank
For some fields there will be a default value,
If you enter '.', the field will be left blank.
-----
Country Name (2 letter code) [AU]:
State or Province Name (full name) [Some-State]:
Locality Name (eg, city) []:
Organization Name (eg, company) [Internet Widgits Pty Ltd]:
Organizational Unit Name (eg, section) []:
Com"mon Name (e.g. server FQDN or YOUR name) []:wipro"
Email Address []:

Please enter the following 'extra' attributes
to be sent with your certificate request
A challenge password []:
An optional company name []:
anji@master:~/wipro$ ll
total 24
drwxrwxr-x  2 anji anji 4096 Dec 30 12:13 ./
drwxr-xr-x 19 anji anji 4096 Dec 30 11:43 ../
-rw-r--r--  1 anji anji 1099 Dec 30 11:45 ca.crt
-rw-------  1 root root 1679 Dec 30 11:47 ca.key
-rw-r--r--  1 root root  980 Dec 30 12:13 "wipro.csr"
-rw-------  1 root root 1679 Dec 30 12:02 wipro.key

anji@master:~/wipro$ sudo openssl x509 -req -in wipro.csr -CA ca.crt  -CAkey ca.key -CAcreateserial -out  wipro.crt -days  365
[sudo] password for anji: 
Signature ok
subject=C = AU, ST = Some-State, O = Internet Widgits Pty Ltd, CN = wipro
Getting CA Private Key
anji@master:~/wipro$ ll
total 32
drwxrwxr-x  2 anji anji 4096 Dec 30 12:27 ./
drwxr-xr-x 19 anji anji 4096 Dec 30 11:43 ../
-rw-r--r--  1 anji anji 1099 Dec 30 11:45 ca.crt
-rw-------  1 root root 1679 Dec 30 11:47 ca.key
-rw-r--r--  1 root root   41 Dec 30 12:27 ca.srl
-rw-r--r--  1 root root 1082 Dec 30 12:27 "wipro.crt"
-rw-r--r--  1 root root  980 Dec 30 12:13 wipro.csr
-rw-------  1 root root 1679 Dec 30 12:02 wipro.key "

anji@master:~/wipro$ file wipro.crt 
wipro.crt: PEM certificate

anji@master:~/wipro$ cat wipro.crt
-----BEGIN CERTIFICATE-----
MIIC8TCCAdkCFHUFE4Hae4I0d0cNDWn82jgk8RH+MA0GCSqGSIb3DQEBCwUAMBUx
E==
-----END CERTIFICATE----- "

anji@master:~/wipro$ cp /home/anji/.kube/config  .
anji@master:~/wipro$ ll
total 24
drwxrwxr-x  2 anji anji 4096 Dec 30 12:38 ./
drwxr-xr-x 19 anji anji 4096 Dec 30 11:43 ../
-rw-------  1 anji anji 5642 Dec 30 12:38 config
-rw-r--r--  1 root root 1082 Dec 30 12:27 wipro.crt
-rw-------  1 root root 1679 Dec 30 12:02 wipro.key

====  begore  " ==/////### BEFORE ==  

er:~/.kube$ cat config 
apiVersion: v1
clusters:
- cluster:
    "certificate-authority-data:" LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMvakNDQWVhZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBRE

    server: https://192.168.122.34:6443
  name: kubernetes
contexts:
- context:
    "cluster: kubernetes"
    user: "kubernetes-admin"
  name: "kubernetes-admin@kubernetes"
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: {}
users:
- name: kubernetes-admin
  user:
   " client-certificate-data:

###\\\/////////////  after == " AFTER  

anji@master:~/wipro$ cat config 
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: 
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: wipro                    # see look  
  name: wipro-context                  # see look  
current-context: wipro-context       # see look  
kind: Config
preferences: {}
users:
- name: wipro                          # see look  
  user:
    client-certificate-data:
    client-key-data: 

#######\\\\////////////=""//////////////////\\\\\\\\\\\\\   after 

apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: 
    server: https://192.168.122.34:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: wipro
  name: wipro-context
current-context: wipro-context
kind: Config
preferences: {}
users:
- name: wipro
  user:
    client-certificate-data:
    client-key-data: 
====++++++++++++++++=///\\\
anji@master:~/wipro$ cat wipro.crt | base64 -w0
LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM4VENDQWRrQ0ZIVUZFNEhhZTRJMGQwY05EV244MmpnazhSSCtNQTBHQ1NxR1NJYjNEUUVCQ3dVQU1CVXgKRXpBUkJnTlZCQU1UQ210MVltVnlibVYwWlhNd0hoY05Nakl4TWpNd01EWTFOekUzV2hjTk1qTXhNak13TURZMQpOekUzV2pCVk1Rc3dDUVlEVlFRR0V3SkJWVEVUTUJFR0ExVUVDQXdLVTI5dFpTMVRkR0YwWlRFaE1COEdBMVVFCkNnd1lTVzUwWlhKdVpYUWdWMmxrWjJsMGN5QlFkSGtnVEhSa01RNHdEQVlEVlFRRERBVjNhWEJ5YnpDQ0FTSXcKRFFZSktvWklodmNOQVFFQkJRQURnZ0VQQURDQ0FRb0NnZ0VCQU81Wll6NEU2Y3Q2V1VPclF6NHFENW9SeW41Twp6eHlCQVluMlorVmdWdVd4WlZMM3J4OFZ5MDh0VmxBVWRPbkNkWFplZzJrQm9GNW9nd0J0ZDZkWmJ4NXkyYWpvCnlWc3JqcWw0aTNMRVoxOGZIZ3UrNVo5UXNBdDlXQVhkV213RVJZaStKZWZ6VDNvdU5zZUM2YTV2dE90ODBUTVoKcVgwL1k3OVBRZFRsTU9iVExDRXc0WmxyeG1LUWRkQXJjZHhicmpRVS9QZXJaWEVNQklxMUI3OUxJVEVOOE1sYwpqV2NpdUVuY012alVtemJVUldnR1o1QS9MY2hPUlh4eUlFZS8zM09ESFFqQzkvd1lOU3BxcjdpRjFpVmNwK3RoCnQzcE43NzMxOUdhRmdxWUpPY0o4Mm5TNS9xRlB3MUx2VHBoNlJOZWxWNGYvZGFCQlAxUThybnJwREVNQ0F3RUEKQVRBTkJna3Foa2lHOXcwQkFRc0ZBQU9DQVFFQWYzR1ZMcHkyZVBlSUE4QzA0UVVxUjNEVzhaNzdCT2NFWHNIagpib0ZacHlBTklUWDV1L2VEc2VxSGJYS0UwdjRkWVdKT1VjT0kzdDV5WTRLWmFzWUJURlppcUpjZ3hJaUNUVFJMCi8wTnFwZXVrTDBLSWordldNNTAyYklpV201Rmw2M2FRdnZSK2paV2dxRlRlTnFiaUUrYmE3K0U0aHNKZHBHN3kKUWpIVFFYZzJZY3VxU1pMUTlBM2dkZ3hxTzl3THY5SXZ3VUFzSFRoYXArbzZNdi9tUS8vT0lFcGt1OXBvdG5ZcApFTUREbnZkWVk4VmJBZ0dRS2h4elZvS1dYQ1ZLa0kvUUJibW01b1ZEUVpkZkwvSWZqWFh5MkZDSnlkTHVaMjlFCnFKVlViSFZ0QmVRSkp2V25ydlNTYlovRWRsTC8yZ28wU0xQdVlWZTBNOHpldnluNkVnPT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=anji@master:~/wipro$ 

anji@master:~/wipro$ vi config
anji@master:~/wipro$ cat config
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMvakNDQWVhZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJeU1USXlPREEyTXprMU1sb1hEVE15TVRJeU5UQTJNemsxTWxvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHA2ClA2ZlhNMTVFR0hhaGFnQTNtbnp0NUkyYXFUWmlBbFhFSEtKazdoRzRxaHYvSEdseC95cy8wUEZTVFdIZWMwSUkKeUdWT055V1BZV2xLMUZmWEM1OVVmVUEwN1dtdzNKcUdkYTcrNmI0ZXNjTituajBHV3lRQUZhMXBoVVFtRzIzNQpvSnFuTm5nekdIR1JDeDhpakJLQnBNQ0Z5NWpEYVA3WEJ3ejV0bWVvNDU5NVM2eHVYcTNvZGRkUGk3OVNnQmlMCjE3di9UNnRTNjFvbE8xQS9wNUUzOVFvSUxyMEpEQ0VlY2h0dXU3RXludlo5QlBSdm9aanF0REI5dTdWOTRuOTMKbnlNQjdWcDRMbnpJMEUxUjNWaCtFbXZVWlluQWo2NUgrMmxMWThKYWRZMTgxS2xNVHhQSG9SSGk3R0I2U0tpbApxOHdkZHowdmJGckppTE54amI4Q0F3RUFBYU5aTUZjd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZDZndxRGZpeXVReCtFeHA5VTdQQTZ4M0JVb0FNQlVHQTFVZEVRUU8KTUF5Q0NtdDFZbVZ5Ym1WMFpYTXdEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUJBQ0I4bjNyUUZDOGpBTkRyYkVGVApMaDloK3hrRHRaMm5kcXc3OWdiVk0xYVpZYlpKUmIvUzdVMGZ0U29MM1hKOWdxOEFpbXRMYTdlTmdEZENCbGZjCnVhaXV1dUFieHl1enByMGxiK01ZclpyV0NUNGcvZ3ZkaTd3cmZBUUZycjlNSWUwb0plaHdYdE5lUzdJZXlORHkKNTJHSnloNFJERHpwMnlIa3lTU0RoM1NvYjhJZTZMSUpqL0pzZ2pJL3hmRWt2cXczdmVDRnkrZ1poek0zdG1FYQppSndmcnBnZldoMVRrcG5sNmFiZ21EWS9iTXd5UUt0YnY0dWptRnpoZ3ptNjg1WWovTStETU5RU2JaZ2ZzZDRlClMxc3BweWdibVkwRllwaWxjVlFZUWtodE9KSm9vZWJBaE00Y0VWRXR2UE9iVnZzdVpzWk1QM3RYRWFscWkwRUcKTUk4PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    server: https://192.168.122.34:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: wipro
  name: wipro-context
current-context: wipro-context
kind: Config
preferences: {}
users:
- name: wipro
  user:
    client-certificate-data:  LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM4VENDQWRrQ0ZIVUZFNEhhZTRJMGQwY05EV244MmpnazhSSCtNQTBHQ1NxR1NJYjNEUUVCQ3dVQU1CVXgKRXpBUkJnTlZCQU1UQ210MVltVnlibVYwWlhNd0hoY05Nakl4TWpNd01EWTFOekUzV2hjTk1qTXhNak13TURZMQpOekUzV2pCVk1Rc3dDUVlEVlFRR0V3SkJWVEVUTUJFR0ExVUVDQXdLVTI5dFpTMVRkR0YwWlRFaE1COEdBMVVFCkNnd1lTVzUwWlhKdVpYUWdWMmxrWjJsMGN5QlFkSGtnVEhSa01RNHdEQVlEVlFRRERBVjNhWEJ5YnpDQ0FTSXcKRFFZSktvWklodmNOQVFFQkJRQURnZ0VQQURDQ0FRb0NnZ0VCQU81Wll6NEU2Y3Q2V1VPclF6NHFENW9SeW41Twp6eHlCQVluMlorVmdWdVd4WlZMM3J4OFZ5MDh0VmxBVWRPbkNkWFplZzJrQm9GNW9nd0J0ZDZkWmJ4NXkyYWpvCnlWc3JqcWw0aTNMRVoxOGZIZ3UrNVo5UXNBdDlXQVhkV213RVJZaStKZWZ6VDNvdU5zZUM2YTV2dE90ODBUTVoKcVgwL1k3OVBRZFRsTU9iVExDRXc0WmxyeG1LUWRkQXJjZHhicmpRVS9QZXJaWEVNQklxMUI3OUxJVEVOOE1sYwpqV2NpdUVuY012alVtemJVUldnR1o1QS9MY2hPUlh4eUlFZS8zM09ESFFqQzkvd1lOU3BxcjdpRjFpVmNwK3RoCnQzcE43NzMxOUdhRmdxWUpPY0o4Mm5TNS9xRlB3MUx2VHBoNlJOZWxWNGYvZGFCQlAxUThybnJwREVNQ0F3RUEKQVRBTkJna3Foa2lHOXcwQkFRc0ZBQU9DQVFFQWYzR1ZMcHkyZVBlSUE4QzA0UVVxUjNEVzhaNzdCT2NFWHNIagpib0ZacHlBTklUWDV1L2VEc2VxSGJYS0UwdjRkWVdKT1VjT0kzdDV5WTRLWmFzWUJURlppcUpjZ3hJaUNUVFJMCi8wTnFwZXVrTDBLSWordldNNTAyYklpV201Rmw2M2FRdnZSK2paV2dxRlRlTnFiaUUrYmE3K0U0aHNKZHBHN3kKUWpIVFFYZzJZY3VxU1pMUTlBM2dkZ3hxTzl3THY5SXZ3VUFzSFRoYXArbzZNdi9tUS8vT0lFcGt1OXBvdG5ZcApFTUREbnZkWVk4VmJBZ0dRS2h4elZvS1dYQ1ZLa0kvUUJibW01b1ZEUVpkZkwvSWZqWFh5MkZDSnlkTHVaMjlFCnFKVlViSFZ0QmVRSkp2V25ydlNTYlovRWRsTC8yZ28wU0xQdVlWZTBNOHpldnluNkVnPT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    client-key-data: 
###3===-----
anji@master:~/wipro$ sudo cat wipro.key  | base64  -w0
[sudo] password for anji: 
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@master:~/wipro$ vi config
anji@master:~/wipro$ cat config
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMvakNDQWVhZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJeU1USXlPREEyTXprMU1sb1hEVE15TVRJeU5UQTJNemsxTWxvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHA2ClA2ZlhNMTVFR0hhaGFnQTNtbnp0NUkyYXFUWmlBbFhFSEtKazdoRzRxaHYvSEdseC95cy8wUEZTVFdIZWMwSUkKeUdWT055V1BZV2xLMUZmWEM1OVVmVUEwN1dtdzNKcUdkYTcrNmI0ZXNjTituajBHV3lRQUZhMXBoVVFtRzIzNQpvSnFuTm5nekdIR1JDeDhpakJLQnBNQ0Z5NWpEYVA3WEJ3ejV0bWVvNDU5NVM2eHVYcTNvZGRkUGk3OVNnQmlMCjE3di9UNnRTNjFvbE8xQS9wNUUzOVFvSUxyMEpEQ0VlY2h0dXU3RXludlo5QlBSdm9aanF0REI5dTdWOTRuOTMKbnlNQjdWcDRMbnpJMEUxUjNWaCtFbXZVWlluQWo2NUgrMmxMWThKYWRZMTgxS2xNVHhQSG9SSGk3R0I2U0tpbApxOHdkZHowdmJGckppTE54amI4Q0F3RUFBYU5aTUZjd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZDZndxRGZpeXVReCtFeHA5VTdQQTZ4M0JVb0FNQlVHQTFVZEVRUU8KTUF5Q0NtdDFZbVZ5Ym1WMFpYTXdEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUJBQ0I4bjNyUUZDOGpBTkRyYkVGVApMaDloK3hrRHRaMm5kcXc3OWdiVk0xYVpZYlpKUmIvUzdVMGZ0U29MM1hKOWdxOEFpbXRMYTdlTmdEZENCbGZjCnVhaXV1dUFieHl1enByMGxiK01ZclpyV0NUNGcvZ3ZkaTd3cmZBUUZycjlNSWUwb0plaHdYdE5lUzdJZXlORHkKNTJHSnloNFJERHpwMnlIa3lTU0RoM1NvYjhJZTZMSUpqL0pzZ2pJL3hmRWt2cXczdmVDRnkrZ1poek0zdG1FYQppSndmcnBnZldoMVRrcG5sNmFiZ21EWS9iTXd5UUt0YnY0dWptRnpoZ3ptNjg1WWovTStETU5RU2JaZ2ZzZDRlClMxc3BweWdibVkwRllwaWxjVlFZUWtodE9KSm9vZWJBaE00Y0VWRXR2UE9iVnZzdVpzWk1QM3RYRWFscWkwRUcKTUk4PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    server: https://192.168.122.34:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: wipro
  name: wipro-context
current-context: wipro-context
kind: Config
preferences: {}
users:
- name: wipro
  user:
    client-certificate-data:  LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM4VENDQWRrQ0ZIVUZFNEhhZTRJMGQwY05EV244MmpnazhSSCtNQTBHQ1NxR1NJYjNEUUVCQ3dVQU1CVXgKRXpBUkJnTlZCQU1UQ210MVltVnlibVYwWlhNd0hoY05Nakl4TWpNd01EWTFOekUzV2hjTk1qTXhNak13TURZMQpOekUzV2pCVk1Rc3dDUVlEVlFRR0V3SkJWVEVUTUJFR0ExVUVDQXdLVTI5dFpTMVRkR0YwWlRFaE1COEdBMVVFCkNnd1lTVzUwWlhKdVpYUWdWMmxrWjJsMGN5QlFkSGtnVEhSa01RNHdEQVlEVlFRRERBVjNhWEJ5YnpDQ0FTSXcKRFFZSktvWklodmNOQVFFQkJRQURnZ0VQQURDQ0FRb0NnZ0VCQU81Wll6NEU2Y3Q2V1VPclF6NHFENW9SeW41Twp6eHlCQVluMlorVmdWdVd4WlZMM3J4OFZ5MDh0VmxBVWRPbkNkWFplZzJrQm9GNW9nd0J0ZDZkWmJ4NXkyYWpvCnlWc3JqcWw0aTNMRVoxOGZIZ3UrNVo5UXNBdDlXQVhkV213RVJZaStKZWZ6VDNvdU5zZUM2YTV2dE90ODBUTVoKcVgwL1k3OVBRZFRsTU9iVExDRXc0WmxyeG1LUWRkQXJjZHhicmpRVS9QZXJaWEVNQklxMUI3OUxJVEVOOE1sYwpqV2NpdUVuY012alVtemJVUldnR1o1QS9MY2hPUlh4eUlFZS8zM09ESFFqQzkvd1lOU3BxcjdpRjFpVmNwK3RoCnQzcE43NzMxOUdhRmdxWUpPY0o4Mm5TNS9xRlB3MUx2VHBoNlJOZWxWNGYvZGFCQlAxUThybnJwREVNQ0F3RUEKQVRBTkJna3Foa2lHOXcwQkFRc0ZBQU9DQVFFQWYzR1ZMcHkyZVBlSUE4QzA0UVVxUjNEVzhaNzdCT2NFWHNIagpib0ZacHlBTklUWDV1L2VEc2VxSGJYS0UwdjRkWVdKT1VjT0kzdDV5WTRLWmFzWUJURlppcUpjZ3hJaUNUVFJMCi8wTnFwZXVrTDBLSWordldNNTAyYklpV201Rmw2M2FRdnZSK2paV2dxRlRlTnFiaUUrYmE3K0U0aHNKZHBHN3kKUWpIVFFYZzJZY3VxU1pMUTlBM2dkZ3hxTzl3THY5SXZ3VUFzSFRoYXArbzZNdi9tUS8vT0lFcGt1OXBvdG5ZcApFTUREbnZkWVk4VmJBZ0dRS2h4elZvS1dYQ1ZLa0kvUUJibW01b1ZEUVpkZkwvSWZqWFh5MkZDSnlkTHVaMjlFCnFKVlViSFZ0QmVRSkp2V25ydlNTYlovRWRsTC8yZ28wU0xQdVlWZTBNOHpldnluNkVnPT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    client-key-data:   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
-----"

root@master:/home/anji/wipro# cp config   /home/wipro/
wipro@master:~$ ll
total 28
drwxr-xr-x 2 wipro wipro 4096 Dec 30 13:44 ./
drwxr-xr-x 4 root  root  4096 Dec 30 11:38 ../
-rw-r--r-- 1 wipro wipro  220 Dec 30 11:38 .bash_logout
-rw-r--r-- 1 wipro wipro 3771 Dec 30 11:38 .bashrc
-rw------- 1 "root  root  5507 Dec 30 13:48 config"
-rw-r--r-- 1 wipro wipro  807 Dec 30 11:38 .profile

root@master:/home/anji/wipro# chown wipro:wipro  /home/wipro/config 
wipro@master:~$ ll
total 28
drwxr-xr-x 2 wipro wipro 4096 Dec 30 13:44 ./
drwxr-xr-x 4 root  root  4096 Dec 30 11:38 ../
-rw-r--r-- 1 wipro wipro  220 Dec 30 11:38 .bash_logout
-rw-r--r-- 1 wipro wipro 3771 Dec 30 11:38 .bashrc
-rw------- 1 "wipro wipro "5507 Dec 30 13:48 config
-rw-r--r-- 1 wipro wipro  807 Dec 30 11:38 .profile

wipro@master:~$ ls
config
wipro@master:~$ mkdir .kube
wipro@master:~$ ll
total 32
drwxr-xr-x 3 wipro wipro 4096 Dec 30 13:55 ./
drwxr-xr-x 4 root  root  4096 Dec 30 11:38 ../
-rw-r--r-- 1 wipro wipro  220 Dec 30 11:38 .bash_logout
-rw-r--r-- 1 wipro wipro 3771 Dec 30 11:38 .bashrc
-rw------- 1 wipro wipro 5507 Dec 30 13:48 config
drwxrwxr-x 2 wipro wipro 4096 Dec 30 13:55 .kube/
-rw-r--r-- 1 wipro wipro  807 Dec 30 11:38 .profile
wipro@master:~$ mv config .kube/
wipro@master:~$ ls -l
total 0
wipro@master:~$ cd .kube/
wipro@master:~/.kube$ ls
config

root@master:~# adduser cpu
anji@master:~/ssd$ sudo openssl genrsa  -out ssd.key 2048
anji@master:~/ssd$ sudo openssl req -new  -key ssd.key -out ssd.csr
anji@master:~/ssd$ sudo openssl x509 -req -in ssd.csr -CA  ca.crt -CAkey ca.key  -CAcreateserial -out ssd.crt  -days 365
anji@master:~/ssd$ sudo cp /home/anji/.kube/config  .
anji@master:~/ssd$ sudo nano config 
anji@master:~/ssd$ cat ssd.crt  | base64 -w0
anji@master:~/ssd$ sudo vi config 
 sudo cat ssd.key  | base64 -w0

anji@master:~/ssd$ sudo vi config 
anji@master:~/ssd$ sudo cp config  /home/ssd/
anji@master:~/ssd$ sudo mkdir  /home/ssd/.kube 
anji@master:~/ssd$ sudo mv  /home/ssd/config   /home/ssd/.kube/
anji@master:~/ssd$ sudo chown ssd:ssd /home/ssd/.kube/config 
anji@master:~$ kubectl create clusterrolebinding --help | less 

   kubectl create clusterrolebinding cluster-admin --clusterrole=cluster-admin --user=user1 
  kubectl create clusterrolebinding  ssd1 --clusterrole=cluster-admin  --user=ssd  "

{{}{{{}{{{{{}}}}}}}}{}====  AUTHENTICATION IS COMPLETED  "NEXT AUTHORIZATION"
+++++++++++///\/\/\/=\\/\\/\/\/\\//
wipro@master:~$ kubectl get nodes 
Error from server (Forbidden): nodes is forbidden: User "wipro" cannot list resource "nodes" in API group "" at the cluster scope

AUTHORIZATION IS MAINTAINED BY "RBAC"
  
 1.CLUSTER-ROLE
   A) cluster-admin if we attach this role to someone , that person will have root access on the cluster
   B) self-provisioner - user with this  role can create namespace and delete them
 2. PROJECT/LOCAL-ROLE = project level
    a) view
    b) edit
    c) admin - full- access
---""
THERE ARE 2 TYPES OF ROLE BINDINGS
 1).CLUSTERROLE == attached== user == clusterrole
 2.).PROJECT/Local role  ==== attached == user == role binding

 kubectl get rolebinding

============ "
https://kubernetes.io/docs/reference/access-authn-authz/rbac/
kubectl create rolebinding --help | less 

kubectl create rolebinding bob-admin-binding --clusterrole=admin --user=bob --namespace=acme

kubectl create rolebinding myapp-view-binding --clusterrole=view --serviceaccount=acme:myapp --namespace=acme

kubectl create rolebinding myappnamespace-myapp-view-binding --clusterrole=view --serviceaccount=myappnamespace:myapp --namespace=acme

kubectl create clusterrolebinding root-cluster-admin-binding --clusterrole=cluster-admin --user=root

kubectl create clusterrolebinding kube-proxy-binding --clusterrole=system:node-proxier --user=system:kube-proxy

kubectl create clusterrolebinding myapp-view-binding --clusterrole=view --serviceaccount=acme:myapp

=====
anji@master:~/wipro$ kubectl create rolebinding --help | less 
Create a role binding for a particular role or cluster role.
Examples:
  # Create a role binding for user1, user2, and group1 using the admin cluster role
  kubectl create rolebinding admin --clusterrole=admin --user=user1 --user=user2 --group=group1

=++++++++++
kubectl create rolebinding wipro1 --clusterrole=admin --user=user1 --user=user2 --group=group1

kubectl create clusterrolebinding wipro1  --clusterrole=cluster-admin --user=wipro  "

anji@master:~$ kubectl create clusterrolebinding wipro1  --clusterrole=cluster-admin --user=wipro 
clusterrolebinding.rbac.authorization.k8s.io/wipro1 created

"wipro"@master:~$ kubectl get nodes  -o wide --show-labels
NAME      STATUS   ROLES           AGE    VERSION   INTERNAL-IP       EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME    LABELS
master    Ready    control-plane   2d2h   v1.25.5   192.168.122.34    <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.5.9   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=master,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node.kubernetes.io/exclude-from-external-load-balancers=
worker1   Ready    <none>          2d2h   v1.25.5   192.168.122.131   <none>        Ubuntu 20.04.5 LTS   5.15.0-56-generic   containerd://1.5.9   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=worker1,kubernetes.io/os=linux "
---"
anji@master:~$ kubectl delete  clusterrolebinding wipro1  
clusterrolebinding.rbac.authorization.k8s.io "wipro1" deleted

wipro@master:~$ kubectl get nodes 
Error from server (Forbidden): nodes is forbidden: User "wipro" cannot list resource "nodes" in API group "" at the cluster scope
=="
anji@master:~$ kubectl create clusterrolebinding wipro1  --clusterrole=cluster-admin --user=wipro 
clusterrolebinding.rbac.authorization.k8s.io/wipro1 created

anji@master:~$ kubectl get clusterrolebinding
NAME                                                   ROLE                                                                               AGE
cluster-admin                                          "ClusterRole/cluster-admin "                                                         2d3h
"kubeadm:get-nodes "                                     ClusterRole/kubeadm:get-nodes                                                      2d3h
kubeadm:kubelet-bootstrap                              ClusterRole/system:node-bootstrapper                                               2d3h
kubeadm:node-autoapprove-bootstrap                     ClusterRole/system:certificates.k8s.io:certificatesigningrequests:nodeclient       2d3h
kubeadm:node-autoapprove-certificate-rotation          ClusterRole/system:certificates.k8s.io:certificatesigningrequests:selfnodeclient   2d3h
kubeadm:node-proxier                                   ClusterRole/system:node-proxier                                                    2d3h
system:basic-user                                      ClusterRole/system:basic-user                                                      2d3h
system:controller:attachdetach-controller              ClusterRole/system:controller:attachdetach-controller                              2d3h
system:controller:certificate-controller               ClusterRole/system:controller:certificate-controller                               2d3h
system:controller:clusterrole-aggregation-controller   ClusterRole/system:controller:clusterrole-aggregation-controller                   2d3h
system:controller:cronjob-controller                   ClusterRole/system:controller:cronjob-controller                                   2d3h
system:controller:daemon-set-controller                ClusterRole/system:controller:daemon-set-controller                                2d3h
s"ystem:controller:deployment-controller                ClusterRole/system:controller:deployment-controller  "                              2d3h
system:controller:disruption-controller                ClusterRole/system:controller:disruption-controller                                2d3h
system:controller:endpoint-controller                  ClusterRole/system:controller:endpoint-controller                                  2d3h
system:controller:endpointslice-controller             ClusterRole/system:controller:endpointslice-controller                             2d3h
system:controller:endpointslicemirroring-controller    ClusterRole/system:controller:endpointslicemirroring-controller                    2d3h
system:controller:ephemeral-volume-controller          ClusterRole/system:controller:ephemeral-volume-controller                          2d3h
system:controller:expand-controller                    ClusterRole/system:controller:expand-controller                                    2d3h
system:controller:generic-garbage-collector            ClusterRole/system:controller:generic-garbage-collector                            2d3h
system:controller:horizontal-pod-autoscaler            ClusterRole/system:controller:horizontal-pod-autoscaler                            2d3h
system:controller:job-controller                       ClusterRole/system:controller:job-controller                                       2d3h
system:controller:namespace-controller                 ClusterRole/system:controller:namespace-controller                                 2d3h
system:controller:node-controller                      ClusterRole/system:controller:node-controller                                      2d3h
s"ystem:controller:persistent-volume-binder             ClusterRole/system:controller:persistent-volume-binder  "                           2d3h
system:controller:pod-garbage-collector                ClusterRole/system:controller:pod-garbage-collector                                2d3h
system:controller:pv-protection-controller             ClusterRole/system:controller:pv-protection-controller                             2d3h
system:controller:pvc-protection-controller            ClusterRole/system:controller:pvc-protection-controller                            2d3h
system:controller:replicaset-controller                ClusterRole/system:controller:replicaset-controller                                2d3h
system:controller:replication-controller               ClusterRole/system:controller:replication-controller                               2d3h
system:controller:resourcequota-controller             ClusterRole/system:controller:resourcequota-controller                             2d3h
system:controller:root-ca-cert-publisher               ClusterRole/system:controller:root-ca-cert-publisher                               2d3h
system:controller:route-controller                     ClusterRole/system:controller:route-controller                                     2d3h
system:controller:service-account-controller           ClusterRole/system:controller:service-account-controller                           2d3h
system:controller:service-controller                   ClusterRole/system:controller:service-controller                                   2d3h
system:controller:statefulset-controller               ClusterRole/system:controller:statefulset-controller                               2d3h
system:controller:ttl-after-finished-controller        ClusterRole/system:controller:ttl-after-finished-controller                        2d3h
system:controller:ttl-controller                       ClusterRole/system:controller:ttl-controller                                       2d3h
system:coredns                                         ClusterRole/system:coredns                                                         2d3h
system:discovery                                       ClusterRole/system:discovery                                                       2d3h
system:kube-controller-manager                         ClusterRole/system:kube-controller-manager                                         2d3h
system:kube-dns                                        ClusterRole/system:kube-dns                                                        2d3h
system:kube-scheduler                                  ClusterRole/system:kube-scheduler                                                  2d3h
system:monitoring                                      ClusterRole/system:monitoring                                                      2d3h
system:node                                            ClusterRole/system:node                                                            2d3h
system:node-proxier                                    ClusterRole/system:node-proxier                                                    2d3h
system:public-info-viewer                              ClusterRole/system:public-info-viewer                                              2d3h
system:service-account-issuer-discovery                ClusterRole/system:service-account-issuer-discovery                                2d3h
system:volume-scheduler                                ClusterRole/system:volume-scheduler                                                2d3h
"weave-net                                              ClusterRole/weave-net    "                                                          2d3h
"wipro1 "                                                ClusterRole/cluster-admin             "                                             2m8s
======+++++++++++""
anji@master:~$ kubectl delete  clusterrolebinding wipro1  
clusterrolebinding.rbac.authorization.k8s.io "wipro1" deleted
----
anji@master:~$ kubectl get clusterrole
NAME                                                                   CREATED AT
admin                                                                  2022-12-28T06:40:09Z
cluster-admin                                                          2022-12-28T06:40:09Z
edit                                                                   2022-12-28T06:40:09Z
kubeadm:get-nodes                                                      2022-12-28T06:40:11Z
system:aggregate-to-admin                                              2022-12-28T06:40:09Z
system:aggregate-to-edit                                               2022-12-28T06:40:09Z
system:aggregate-to-view                                               2022-12-28T06:40:09Z
system:auth-delegator                                                  2022-12-28T06:40:09Z
system:basic-user                                                      2022-12-28T06:40:09Z
system:certificates.k8s.io:certificatesigningrequests:nodeclient       2022-12-28T06:40:09Z
system:certificates.k8s.io:certificatesigningrequests:selfnodeclient   2022-12-28T06:40:09Z
system:certificates.k8s.io:kube-apiserver-client-approver              2022-12-28T06:40:09Z
system:certificates.k8s.io:kube-apiserver-client-kubelet-approver      2022-12-28T06:40:09Z
system:certificates.k8s.io:kubelet-serving-approver                    2022-12-28T06:40:09Z
system:certificates.k8s.io:legacy-unknown-approver                     2022-12-28T06:40:09Z
system:controller:attachdetach-controller                              2022-12-28T06:40:09Z
system:controller:certificate-controller                               2022-12-28T06:40:09Z
system:controller:clusterrole-aggregation-controller                   2022-12-28T06:40:09Z
system:controller:cronjob-controller                                   2022-12-28T06:40:09Z
system:controller:daemon-set-controller                                2022-12-28T06:40:09Z
system:controller:deployment-controller                                2022-12-28T06:40:09Z
system:controller:disruption-controller                                2022-12-28T06:40:09Z
system:controller:endpoint-controller                                  2022-12-28T06:40:09Z
system:controller:endpointslice-controller                             2022-12-28T06:40:09Z
system:controller:endpointslicemirroring-controller                    2022-12-28T06:40:09Z
system:controller:ephemeral-volume-controller                          2022-12-28T06:40:09Z
system:controller:expand-controller                                    2022-12-28T06:40:09Z
system:controller:generic-garbage-collector                            2022-12-28T06:40:09Z
system:controller:horizontal-pod-autoscaler                            2022-12-28T06:40:09Z
system:controller:job-controller                                       2022-12-28T06:40:09Z
system:controller:namespace-controller                                 2022-12-28T06:40:09Z
system:controller:node-controller                                      2022-12-28T06:40:09Z
system:controller:persistent-volume-binder                             2022-12-28T06:40:09Z
system:controller:pod-garbage-collector                                2022-12-28T06:40:09Z
system:controller:pv-protection-controller                             2022-12-28T06:40:09Z
system:controller:pvc-protection-controller                            2022-12-28T06:40:09Z
system:controller:replicaset-controller                                2022-12-28T06:40:09Z
system:controller:replication-controller                               2022-12-28T06:40:09Z
system:controller:resourcequota-controller                             2022-12-28T06:40:09Z
system:controller:root-ca-cert-publisher                               2022-12-28T06:40:09Z
system:controller:route-controller                                     2022-12-28T06:40:09Z
system:controller:service-account-controller                           2022-12-28T06:40:09Z
system:controller:service-controller                                   2022-12-28T06:40:09Z
system:controller:statefulset-controller                               2022-12-28T06:40:09Z
system:controller:ttl-after-finished-controller                        2022-12-28T06:40:09Z
system:controller:ttl-controller                                       2022-12-28T06:40:09Z
system:coredns                                                         2022-12-28T06:40:12Z
system:discovery                                                       2022-12-28T06:40:09Z
system:heapster                                                        2022-12-28T06:40:09Z
system:kube-aggregator                                                 2022-12-28T06:40:09Z
system:kube-controller-manager                                         2022-12-28T06:40:09Z
system:kube-dns                                                        2022-12-28T06:40:09Z
system:kube-scheduler                                                  2022-12-28T06:40:09Z
system:kubelet-api-admin                                               2022-12-28T06:40:09Z
system:monitoring                                                      2022-12-28T06:40:09Z
system:node                                                            2022-12-28T06:40:09Z
system:node-bootstrapper                                               2022-12-28T06:40:09Z
system:node-problem-detector                                           2022-12-28T06:40:09Z
system:node-proxier                                                    2022-12-28T06:40:09Z
system:persistent-volume-provisioner                                   2022-12-28T06:40:09Z
system:public-info-viewer                                              2022-12-28T06:40:09Z
system:service-account-issuer-discovery                                2022-12-28T06:40:09Z
system:volume-scheduler                                                2022-12-28T06:40:09Z
view                                                                   2022-12-28T06:40:09Z
weave-net                                                              2022-12-28T06:43:21Z"
====+++++++# /\/\/\/\/\\\\\\\\\\=================/{}{}/\\/\/\/\\///
anji@master:~$ kubectl describe clusterrole
Name:         admin
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                                       Non-Resource URLs  Resource Names  Verbs
  ---------                                       -----------------  --------------  -----
  leases.coordination.k8s.io                      []                 []              [create delete deletecollection get list patch update watch]
  rolebindings.rbac.authorization.k8s.io          []                 []              [create delete deletecollection get list patch update watch]
  roles.rbac.authorization.k8s.io                 []                 []              [create delete deletecollection get list patch update watch]
  configmaps                                      []                 []              [create delete deletecollection patch update get list watch]
  events                                          []                 []              [create delete deletecollection patch update get list watch]
  persistentvolumeclaims                          []                 []              [create delete deletecollection patch update get list watch]
  pods                                            []                 []              [create delete deletecollection patch update get list watch]
  replicationcontrollers/scale                    []                 []              [create delete deletecollection patch update get list watch]
  replicationcontrollers                          []                 []              [create delete deletecollection patch update get list watch]
  services                                        []                 []              [create delete deletecollection patch update get list watch]
  daemonsets.apps                                 []                 []              [create delete deletecollection patch update get list watch]
  deployments.apps/scale                          []                 []              [create delete deletecollection patch update get list watch]
  deployments.apps                                []                 []              [create delete deletecollection patch update get list watch]
  replicasets.apps/scale                          []                 []              [create delete deletecollection patch update get list watch]
  replicasets.apps                                []                 []              [create delete deletecollection patch update get list watch]
  statefulsets.apps/scale                         []                 []              [create delete deletecollection patch update get list watch]
  statefulsets.apps                               []                 []              [create delete deletecollection patch update get list watch]
  horizontalpodautoscalers.autoscaling            []                 []              [create delete deletecollection patch update get list watch]
  cronjobs.batch                                  []                 []              [create delete deletecollection patch update get list watch]
  jobs.batch                                      []                 []              [create delete deletecollection patch update get list watch]
  daemonsets.extensions                           []                 []              [create delete deletecollection patch update get list watch]
  deployments.extensions/scale                    []                 []              [create delete deletecollection patch update get list watch]
  deployments.extensions                          []                 []              [create delete deletecollection patch update get list watch]
  ingresses.extensions                            []                 []              [create delete deletecollection patch update get list watch]
  networkpolicies.extensions                      []                 []              [create delete deletecollection patch update get list watch]
  replicasets.extensions/scale                    []                 []              [create delete deletecollection patch update get list watch]
  replicasets.extensions                          []                 []              [create delete deletecollection patch update get list watch]
  replicationcontrollers.extensions/scale         []                 []              [create delete deletecollection patch update get list watch]
  ingresses.networking.k8s.io                     []                 []              [create delete deletecollection patch update get list watch]
  networkpolicies.networking.k8s.io               []                 []              [create delete deletecollection patch update get list watch]
  poddisruptionbudgets.policy                     []                 []              [create delete deletecollection patch update get list watch]
  deployments.apps/rollback                       []                 []              [create delete deletecollection patch update]
  deployments.extensions/rollback                 []                 []              [create delete deletecollection patch update]
  pods/eviction                                   []                 []              [create]
  serviceaccounts/token                           []                 []              [create]
  localsubjectaccessreviews.authorization.k8s.io  []                 []              [create]
  pods/attach                                     []                 []              [get list watch create delete deletecollection patch update]
  pods/exec                                       []                 []              [get list watch create delete deletecollection patch update]
  pods/portforward                                []                 []              [get list watch create delete deletecollection patch update]
  pods/proxy                                      []                 []              [get list watch create delete deletecollection patch update]
  secrets                                         []                 []              [get list watch create delete deletecollection patch update]
  services/proxy                                  []                 []              [get list watch create delete deletecollection patch update]
  bindings                                        []                 []              [get list watch]
  endpoints                                       []                 []              [get list watch]
  limitranges                                     []                 []              [get list watch]
  namespaces/status                               []                 []              [get list watch]
  namespaces                                      []                 []              [get list watch]
  persistentvolumeclaims/status                   []                 []              [get list watch]
  pods/log                                        []                 []              [get list watch]
  pods/status                                     []                 []              [get list watch]
  replicationcontrollers/status                   []                 []              [get list watch]
  resourcequotas/status                           []                 []              [get list watch]
  resourcequotas                                  []                 []              [get list watch]
  services/status                                 []                 []              [get list watch]
  controllerrevisions.apps                        []                 []              [get list watch]
  daemonsets.apps/status                          []                 []              [get list watch]
  deployments.apps/status                         []                 []              [get list watch]
  replicasets.apps/status                         []                 []              [get list watch]
  statefulsets.apps/status                        []                 []              [get list watch]
  horizontalpodautoscalers.autoscaling/status     []                 []              [get list watch]
  cronjobs.batch/status                           []                 []              [get list watch]
  jobs.batch/status                               []                 []              [get list watch]
  endpointslices.discovery.k8s.io                 []                 []              [get list watch]
  daemonsets.extensions/status                    []                 []              [get list watch]
  deployments.extensions/status                   []                 []              [get list watch]
  ingresses.extensions/status                     []                 []              [get list watch]
  replicasets.extensions/status                   []                 []              [get list watch]
  ingresses.networking.k8s.io/status              []                 []              [get list watch]
  poddisruptionbudgets.policy/status              []                 []              [get list watch]
  serviceaccounts                                 []                 []              [impersonate create delete deletecollection patch update get list watch] "


Name:         cluster-admin
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  -----
  *.*        []                 []              [*]
             [*]                []              [*]


Name:         edit
Labels:       kubernetes.io/bootstrapping=rbac-defaults
              rbac.authorization.k8s.io/aggregate-to-admin=true
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                                    Non-Resource URLs  Resource Names  Verbs
  ---------                                    -----------------  --------------  -----
  leases.coordination.k8s.io                   []                 []              [create delete deletecollection get list patch update watch]
  configmaps                                   []                 []              [create delete deletecollection patch update get list watch]
  events                                       []                 []              [create delete deletecollection patch update get list watch]
  persistentvolumeclaims                       []                 []              [create delete deletecollection patch update get list watch]
  pods                                         []                 []              [create delete deletecollection patch update get list watch]
  replicationcontrollers/scale                 []                 []              [create delete deletecollection patch update get list watch]
  replicationcontrollers                       []                 []              [create delete deletecollection patch update get list watch]
  services                                     []                 []              [create delete deletecollection patch update get list watch]
  daemonsets.apps                              []                 []              [create delete deletecollection patch update get list watch]
  deployments.apps/scale                       []                 []              [create delete deletecollection patch update get list watch]
  deployments.apps                             []                 []              [create delete deletecollection patch update get list watch]
  replicasets.apps/scale                       []                 []              [create delete deletecollection patch update get list watch]
  replicasets.apps                             []                 []              [create delete deletecollection patch update get list watch]
  statefulsets.apps/scale                      []                 []              [create delete deletecollection patch update get list watch]
  statefulsets.apps                            []                 []              [create delete deletecollection patch update get list watch]
  horizontalpodautoscalers.autoscaling         []                 []              [create delete deletecollection patch update get list watch]
  cronjobs.batch                               []                 []              [create delete deletecollection patch update get list watch]
  jobs.batch                                   []                 []              [create delete deletecollection patch update get list watch]
  daemonsets.extensions                        []                 []              [create delete deletecollection patch update get list watch]
  deployments.extensions/scale                 []                 []              [create delete deletecollection patch update get list watch]
  deployments.extensions                       []                 []              [create delete deletecollection patch update get list watch]
  ingresses.extensions                         []                 []              [create delete deletecollection patch update get list watch]
  networkpolicies.extensions                   []                 []              [create delete deletecollection patch update get list watch]
  replicasets.extensions/scale                 []                 []              [create delete deletecollection patch update get list watch]
  replicasets.extensions                       []                 []              [create delete deletecollection patch update get list watch]
  replicationcontrollers.extensions/scale      []                 []              [create delete deletecollection patch update get list watch]
  ingresses.networking.k8s.io                  []                 []              [create delete deletecollection patch update get list watch]
  networkpolicies.networking.k8s.io            []                 []              [create delete deletecollection patch update get list watch]
  poddisruptionbudgets.policy                  []                 []              [create delete deletecollection patch update get list watch]
  deployments.apps/rollback                    []                 []              [create delete deletecollection patch update]
  deployments.extensions/rollback              []                 []              [create delete deletecollection patch update]
  pods/eviction                                []                 []              [create]
  serviceaccounts/token                        []                 []              [create]
  pods/attach                                  []                 []              [get list watch create delete deletecollection patch update]
  pods/exec                                    []                 []              [get list watch create delete deletecollection patch update]
  pods/portforward                             []                 []              [get list watch create delete deletecollection patch update]
  pods/proxy                                   []                 []              [get list watch create delete deletecollection patch update]
  secrets                                      []                 []              [get list watch create delete deletecollection patch update]
  services/proxy                               []                 []              [get list watch create delete deletecollection patch update]
  bindings                                     []                 []              [get list watch]
  endpoints                                    []                 []              [get list watch]
  limitranges                                  []                 []              [get list watch]
  namespaces/status                            []                 []              [get list watch]
  namespaces                                   []                 []              [get list watch]
  persistentvolumeclaims/status                []                 []              [get list watch]
  pods/log                                     []                 []              [get list watch]
  pods/status                                  []                 []              [get list watch]
  replicationcontrollers/status                []                 []              [get list watch]
  resourcequotas/status                        []                 []              [get list watch]
  resourcequotas                               []                 []              [get list watch]
  services/status                              []                 []              [get list watch]
  controllerrevisions.apps                     []                 []              [get list watch]
  daemonsets.apps/status                       []                 []              [get list watch]
  deployments.apps/status                      []                 []              [get list watch]
  replicasets.apps/status                      []                 []              [get list watch]
  statefulsets.apps/status                     []                 []              [get list watch]
  horizontalpodautoscalers.autoscaling/status  []                 []              [get list watch]
  cronjobs.batch/status                        []                 []              [get list watch]
  jobs.batch/status                            []                 []              [get list watch]
  endpointslices.discovery.k8s.io              []                 []              [get list watch]
  daemonsets.extensions/status                 []                 []              [get list watch]
  deployments.extensions/status                []                 []              [get list watch]
  ingresses.extensions/status                  []                 []              [get list watch]
  replicasets.extensions/status                []                 []              [get list watch]
  ingresses.networking.k8s.io/status           []                 []              [get list watch]
  poddisruptionbudgets.policy/status           []                 []              [get list watch]
  serviceaccounts                              []                 []              [impersonate create delete deletecollection patch update get list watch]


Name:         kubeadm:get-nodes
Labels:       <none>
Annotations:  <none>
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  -----
  nodes      []                 []              [get]


Name:         system:aggregate-to-admin
Labels:       kubernetes.io/bootstrapping=rbac-defaults
              rbac.authorization.k8s.io/aggregate-to-admin=true
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                                       Non-Resource URLs  Resource Names  Verbs
  ---------                                       -----------------  --------------  -----
  rolebindings.rbac.authorization.k8s.io          []                 []              [create delete deletecollection get list patch update watch]
  roles.rbac.authorization.k8s.io                 []                 []              [create delete deletecollection get list patch update watch]
  localsubjectaccessreviews.authorization.k8s.io  []                 []              [create]


Name:         system:aggregate-to-edit
Labels:       kubernetes.io/bootstrapping=rbac-defaults
              rbac.authorization.k8s.io/aggregate-to-edit=true
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                                Non-Resource URLs  Resource Names  Verbs
  ---------                                -----------------  --------------  -----
  leases.coordination.k8s.io               []                 []              [create delete deletecollection get list patch update watch]
  configmaps                               []                 []              [create delete deletecollection patch update]
  events                                   []                 []              [create delete deletecollection patch update]
  persistentvolumeclaims                   []                 []              [create delete deletecollection patch update]
  pods                                     []                 []              [create delete deletecollection patch update]
  replicationcontrollers/scale             []                 []              [create delete deletecollection patch update]
  replicationcontrollers                   []                 []              [create delete deletecollection patch update]
  services                                 []                 []              [create delete deletecollection patch update]
  daemonsets.apps                          []                 []              [create delete deletecollection patch update]
  deployments.apps/rollback                []                 []              [create delete deletecollection patch update]
  deployments.apps/scale                   []                 []              [create delete deletecollection patch update]
  deployments.apps                         []                 []              [create delete deletecollection patch update]
  replicasets.apps/scale                   []                 []              [create delete deletecollection patch update]
  replicasets.apps                         []                 []              [create delete deletecollection patch update]
  statefulsets.apps/scale                  []                 []              [create delete deletecollection patch update]
  statefulsets.apps                        []                 []              [create delete deletecollection patch update]
  horizontalpodautoscalers.autoscaling     []                 []              [create delete deletecollection patch update]
  cronjobs.batch                           []                 []              [create delete deletecollection patch update]
  jobs.batch                               []                 []              [create delete deletecollection patch update]
  daemonsets.extensions                    []                 []              [create delete deletecollection patch update]
  deployments.extensions/rollback          []                 []              [create delete deletecollection patch update]
  deployments.extensions/scale             []                 []              [create delete deletecollection patch update]
  deployments.extensions                   []                 []              [create delete deletecollection patch update]
  ingresses.extensions                     []                 []              [create delete deletecollection patch update]
  networkpolicies.extensions               []                 []              [create delete deletecollection patch update]
  replicasets.extensions/scale             []                 []              [create delete deletecollection patch update]
  replicasets.extensions                   []                 []              [create delete deletecollection patch update]
  replicationcontrollers.extensions/scale  []                 []              [create delete deletecollection patch update]
  ingresses.networking.k8s.io              []                 []              [create delete deletecollection patch update]
  networkpolicies.networking.k8s.io        []                 []              [create delete deletecollection patch update]
  poddisruptionbudgets.policy              []                 []              [create delete deletecollection patch update]
  pods/eviction                            []                 []              [create]
  serviceaccounts/token                    []                 []              [create]
  pods/attach                              []                 []              [get list watch create delete deletecollection patch update]
  pods/exec                                []                 []              [get list watch create delete deletecollection patch update]
  pods/portforward                         []                 []              [get list watch create delete deletecollection patch update]
  pods/proxy                               []                 []              [get list watch create delete deletecollection patch update]
  secrets                                  []                 []              [get list watch create delete deletecollection patch update]
  services/proxy                           []                 []              [get list watch create delete deletecollection patch update]
  serviceaccounts                          []                 []              [impersonate create delete deletecollection patch update]


Name:         system:aggregate-to-view
Labels:       kubernetes.io/bootstrapping=rbac-defaults
              rbac.authorization.k8s.io/aggregate-to-view=true
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                                    Non-Resource URLs  Resource Names  Verbs
  ---------                                    -----------------  --------------  -----
  bindings                                     []                 []              [get list watch]
  configmaps                                   []                 []              [get list watch]
  endpoints                                    []                 []              [get list watch]
  events                                       []                 []              [get list watch]
  limitranges                                  []                 []              [get list watch]
  namespaces/status                            []                 []              [get list watch]
  namespaces                                   []                 []              [get list watch]
  persistentvolumeclaims/status                []                 []              [get list watch]
  persistentvolumeclaims                       []                 []              [get list watch]
  pods/log                                     []                 []              [get list watch]
  pods/status                                  []                 []              [get list watch]
  pods                                         []                 []              [get list watch]
  replicationcontrollers/scale                 []                 []              [get list watch]
  replicationcontrollers/status                []                 []              [get list watch]
  replicationcontrollers                       []                 []              [get list watch]
  resourcequotas/status                        []                 []              [get list watch]
  resourcequotas                               []                 []              [get list watch]
  serviceaccounts                              []                 []              [get list watch]
  services/status                              []                 []              [get list watch]
  services                                     []                 []              [get list watch]
  controllerrevisions.apps                     []                 []              [get list watch]
  daemonsets.apps/status                       []                 []              [get list watch]
  daemonsets.apps                              []                 []              [get list watch]
  deployments.apps/scale                       []                 []              [get list watch]
  deployments.apps/status                      []                 []              [get list watch]
  deployments.apps                             []                 []              [get list watch]
  replicasets.apps/scale                       []                 []              [get list watch]
  replicasets.apps/status                      []                 []              [get list watch]
  replicasets.apps                             []                 []              [get list watch]
  statefulsets.apps/scale                      []                 []              [get list watch]
  statefulsets.apps/status                     []                 []              [get list watch]
  statefulsets.apps                            []                 []              [get list watch]
  horizontalpodautoscalers.autoscaling/status  []                 []              [get list watch]
  horizontalpodautoscalers.autoscaling         []                 []              [get list watch]
  cronjobs.batch/status                        []                 []              [get list watch]
  cronjobs.batch                               []                 []              [get list watch]
  jobs.batch/status                            []                 []              [get list watch]
  jobs.batch                                   []                 []              [get list watch]
  endpointslices.discovery.k8s.io              []                 []              [get list watch]
  daemonsets.extensions/status                 []                 []              [get list watch]
  daemonsets.extensions                        []                 []              [get list watch]
  deployments.extensions/scale                 []                 []              [get list watch]
  deployments.extensions/status                []                 []              [get list watch]
  deployments.extensions                       []                 []              [get list watch]
  ingresses.extensions/status                  []                 []              [get list watch]
  ingresses.extensions                         []                 []              [get list watch]
  networkpolicies.extensions                   []                 []              [get list watch]
  replicasets.extensions/scale                 []                 []              [get list watch]
  replicasets.extensions/status                []                 []              [get list watch]
  replicasets.extensions                       []                 []              [get list watch]
  replicationcontrollers.extensions/scale      []                 []              [get list watch]
  ingresses.networking.k8s.io/status           []                 []              [get list watch]
  ingresses.networking.k8s.io                  []                 []              [get list watch]
  networkpolicies.networking.k8s.io            []                 []              [get list watch]
  poddisruptionbudgets.policy/status           []                 []              [get list watch]
  poddisruptionbudgets.policy                  []                 []              [get list watch]


Name:         system:auth-delegator
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                                  Non-Resource URLs  Resource Names  Verbs
  ---------                                  -----------------  --------------  -----
  tokenreviews.authentication.k8s.io         []                 []              [create]
  subjectaccessreviews.authorization.k8s.io  []                 []              [create]


Name:         system:basic-user
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                                      Non-Resource URLs  Resource Names  Verbs
  ---------                                      -----------------  --------------  -----
  selfsubjectaccessreviews.authorization.k8s.io  []                 []              [create]
  selfsubjectrulesreviews.authorization.k8s.io   []                 []              [create]


Name:         system:certificates.k8s.io:certificatesigningrequests:nodeclient
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                                                  Non-Resource URLs  Resource Names  Verbs
  ---------                                                  -----------------  --------------  -----
  certificatesigningrequests.certificates.k8s.io/nodeclient  []                 []              [create]


Name:         system:certificates.k8s.io:certificatesigningrequests:selfnodeclient
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                                                      Non-Resource URLs  Resource Names  Verbs
  ---------                                                      -----------------  --------------  -----
  certificatesigningrequests.certificates.k8s.io/selfnodeclient  []                 []              [create]


Name:         system:certificates.k8s.io:kube-apiserver-client-approver
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                    Non-Resource URLs  Resource Names                         Verbs
  ---------                    -----------------  --------------                         -----
  signers.certificates.k8s.io  []                 [kubernetes.io/kube-apiserver-client]  [approve]


Name:         system:certificates.k8s.io:kube-apiserver-client-kubelet-approver
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                    Non-Resource URLs  Resource Names                                 Verbs
  ---------                    -----------------  --------------                                 -----
  signers.certificates.k8s.io  []                 [kubernetes.io/kube-apiserver-client-kubelet]  [approve]


Name:         system:certificates.k8s.io:kubelet-serving-approver
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                    Non-Resource URLs  Resource Names                   Verbs
  ---------                    -----------------  --------------                   -----
  signers.certificates.k8s.io  []                 [kubernetes.io/kubelet-serving]  [approve]


Name:         system:certificates.k8s.io:legacy-unknown-approver
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                    Non-Resource URLs  Resource Names                  Verbs
  ---------                    -----------------  --------------                  -----
  signers.certificates.k8s.io  []                 [kubernetes.io/legacy-unknown]  [approve]


Name:         system:controller:attachdetach-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                         Non-Resource URLs  Resource Names  Verbs
  ---------                         -----------------  --------------  -----
  volumeattachments.storage.k8s.io  []                 []              [create delete get list watch]
  events                            []                 []              [create patch update]
  events.events.k8s.io              []                 []              [create patch update]
  nodes                             []                 []              [get list watch]
  csidrivers.storage.k8s.io         []                 []              [get list watch]
  csinodes.storage.k8s.io           []                 []              [get list watch]
  persistentvolumeclaims            []                 []              [list watch]
  persistentvolumes                 []                 []              [list watch]
  pods                              []                 []              [list watch]
  nodes/status                      []                 []              [patch update]


Name:         system:controller:certificate-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                                                Non-Resource URLs  Resource Names                                 Verbs
  ---------                                                -----------------  --------------                                 -----
  signers.certificates.k8s.io                              []                 [kubernetes.io/kube-apiserver-client-kubelet]  [approve sign]
  events                                                   []                 []                                             [create patch update]
  events.events.k8s.io                                     []                 []                                             [create patch update]
  subjectaccessreviews.authorization.k8s.io                []                 []                                             [create]
  certificatesigningrequests.certificates.k8s.io           []                 []                                             [delete get list watch]
  signers.certificates.k8s.io                              []                 [kubernetes.io/kube-apiserver-client]          [sign]
  signers.certificates.k8s.io                              []                 [kubernetes.io/kubelet-serving]                [sign]
  signers.certificates.k8s.io                              []                 [kubernetes.io/legacy-unknown]                 [sign]
  certificatesigningrequests.certificates.k8s.io/approval  []                 []                                             [update]
  certificatesigningrequests.certificates.k8s.io/status    []                 []                                             [update]


Name:         system:controller:clusterrole-aggregation-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                               Non-Resource URLs  Resource Names  Verbs
  ---------                               -----------------  --------------  -----
  clusterroles.rbac.authorization.k8s.io  []                 []              [escalate get list patch update watch]


Name:         system:controller:cronjob-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                  Non-Resource URLs  Resource Names  Verbs
  ---------                  -----------------  --------------  -----
  jobs.batch                 []                 []              [create delete get list patch update watch]
  events                     []                 []              [create patch update]
  events.events.k8s.io       []                 []              [create patch update]
  pods                       []                 []              [delete list]
  cronjobs.batch             []                 []              [get list update watch]
  cronjobs.batch/finalizers  []                 []              [update]
  cronjobs.batch/status      []                 []              [update]


Name:         system:controller:daemon-set-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                         Non-Resource URLs  Resource Names  Verbs
  ---------                         -----------------  --------------  -----
  controllerrevisions.apps          []                 []              [create delete get list patch update watch]
  pods                              []                 []              [create delete list patch watch]
  events                            []                 []              [create patch update]
  events.events.k8s.io              []                 []              [create patch update]
  pods/binding                      []                 []              [create]
  daemonsets.apps                   []                 []              [get list watch]
  daemonsets.extensions             []                 []              [get list watch]
  nodes                             []                 []              [list watch]
  daemonsets.apps/finalizers        []                 []              [update]
  daemonsets.apps/status            []                 []              [update]
  daemonsets.extensions/finalizers  []                 []              [update]
  daemonsets.extensions/status      []                 []              [update]


Name:         system:controller:deployment-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                          Non-Resource URLs  Resource Names  Verbs
  ---------                          -----------------  --------------  -----
  replicasets.apps                   []                 []              [create delete get list patch update watch]
  replicasets.extensions             []                 []              [create delete get list patch update watch]
  events                             []                 []              [create patch update]
  events.events.k8s.io               []                 []              [create patch update]
  pods                               []                 []              [get list update watch]
  deployments.apps                   []                 []              [get list update watch]
  deployments.extensions             []                 []              [get list update watch]
  deployments.apps/finalizers        []                 []              [update]
  deployments.apps/status            []                 []              [update]
  deployments.extensions/finalizers  []                 []              [update]
  deployments.extensions/status      []                 []              [update]


Name:         system:controller:disruption-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                           Non-Resource URLs  Resource Names  Verbs
  ---------                           -----------------  --------------  -----
  events                              []                 []              [create patch update]
  events.events.k8s.io                []                 []              [create patch update]
  replicationcontrollers              []                 []              [get list watch]
  deployments.apps                    []                 []              [get list watch]
  replicasets.apps                    []                 []              [get list watch]
  statefulsets.apps                   []                 []              [get list watch]
  deployments.extensions              []                 []              [get list watch]
  replicasets.extensions              []                 []              [get list watch]
  poddisruptionbudgets.policy         []                 []              [get list watch]
  *.*/scale                           []                 []              [get]
  poddisruptionbudgets.policy/status  []                 []              [update]


Name:         system:controller:endpoint-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources             Non-Resource URLs  Resource Names  Verbs
  ---------             -----------------  --------------  -----
  endpoints             []                 []              [create delete get list update]
  events                []                 []              [create patch update]
  events.events.k8s.io  []                 []              [create patch update]
  endpoints/restricted  []                 []              [create]
  pods                  []                 []              [get list watch]
  services              []                 []              [get list watch]


Name:         system:controller:endpointslice-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                        Non-Resource URLs  Resource Names  Verbs
  ---------                        -----------------  --------------  -----
  endpointslices.discovery.k8s.io  []                 []              [create delete get list update]
  events                           []                 []              [create patch update]
  events.events.k8s.io             []                 []              [create patch update]
  nodes                            []                 []              [get list watch]
  pods                             []                 []              [get list watch]
  services                         []                 []              [get list watch]
  services/finalizers              []                 []              [update]


Name:         system:controller:endpointslicemirroring-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                        Non-Resource URLs  Resource Names  Verbs
  ---------                        -----------------  --------------  -----
  endpointslices.discovery.k8s.io  []                 []              [create delete get list update]
  events                           []                 []              [create patch update]
  events.events.k8s.io             []                 []              [create patch update]
  endpoints                        []                 []              [get list watch]
  services                         []                 []              [get list watch]
  endpoints/finalizers             []                 []              [update]
  services/finalizers              []                 []              [update]


Name:         system:controller:ephemeral-volume-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources               Non-Resource URLs  Resource Names  Verbs
  ---------               -----------------  --------------  -----
  persistentvolumeclaims  []                 []              [create get list watch]
  events                  []                 []              [create patch update]
  events.events.k8s.io    []                 []              [create patch update]
  pods                    []                 []              [get list watch]
  pods/finalizers         []                 []              [update]


Name:         system:controller:expand-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                      Non-Resource URLs  Resource Names  Verbs
  ---------                      -----------------  --------------  -----
  events                         []                 []              [create patch update]
  events.events.k8s.io           []                 []              [create patch update]
  persistentvolumes              []                 []              [get list patch update watch]
  persistentvolumeclaims         []                 []              [get list watch]
  storageclasses.storage.k8s.io  []                 []              [get list watch]
  endpoints                      []                 []              [get]
  secrets                        []                 []              [get]
  services                       []                 []              [get]
  persistentvolumeclaims/status  []                 []              [patch update]


Name:         system:controller:generic-garbage-collector
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources             Non-Resource URLs  Resource Names  Verbs
  ---------             -----------------  --------------  -----
  events                []                 []              [create patch update]
  events.events.k8s.io  []                 []              [create patch update]
  *.*                   []                 []              [delete get list patch update watch]


Name:         system:controller:horizontal-pod-autoscaler
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                                    Non-Resource URLs  Resource Names     Verbs
  ---------                                    -----------------  --------------     -----
  events                                       []                 []                 [create patch update]
  events.events.k8s.io                         []                 []                 [create patch update]
  horizontalpodautoscalers.autoscaling         []                 []                 [get list watch]
  *.custom.metrics.k8s.io                      []                 []                 [get list]
  *.external.metrics.k8s.io                    []                 []                 [get list]
  *.*/scale                                    []                 []                 [get update]
  services/proxy                               []                 [http:heapster:]   [get]
  services/proxy                               []                 [https:heapster:]  [get]
  pods                                         []                 []                 [list]
  pods.metrics.k8s.io                          []                 []                 [list]
  horizontalpodautoscalers.autoscaling/status  []                 []                 [update]


Name:         system:controller:job-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources              Non-Resource URLs  Resource Names  Verbs
  ---------              -----------------  --------------  -----
  pods                   []                 []              [create delete list patch watch]
  events                 []                 []              [create patch update]
  events.events.k8s.io   []                 []              [create patch update]
  jobs.batch             []                 []              [get list patch update watch]
  jobs.batch/finalizers  []                 []              [update]
  jobs.batch/status      []                 []              [update]


Name:         system:controller:namespace-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources            Non-Resource URLs  Resource Names  Verbs
  ---------            -----------------  --------------  -----
  *.*                  []                 []              [delete deletecollection get list]
  namespaces           []                 []              [delete get list watch]
  namespaces/finalize  []                 []              [update]
  namespaces/status    []                 []              [update]


Name:         system:controller:node-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                       Non-Resource URLs  Resource Names  Verbs
  ---------                       -----------------  --------------  -----
  clustercidrs.networking.k8s.io  []                 []              [create get list update]
  events                          []                 []              [create patch update]
  events.events.k8s.io            []                 []              [create patch update]
  nodes                           []                 []              [delete get list patch update]
  pods                            []                 []              [delete list]
  nodes/status                    []                 []              [patch update]
  pods/status                     []                 []              [update]


Name:         system:controller:persistent-volume-binder
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                      Non-Resource URLs  Resource Names  Verbs
  ---------                      -----------------  --------------  -----
  persistentvolumes              []                 []              [create delete get list update watch]
  pods                           []                 []              [create delete get list watch]
  endpoints                      []                 []              [create delete get update]
  services                       []                 []              [create delete get]
  events.events.k8s.io           []                 []              [create patch update]
  persistentvolumeclaims         []                 []              [get list update watch]
  storageclasses.storage.k8s.io  []                 []              [get list watch]
  nodes                          []                 []              [get list]
  secrets                        []                 []              [get]
  persistentvolumeclaims/status  []                 []              [update]
  persistentvolumes/status       []                 []              [update]
  events                         []                 []              [watch create patch update]


Name:         system:controller:pod-garbage-collector
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  -----
  pods       []                 []              [delete list watch]
  nodes      []                 []              [get list]


Name:         system:controller:pv-protection-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources             Non-Resource URLs  Resource Names  Verbs
  ---------             -----------------  --------------  -----
  events                []                 []              [create patch update]
  events.events.k8s.io  []                 []              [create patch update]
  persistentvolumes     []                 []              [get list update watch]


Name:         system:controller:pvc-protection-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources               Non-Resource URLs  Resource Names  Verbs
  ---------               -----------------  --------------  -----
  events                  []                 []              [create patch update]
  events.events.k8s.io    []                 []              [create patch update]
  persistentvolumeclaims  []                 []              [get list update watch]
  pods                    []                 []              [get list watch]


Name:         system:controller:replicaset-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                          Non-Resource URLs  Resource Names  Verbs
  ---------                          -----------------  --------------  -----
  pods                               []                 []              [create delete list patch watch]
  events                             []                 []              [create patch update]
  events.events.k8s.io               []                 []              [create patch update]
  replicasets.apps                   []                 []              [get list update watch]
  replicasets.extensions             []                 []              [get list update watch]
  replicasets.apps/finalizers        []                 []              [update]
  replicasets.apps/status            []                 []              [update]
  replicasets.extensions/finalizers  []                 []              [update]
  replicasets.extensions/status      []                 []              [update]


Name:         system:controller:replication-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                          Non-Resource URLs  Resource Names  Verbs
  ---------                          -----------------  --------------  -----
  pods                               []                 []              [create delete list patch watch]
  events                             []                 []              [create patch update]
  events.events.k8s.io               []                 []              [create patch update]
  replicationcontrollers             []                 []              [get list update watch]
  replicationcontrollers/finalizers  []                 []              [update]
  replicationcontrollers/status      []                 []              [update]


Name:         system:controller:resourcequota-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources              Non-Resource URLs  Resource Names  Verbs
  ---------              -----------------  --------------  -----
  events                 []                 []              [create patch update]
  events.events.k8s.io   []                 []              [create patch update]
  *.*                    []                 []              [list watch]
  resourcequotas/status  []                 []              [update]


Name:         system:controller:root-ca-cert-publisher
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources             Non-Resource URLs  Resource Names  Verbs
  ---------             -----------------  --------------  -----
  events                []                 []              [create patch update]
  events.events.k8s.io  []                 []              [create patch update]
  configmaps            []                 []              [create update]


Name:         system:controller:route-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources             Non-Resource URLs  Resource Names  Verbs
  ---------             -----------------  --------------  -----
  events                []                 []              [create patch update]
  events.events.k8s.io  []                 []              [create patch update]
  nodes                 []                 []              [list watch]
  nodes/status          []                 []              [patch]


Name:         system:controller:service-account-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources             Non-Resource URLs  Resource Names  Verbs
  ---------             -----------------  --------------  -----
  events                []                 []              [create patch update]
  events.events.k8s.io  []                 []              [create patch update]
  serviceaccounts       []                 []              [create]


Name:         system:controller:service-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources             Non-Resource URLs  Resource Names  Verbs
  ---------             -----------------  --------------  -----
  events                []                 []              [create patch update]
  events.events.k8s.io  []                 []              [create patch update]
  services              []                 []              [get list watch]
  nodes                 []                 []              [list watch]
  services/status       []                 []              [patch update]


Name:         system:controller:statefulset-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                     Non-Resource URLs  Resource Names  Verbs
  ---------                     -----------------  --------------  -----
  controllerrevisions.apps      []                 []              [create delete get list patch update watch]
  persistentvolumeclaims        []                 []              [create get]
  events                        []                 []              [create patch update]
  events.events.k8s.io          []                 []              [create patch update]
  statefulsets.apps             []                 []              [get list watch]
  pods                          []                 []              [list watch create delete get patch update]
  statefulsets.apps/finalizers  []                 []              [update]
  statefulsets.apps/status      []                 []              [update]


Name:         system:controller:ttl-after-finished-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources             Non-Resource URLs  Resource Names  Verbs
  ---------             -----------------  --------------  -----
  events                []                 []              [create patch update]
  events.events.k8s.io  []                 []              [create patch update]
  jobs.batch            []                 []              [delete get list watch]


Name:         system:controller:ttl-controller
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources             Non-Resource URLs  Resource Names  Verbs
  ---------             -----------------  --------------  -----
  events                []                 []              [create patch update]
  events.events.k8s.io  []                 []              [create patch update]
  nodes                 []                 []              [list patch update watch]


Name:         system:coredns
Labels:       <none>
Annotations:  <none>
PolicyRule:
  Resources                        Non-Resource URLs  Resource Names  Verbs
  ---------                        -----------------  --------------  -----
  nodes                            []                 []              [get]
  endpoints                        []                 []              [list watch]
  namespaces                       []                 []              [list watch]
  pods                             []                 []              [list watch]
  services                         []                 []              [list watch]
  endpointslices.discovery.k8s.io  []                 []              [list watch]


Name:         system:discovery
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  -----
             [/api/*]           []              [get]
             [/api]             []              [get]
             [/apis/*]          []              [get]
             [/apis]            []              [get]
             [/healthz]         []              [get]
             [/livez]           []              [get]
             [/openapi/*]       []              [get]
             [/openapi]         []              [get]
             [/readyz]          []              [get]
             [/version/]        []              [get]
             [/version]         []              [get]


Name:         system:heapster
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources               Non-Resource URLs  Resource Names  Verbs
  ---------               -----------------  --------------  -----
  events                  []                 []              [get list watch]
  namespaces              []                 []              [get list watch]
  nodes                   []                 []              [get list watch]
  pods                    []                 []              [get list watch]
  deployments.extensions  []                 []              [get list watch]


Name:         system:kube-aggregator
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  -----
  endpoints  []                 []              [get list watch]
  services   []                 []              [get list watch]


Name:         system:kube-controller-manager
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                                  Non-Resource URLs  Resource Names             Verbs
  ---------                                  -----------------  --------------             -----
  secrets                                    []                 []                         [create delete get update]
  serviceaccounts                            []                 []                         [create get update]
  events                                     []                 []                         [create patch update]
  events.events.k8s.io                       []                 []                         [create patch update]
  endpoints                                  []                 []                         [create]
  serviceaccounts/token                      []                 []                         [create]
  tokenreviews.authentication.k8s.io         []                 []                         [create]
  subjectaccessreviews.authorization.k8s.io  []                 []                         [create]
  leases.coordination.k8s.io                 []                 []                         [create]
  endpoints                                  []                 [kube-controller-manager]  [get update]
  leases.coordination.k8s.io                 []                 [kube-controller-manager]  [get update]
  configmaps                                 []                 []                         [get]
  namespaces                                 []                 []                         [get]
  *.*                                        []                 []                         [list watch]


Name:         system:kube-dns
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  -----
  endpoints  []                 []              [list watch]
  services   []                 []              [list watch]


Name:         system:kube-scheduler
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                                  Non-Resource URLs  Resource Names    Verbs
  ---------                                  -----------------  --------------    -----
  events                                     []                 []                [create patch update]
  events.events.k8s.io                       []                 []                [create patch update]
  bindings                                   []                 []                [create]
  endpoints                                  []                 []                [create]
  pods/binding                               []                 []                [create]
  tokenreviews.authentication.k8s.io         []                 []                [create]
  subjectaccessreviews.authorization.k8s.io  []                 []                [create]
  leases.coordination.k8s.io                 []                 []                [create]
  pods                                       []                 []                [delete get list watch]
  namespaces                                 []                 []                [get list watch]
  nodes                                      []                 []                [get list watch]
  persistentvolumeclaims                     []                 []                [get list watch]
  persistentvolumes                          []                 []                [get list watch]
  replicationcontrollers                     []                 []                [get list watch]
  services                                   []                 []                [get list watch]
  replicasets.apps                           []                 []                [get list watch]
  statefulsets.apps                          []                 []                [get list watch]
  replicasets.extensions                     []                 []                [get list watch]
  poddisruptionbudgets.policy                []                 []                [get list watch]
  csidrivers.storage.k8s.io                  []                 []                [get list watch]
  csinodes.storage.k8s.io                    []                 []                [get list watch]
  csistoragecapacities.storage.k8s.io        []                 []                [get list watch]
  endpoints                                  []                 [kube-scheduler]  [get update]
  leases.coordination.k8s.io                 []                 [kube-scheduler]  [get update]
  pods/status                                []                 []                [patch update]


Name:         system:kubelet-api-admin
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources      Non-Resource URLs  Resource Names  Verbs
  ---------      -----------------  --------------  -----
  nodes/log      []                 []              [*]
  nodes/metrics  []                 []              [*]
  nodes/proxy    []                 []              [*]
  nodes/spec     []                 []              [*]
  nodes/stats    []                 []              [*]
  nodes          []                 []              [get list watch proxy]


Name:         system:monitoring
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  -----
             [/healthz/*]       []              [get]
             [/healthz]         []              [get]
             [/livez/*]         []              [get]
             [/livez]           []              [get]
             [/metrics]         []              [get]
             [/readyz/*]        []              [get]
             [/readyz]          []              [get]


Name:         system:node
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                                       Non-Resource URLs  Resource Names  Verbs
  ---------                                       -----------------  --------------  -----
  leases.coordination.k8s.io                      []                 []              [create delete get patch update]
  csinodes.storage.k8s.io                         []                 []              [create delete get patch update]
  nodes                                           []                 []              [create get list watch patch update]
  certificatesigningrequests.certificates.k8s.io  []                 []              [create get list watch]
  events                                          []                 []              [create patch update]
  pods/eviction                                   []                 []              [create]
  serviceaccounts/token                           []                 []              [create]
  tokenreviews.authentication.k8s.io              []                 []              [create]
  localsubjectaccessreviews.authorization.k8s.io  []                 []              [create]
  subjectaccessreviews.authorization.k8s.io       []                 []              [create]
  pods                                            []                 []              [get list watch create delete]
  configmaps                                      []                 []              [get list watch]
  secrets                                         []                 []              [get list watch]
  services                                        []                 []              [get list watch]
  runtimeclasses.node.k8s.io                      []                 []              [get list watch]
  csidrivers.storage.k8s.io                       []                 []              [get list watch]
  persistentvolumeclaims/status                   []                 []              [get patch update]
  endpoints                                       []                 []              [get]
  persistentvolumeclaims                          []                 []              [get]
  persistentvolumes                               []                 []              [get]
  volumeattachments.storage.k8s.io                []                 []              [get]
  nodes/status                                    []                 []              [patch update]
  pods/status                                     []                 []              [patch update]


Name:         system:node-bootstrapper
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                                       Non-Resource URLs  Resource Names  Verbs
  ---------                                       -----------------  --------------  -----
  certificatesigningrequests.certificates.k8s.io  []                 []              [create get list watch]


Name:         system:node-problem-detector
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources             Non-Resource URLs  Resource Names  Verbs
  ---------             -----------------  --------------  -----
  events                []                 []              [create patch update]
  events.events.k8s.io  []                 []              [create patch update]
  nodes                 []                 []              [get]
  nodes/status          []                 []              [patch]


Name:         system:node-proxier
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                        Non-Resource URLs  Resource Names  Verbs
  ---------                        -----------------  --------------  -----
  events                           []                 []              [create patch update]
  events.events.k8s.io             []                 []              [create patch update]
  nodes                            []                 []              [get list watch]
  endpoints                        []                 []              [list watch]
  services                         []                 []              [list watch]
  endpointslices.discovery.k8s.io  []                 []              [list watch]


Name:         system:persistent-volume-provisioner
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                      Non-Resource URLs  Resource Names  Verbs
  ---------                      -----------------  --------------  -----
  persistentvolumes              []                 []              [create delete get list watch]
  events.events.k8s.io           []                 []              [create patch update]
  persistentvolumeclaims         []                 []              [get list update watch]
  storageclasses.storage.k8s.io  []                 []              [get list watch]
  events                         []                 []              [watch create patch update]


Name:         system:public-info-viewer
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  -----
             [/healthz]         []              [get]
             [/livez]           []              [get]
             [/readyz]          []              [get]
             [/version/]        []              [get]
             [/version]         []              [get]


Name:         system:service-account-issuer-discovery
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources  Non-Resource URLs                    Resource Names  Verbs
  ---------  -----------------                    --------------  -----
             [/.well-known/openid-configuration]  []              [get]
             [/openid/v1/jwks]                    []              [get]


Name:         system:volume-scheduler
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                      Non-Resource URLs  Resource Names  Verbs
  ---------                      -----------------  --------------  -----
  persistentvolumeclaims         []                 []              [get list patch update watch]
  persistentvolumes              []                 []              [get list patch update watch]
  storageclasses.storage.k8s.io  []                 []              [get list watch]


Name:         view
Labels:       kubernetes.io/bootstrapping=rbac-defaults
              rbac.authorization.k8s.io/aggregate-to-edit=true
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                                    Non-Resource URLs  Resource Names  Verbs
  ---------                                    -----------------  --------------  -----
  bindings                                     []                 []              [get list watch]
  configmaps                                   []                 []              [get list watch]
  endpoints                                    []                 []              [get list watch]
  events                                       []                 []              [get list watch]
  limitranges                                  []                 []              [get list watch]
  namespaces/status                            []                 []              [get list watch]
  namespaces                                   []                 []              [get list watch]
  persistentvolumeclaims/status                []                 []              [get list watch]
  persistentvolumeclaims                       []                 []              [get list watch]
  pods/log                                     []                 []              [get list watch]
  pods/status                                  []                 []              [get list watch]
  pods                                         []                 []              [get list watch]
  replicationcontrollers/scale                 []                 []              [get list watch]
  replicationcontrollers/status                []                 []              [get list watch]
  replicationcontrollers                       []                 []              [get list watch]
  resourcequotas/status                        []                 []              [get list watch]
  resourcequotas                               []                 []              [get list watch]
  serviceaccounts                              []                 []              [get list watch]
  services/status                              []                 []              [get list watch]
  services                                     []                 []              [get list watch]
  controllerrevisions.apps                     []                 []              [get list watch]
  daemonsets.apps/status                       []                 []              [get list watch]
  daemonsets.apps                              []                 []              [get list watch]
  deployments.apps/scale                       []                 []              [get list watch]
  deployments.apps/status                      []                 []              [get list watch]
  deployments.apps                             []                 []              [get list watch]
  replicasets.apps/scale                       []                 []              [get list watch]
  replicasets.apps/status                      []                 []              [get list watch]
  replicasets.apps                             []                 []              [get list watch]
  statefulsets.apps/scale                      []                 []              [get list watch]
  statefulsets.apps/status                     []                 []              [get list watch]
  statefulsets.apps                            []                 []              [get list watch]
  horizontalpodautoscalers.autoscaling/status  []                 []              [get list watch]
  horizontalpodautoscalers.autoscaling         []                 []              [get list watch]
  cronjobs.batch/status                        []                 []              [get list watch]
  cronjobs.batch                               []                 []              [get list watch]
  jobs.batch/status                            []                 []              [get list watch]
  jobs.batch                                   []                 []              [get list watch]
  endpointslices.discovery.k8s.io              []                 []              [get list watch]
  daemonsets.extensions/status                 []                 []              [get list watch]
  daemonsets.extensions                        []                 []              [get list watch]
  deployments.extensions/scale                 []                 []              [get list watch]
  deployments.extensions/status                []                 []              [get list watch]
  deployments.extensions                       []                 []              [get list watch]
  ingresses.extensions/status                  []                 []              [get list watch]
  ingresses.extensions                         []                 []              [get list watch]
  networkpolicies.extensions                   []                 []              [get list watch]
  replicasets.extensions/scale                 []                 []              [get list watch]
  replicasets.extensions/status                []                 []              [get list watch]
  replicasets.extensions                       []                 []              [get list watch]
  replicationcontrollers.extensions/scale      []                 []              [get list watch]
  ingresses.networking.k8s.io/status           []                 []              [get list watch]
  ingresses.networking.k8s.io                  []                 []              [get list watch]
  networkpolicies.networking.k8s.io            []                 []              [get list watch]
  poddisruptionbudgets.policy/status           []                 []              [get list watch]
  poddisruptionbudgets.policy                  []                 []              [get list watch]


Name:         weave-net
Labels:       name=weave-net
Annotations:  <none>
PolicyRule:
  Resources                          Non-Resource URLs  Resource Names  Verbs
  ---------                          -----------------  --------------  -----
  namespaces                         []                 []              [get list watch]
  nodes                              []                 []              [get list watch]
  pods                               []                 []              [get list watch]
  networkpolicies.extensions         []                 []              [get list watch]
  networkpolicies.networking.k8s.io  []                 []              [get list watch]
  nodes/status                       []                 []              [patch update]
###################################################################++==++++++++++++++++++++++++++++++=========================
anji@master:~$ kubectl describe clusterrole  cluster-admin
Name:         cluster-admin
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  -----
  *.*        []                 []              [*]
             [*]                []              [*]

=====
anji@master:~$ kubectl describe clusterrole  view

anji@master:~$ kubectl describe clusterrole  edit

anji@master:~$ kubectl describe clusterrole  admin


====++++++++
anji@master:~$ kubectl create namespace  godfather
namespace/godfather created
anji@master:~$ kubectl get  ns
NAME              STATUS   AGE
default           Active   2d6h
godfather         Active   7s
kube-node-lease   Active   2d6h
kube-public       Active   2d6h
kube-system       Active   2d6h

anji@master:~$ kubectl create deployment nginx  --image=nginx -n godfather
deployment.apps/nginx created

anji@master:~$ kubectl get all 
NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   2d6h
anji@master:~$ kubectl get all  -n godfather
NAME                        READY   STATUS    RESTARTS   AGE
pod/nginx-76d6c9b8c-99tnd   1/1     Running   0          33s

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nginx   1/1     1            1           33s

NAME                              DESIRED   CURRENT   READY   AGE
replicaset.apps/nginx-76d6c9b8c   1         1         1       33s
======-----{}{]]]}  =---""  
-------------------------------------------------------------------------------------------------------------"
wipro@master:~$ kubectl get all -n godfather 
Error from server (Forbidden): "pods "is forbidden: User "wipro" cannot list resource "pods" in API group "" in the namespace "godfather"
Error from server (Forbidden): "replicationcontrollers "is forbidden: User "wipro" cannot list resource "replicationcontrollers" in API group "" in the namespace "godfather"
Error from server (Forbidden): s"ervices is forbidden": User "wipro" cannot list resource "services" in API group "" in the namespace "godfather"
Error from server (Forbidden): d"aemonsets.apps" is forbidden: User "wipro" cannot list resource "daemonsets" in API group "apps" in the namespace "godfather"
Error from server (Forbidden): "deployments."apps is forbidden: User "wipro" cannot list resource "deployments" in API group "apps" in the namespace "godfather"
Error from server (Forbidden): r"eplicasets."apps is forbidden: User "wipro" cannot list resource "replicasets" in API group "apps" in the namespace "godfather"
Error from server (Forbidden): "statefulsets."apps is forbidden: User "wipro" cannot list resource "statefulsets" in API group "apps" in the namespace "godfather"
Error from server (Forbidden): "horizontalpodautoscalers".autoscaling is forbidden: User "wipro" cannot list resource "horizontalpodautoscalers" in API group "autoscaling" in the namespace "godfather"
Error from server (Forbidden): "cronjobs."batch is forbidden: User "wipro" cannot list resource "cronjobs" in API group "batch" in the namespace "godfather"
Error from server (Forbidden): j"obs."batch is forbidden: User "wipro" cannot list resource "jobs" in API group "batch" in the namespace "godfather" "

kubectl create rolebinding --help | less

  kubectl create rolebinding admin --clusterrole=admin --user=user1 --user=user2 --group=group1

anji@master:~$   kubectl create rolebinding admin --clusterrole=admin --user=user1 --user=user2 --group=group1
rolebinding.rbac.authorization.k8s.io/admin created
anji@master:~$  kubectl delete rolebinding admin 
'rolebinding.rbac.authorization.k8s.io "admin" deleted


anji@master:~$   kubectl create rolebinding wipro  --clusterrole=view --user=wipro -n godfather
rolebinding.rbac.authorization.k8s.io/wipro created

====\\\\\\\\\\\
wipro@master:~$ kubectl get all -n godfather
NAME                        READY   STATUS    RESTARTS      AGE
pod/nginx-76d6c9b8c-99tnd   1/1     Running   1 (50m ago)   70m

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nginx   1/1     1            1           70m

NAME                              DESIRED   CURRENT   READY   AGE
replicaset.apps/nginx-76d6c9b8c   1         1         1       70m
=----
anji@master:~$ kubectl get rolebinding  
No resources found in default namespace.

anji@master:~$ kubectl get rolebinding  -n godfather  -o wide 
NAME    ROLE               AGE     USERS   GROUPS   SERVICEACCOUNTS
wipro   ClusterRole/view   3m58s   wipro 
=======-------------////////
wipro@master:~$ kubectl edit deployment nginx -n godfather 
error: deployments.apps "nginx" could not be patched: deployments.apps "nginx" is forbidden: User "wipro" cannot patch resource "deployments" in API group "apps" in the namespace "godfather"
You can run `kubectl replace -f /tmp/kubectl-edit-1600269644.yaml` to try this update again.

anji@master:~$ kubectl delete  rolebinding wipro -n godfather
rolebinding.rbac.authorization.k8s.io "wipro" deleted

nji@master:~$ kubectl create rolebinding wipro1 --clusterrole=edit --user=wipro -n godfather 
rolebinding.rbac.authorization.k8s.io/wipro1 created

anji@master:~$ kubectl get rolebinding -n godfather  -o wide 
NAME     ROLE               AGE   USERS   GROUPS   SERVICEACCOUNTS
wipro1   ClusterRole/edit   79s   wipro            
========================#########
anji@master:~$ kubectl get all -n godfather -o wide 
NAME                        READY   STATUS    RESTARTS      AGE   IP          NODE      NOMINATED NODE   READINESS GATES
pod/nginx-76d6c9b8c-99tnd   1/1     Running   1 (67m ago)   86m   10.44.0.3   worker1   <none>           <none>

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES   SELECTOR
deployment.apps/nginx   1/1     1            1           86m   nginx        nginx    app=nginx

NAME                              DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES   SELECTOR
replicaset.apps/nginx-76d6c9b8c   1         1         1       86m   nginx        nginx    app=nginx,pod-template-hash=76d6c9b8c
anji@master:~$ kubectl get all -n godfather -o wide 
----=========+++++++++++
anji@master:~$ kubectl get all -n godfather -o wide 
NAME                        READY   STATUS    RESTARTS      AGE   IP          NODE      NOMINATED NODE   READINESS GATES
pod/nginx-76d6c9b8c-99tnd   1/1     Running   1 (67m ago)   86m   10.44.0.3   worker1   <none>           <none>

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES   SELECTOR
deployment.apps/nginx   1/1     1            1           86m   nginx        nginx    app=nginx

NAME                              DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES   SELECTOR
replicaset.apps/nginx-76d6c9b8c   1         1         1       86m   nginx        nginx    app=nginx,pod-template-hash=76d6c9b8c "
#########################################################################################################################"
""=============////////////\\\\\\\\\=\\
https://www.youtube.com/watch?v=rXTpVGIENAQ&list=PL8SR-mMnL9f6KKYDnrG3saVnyLqNDz1GP&index=17
Kubernetes-Day-17
Kubernetes - Secrets and Configmaps

https://medium.com/open-devops-academy/deploy-a-mysql-db-on-kubernetes-b268333a4881
https://bluexp.netapp.com/blog/how-to-set-up-mysql-kubernetes-deployments
https://github.com/kubernetes/examples/blob/master/mysql-wordpress-pd/mysql-deployment.yaml
https://dev.to/musolemasu/deploy-a-mysql-database-server-in-kubernetes-static-dpc
https://phoenixnap.com/kb/kubernetes-mysql


apiVersion: apps/v1
kind: Deployment
metadata: 
   name: mysql
   labels: 
     app: mysql
spec: 
  selector:
    matchLabels:
      app: mysql
      tier: mysql
  strategy:
     type: Recreate
  template:
     metadata: 
       labels: 
         app: mysql
         tier: mysql
     spec: 
       containers: 
       - name: mysql
         image: mysql:5.6
         env:                            #\\\ see look 
          -  name: MYSQL_ROOT_PASSWORD
             value: redhat                   ## look see 
         ports: 
         - containerPort: 3306
           name: mysql
"
anji@master:~$ kubectl exec mysql-cc4cbfff4-99nh2  -it -- /bin/bash

root@mysql-cc4cbfff4-99nh2:/# mysql -p
Enter password: redhat
mysql> 
==========="
anji@master:~$ kubectl get po
NAME                    READY   STATUS    RESTARTS   AGE
mysql-cc4cbfff4-99nh2   1/1     Running   0          80m

anji@master:~$ kubectl describe pod mysql-cc4cbfff4-99nh2 
Name:             mysql-cc4cbfff4-99nh2

Node:             worker1/192.168.122.131
Start Time:       Sun, 01 Jan 2023 15:21:43 +0530
Labels:           app=mysql
                  pod-template-hash=cc4cbfff4
                  tier=mysql
Controlled By:  ReplicaSet/mysql-cc4cbfff4
Containers:
  mysql:
    Image:          mysql:5.6
    Port:           3306/TCP
    Environment:
      MYSQL_ROOT_PASSWORD:  redhat  #////  see look 
=========------"
anji@master:~/test$ kubectl create secret generic book --from-literal=password=redhat
secret/book created

anji@master:~/test$ kubectl get secret
NAME   TYPE     DATA   AGE
book   Opaque   1      16s

anji@master:~/test$ kubectl describe secret
Name:         book
Namespace:    default
Labels:       <none>
Annotations:  <none>
Type:  Opaque
Data
====
password:  6 bytes
====----"
anji@master:~$ kubectl edit secret
# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: v1
data:
  password: cmVkaGF0         ## look see  encode base64 
kind: Secret
metadata:
  creationTimestamp: "2023-01-01T11:26:01Z"
  name: book
  namespace: default
  resourceVersion: "242656"
  uid: a1461a2a-6e60-42a9-99a5-7616c04af01e
type: Opaque
----===/////
anji@master:~$ echo "redhat" | base64 -w0
cmVkaGF0

anji@master:~$ echo "anjireddy"  | base64
YW5qaXJlZGR5Cg==

anji@master:~$ echo "venkata" | base64
dmVua2F0YQo=

anji@master:~$ echo "a" | base64
YQo=

=========================================+++++++++++++++++++++++"
apiVersion: apps/v1
kind: Deployment
metadata: 
   name: mysql
   labels: 
     app: mysql
spec: 
   selector: 
     matchLabels:
        app: mysql
        tier: mysql
   strategy: 
      type: Recreate
   template: 
      metadata: 
        labels: 
           app: mysql
           tier: mysql
      spec:
        containers:
        - image: mysql
          name: mysql
          env: 
          - name: MYSQL_ROOT_PASSWORD     
            valueFrom:                    ## see look 
              secretKeyRef:            /// see look 
                name: book             ## book 
                key: password
"                                  
anji@master:~$ kubectl get po,secret  -o wide 
NAME                         READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES
pod/mysql-85b96fd6d9-zntqr   1/1     Running   0          3s    10.44.0.3   worker1   <none>           <none>

NAME          TYPE     DATA   AGE
"secret/book "  Opaque   1      47m "

anji@master:~$ kubectl describe secret  book 
Name:         book
Namespace:    default
Labels:       <none>
Annotations:  <none>
Type:  Opaque
Data
====
password:  6 bytes "
--------------------
anji@master:~$ kubectl describe pod mysql-85b96fd6d9-zntqr 
Name:             mysql-85b96fd6d9-zntqr
Labels:           app=mysql
                  pod-template-hash=85b96fd6d9
                  tier=mysql
Controlled By:  ReplicaSet/mysql-85b96fd6d9
Containers:
  mysql:
    Image:          mysql                                         ||  see look 
    Environment:
      MYSQL_ROOT_PASSWORD: " <set to the key 'password' in secret 'book'>  Optional: false"

-----=== ## @@@@  BEFORE  BEFORE ---==== ### 
anji@master:~$ kubectl edit secret  BOOK

# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: v1
data:
  password: cmVkaGF0    ##  SEE LOOK IF  CHANDED POD IS RE-STARTED 
kind: Secret
metadata:
  creationTimestamp: "2023-01-01T11:26:01Z"
  name: book
  namespace: default
  resourceVersion: "242656"
  uid: a1461a2a-6e60-42a9-99a5-7616c04af01e
type: Opaque
~            
++++ "
anji@master:~/test$ echo  "anjireddy" | base64
YW5qaXJlZGR5Cg==                                  ###  REPLACE THE VALUE 
####"
anji@master:~$ kubectl get po,secret
NAME                         READY   STATUS    RESTARTS   AGE
pod/"mysql-85b96fd6d9-zntqr "  1/1     Running   0          31m     ## restart restart restart 

NAME          TYPE     DATA   AGE
secret/book   Opaque   1      78m
-----   ##  AFTER  AFTER   AFTER  ===  
nji@master:~$ kubectl edit secret  BOOK

# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: v1
data:
  password: YW5qaXJlZGR5Cg==   ##  SEE LOOK IF  CHANDED POD IS RE-STARTED 
kind: Secret
metadata:
  creationTimestamp: "2023-01-01T11:26:01Z"
  name: book
  namespace: default
  resourceVersion: "242656"
  uid: a1461a2a-6e60-42a9-99a5-7616c04af01e
type: Opaque

anji@master:~$ kubectl get all 
NAME                         READY   STATUS    RESTARTS   AGE
pod/mysql-85b96fd6d9-zntqr   1/1     Running   0          53m

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4d6h

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/mysql   1/1     1            1           53m

NAME                               DESIRED   CURRENT   READY   AGE
replicaset.apps/mysql-85b96fd6d9   1         1         1       53m
===============+++++++++++++++++#############################"
" \\\\\\\\\///  configmap  
anji@master:~$ kubectl create configmap devops --from-literal=password=sony
configmap/devops created

anji@master:~$ kubectl get configmap  -o wide 
NAME               DATA   AGE
devops             1      29s
kube-root-ca.crt   1      4d6h

anji@master:~$ kubectl describe configmap devops  "
Name:         devops
Namespace:    default
Labels:       <none>
Annotations:  <none>
Data
====
password:
----
sony       ##  is password is showing appeared 
BinaryData
====
Events:  <none>
++++++++++++++== "
anji@master:~$ kubectl edit configmap devops
# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: v1
data:
  password: sony    "##  is password is appeared  "
kind: ConfigMap     
metadata:
  creationTimestamp: "2023-01-01T13:24:26Z"
  name: devops
  namespace: default
  resourceVersion: "251914"
  uid: 9bb15c73-18f8-4a15-a040-530368c8adbd
++++++++++++++++++===

apiVersion: apps/v1
kind: Deployment
metadata: 
   name: mysql
   labels: 
      app: mysql
spec: 
  selector: 
     matchLabels: 
       app: mysql
       tier: mysql
  strategy: 
     type: Recreate
  template: 
     metadata:
        labels:
          app: mysql
          tier: mysql
     spec: 
       containers:
       - name: mysql
         image: mysql
         env:
         - name: MYSQL_ROOT_PASSWORD
           valueFrom:  
              configMapKeyRef:
                 name: devops
                 key: password
"                                      
anji@master:~$ kubectl get po 
NAME                     READY   STATUS    RESTARTS   AGE
mysql-7bff89f75f-wfwhw   1/1     Running   0          34m

anji@master:~$ kubectl describe pod  mysql-7bff89f75f-wfwhw 
Name:             mysql-7bff89f75f-wfwhw
Labels:           app=mysql
                  pod-template-hash=7bff89f75f
                  tier=mysql
Containers:
  mysql:
    Image:          mysql
    Environment:
      MYSQL_ROOT_PASSWORD:  <set to the key 'password' of config map 'devops'>  Optional: false "
#########################################+++++++++++++++++++++++++++++++++++++++++++
anji@master:~$ cat ./index.html 
<h1> WISH YOU HAPPYNEWYEAR  <h1>

anji@master:~$ kubectl create secret  generic  htmlfile --from-file=index.html=./index.html
secret/htmlfile created

anji@master:~$ kubectl get secret  "
NAME       TYPE     DATA   AGE
htmlfile   Opaque   1      59s

anji@master:~$ kubectl describe secret  htmlfile
Name:         htmlfile
Namespace:    default
Labels:       <none>
Annotations:  <none>
Type:  Opaque
Data
====
index.html:  33 bytes "
----------------------
anji@master:~$ kubectl edit secret htmlfile
# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: v1
data:
  index.html: PGgxPiBXSVNIIFlPVSBIQVBQWU5FV1lFQVIgIDxoMT4K
kind: Secret
metadata:
  creationTimestamp: "2023-01-01T14:48:37Z"
  name: htmlfile
  namespace: default
  resourceVersion: "258489"
  uid: eeafef1c-0f3b-4452-986e-a4be244f0614
type: Opaque 
----"
apiVersion: apps/v1
kind: Deployment
metadata: 
   name: nginx
   labels:
     app: nginx
spec: 
  replicas: 1
  selector:
     matchLabels:
       app: nginx
  template:
    metadata:
       labels:
         app: nginx
    spec: 
      volumes:
        - name: myvol
          secret:
            secretName: htmlfile
      containers: 
       - name: nginx
         image: nginx
         ports:
           - containerPort: 80
         volumeMounts: 
           - name: myvol
             mountPath: /usr/share/nginx/html               
             readOnly: true

======---"
anji@master:~$ kubectl describe pod  nginx-5ddb4f4bdb-cl42n
Name:             nginx-5ddb4f4bdb-cl42n
Labels:           app=nginx
                  pod-template-hash=5ddb4f4bdb
Controlled By:  ReplicaSet/nginx-5ddb4f4bdb
Containers:
  nginx:
    Image:          nginx
      Environment:    <none>
    Mounts:
      /usr/share/nginx/html from myvol (ro)
Volumes:
  myvol:
    Type:        Secret (a volume populated by a Secret)   ## see look 
    SecretName:  htmlfile      ##  see look  "
-------------------------"
https://kubernetes.io/docs/concepts/configuration/secret/

apiVersion: v1
kind: Secret
metadata:
  name: secret-sa-sample
  annotations:
    kubernetes.io/service-account.name: "sa-name"
type: kubernetes.io/service-account-token
data:
  # You can include additional key value pairs as you do with Opaque Secrets
  extra: YmFyCg==
------"
Types of Secret : = ## 

Built-in Type                	                                    Usage
Opaque	                                                          arbitrary user-defined data
kubernetes.io/service-account-token	                              ServiceAccount token
kubernetes.io/dockercfg	                                          serialized ~/.dockercfg file
kubernetes.io/dockerconfigjson                                   	serialized ~/.docker/config.json file
kubernetes.io/basic-auth	                                        credentials for basic authentication
kubernetes.io/ssh-auth	                                          credentials for SSH authentication
kubernetes.io/tls	                                                data for a TLS client or server
bootstrap.kubernetes.io/token	                                    bootstrap token data

apiVersion: v1
kind: Secret
metadata:
  name: mysecret
type: Opaque
data:
  username: YWRtaW4=
  password: MWYyZDFlMmU2N2Rm

===----"
apiVersion: v1
kind: Secret
metadata:
  name: indexfile
type: Opaque
data: 
  index.html: PGgxPiBXSVNIIFlPVSBIQVBQWU5FV1lFQVIgIDxoMT4K
  
=====   "
anji@master:~$ kubectl get secret 
NAME        TYPE     DATA   AGE
indexfile   Opaque   1      40s
anji@master:~$ kubectl describe secret 
Name:         indexfile
Namespace:    default
Labels:       <none>
Annotations:  <none>
Type:  Opaque
Data
====
index.html:  33 bytes
=====++++++++++"
anji@master:~$ kubectl edit secret indexfile
# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: v1
data:
  index.html: PGgxPiBXSVNIIFlPVSBIQVBQWU5FV1lFQVIgIDxoMT4K
kind: Secret
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","data":{"index.html":"PGgxPiBXSVNIIFlPVSBIQVBQWU5FV1lFQVIgIDxoMT4K"},"kind":"Secret","metadata":{"annotations":{},"name":"indexfile","namespace":"default"},"type":"Opaque"}
  creationTimestamp: "2023-01-02T07:16:07Z"
  name: indexfile
  namespace: default
  resourceVersion: "286737"
  uid: 17a350fa-ffce-4afa-8c56-2233686d6d9a
type: Opaque
=====================
anji@master:~$ kubectl create configmap indexmap  --from-file=index.html=./index.html
configmap/indexmap created

anji@master:~$ kubectl get configmap 
NAME               DATA   AGE
indexmap           1      15s
kube-root-ca.crt   1      5d1h "
anji@master:~$ kubectl describe configmap  indexmap
Name:         indexmap
Namespace:    default
Labels:       <none>
Annotations:  <none>
Data
====
index.html:
----
<h1> WISH YOU HAPPYNEWYEAR  <h1>
BinaryData
====
Events:  <none>
============+++++++++++"
anji@master:~$ kubectl edit configmap indexmap
# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: v1
data:
  index.html: |
    <h1> WISH YOU HAPPYNEWYEAR  <h1>
kind: ConfigMap
metadata:
  creationTimestamp: "2023-01-02T07:47:29Z"
  name: indexmap
  namespace: default
  resourceVersion: "289179"
  uid: 6748cd23-23fa-464b-89d3-d19f0f0a0168
############################################ "
---"
anji@master:~$ kubectl create configmap indexmap  --from-file=index.html=./index.html
configmap/indexmap created


apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec: 
  replicas: 1
  selector:
    matchLabels: 
      app: nginx
  template:
     metadata: 
       labels: 
         app: nginx
     spec: 
       volumes: 
         - name: myvol
           configMap:
             name: indexmap
       containers: 
       - name: nginx
         image: nginx
         volumeMounts:
           - name: myvol
             mountPath: /usr/share/nginx/html
             readOnly: true       "
      
anji@master:~$ kubectl get pod,configmap  -o wide 
NAME                         READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES
pod/nginx-6bf598886b-dpfkb   1/1     Running   0          5m52s   10.44.0.3   worker1   <none>           <none>

NAME                         DATA   AGE
configmap/"indexmap     "      1      4m23s
configmap/kube-root-ca.crt   1      5d1h "
-========
anji@master:~$ kubectl describe pod nginx-6bf598886b-dpfkb
Name:             nginx-6bf598886b-dpfkb
Labels:           app=nginx
                  pod-template-hash=6bf598886b
Containers:
  nginx:
    Image:          nginx
    Mounts:
      /usr/share/nginx/html from myvol (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-nrpkr (ro)
Volumes:
  myvol:
    Type:     " ConfigMap (a volume populated by a ConfigMap)"
    Name:      "indexmap"
    Optional:  false
----------"    ##  BEFORE   BEFORE  BEFORE  
anji@master:~$ kubectl edit configmap indexmap
# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: v1
data:
  index.html: |
    <h1> WISH YOU HAPPYNEWYEAR  <h1>
kind: ConfigMap
metadata:
  creationTimestamp: "2023-01-02T08:14:50Z"
  name: indexmap
  namespace: default
  resourceVersion: "291333"
  uid: 0d3e9b51-dc20-4401-92e3-4fc397c72444
=====  ## AFTER  AFTER   
# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: v1
data:
  index.html: |
    <h1> WISH YOU HAPPYNEWYEAR  <h1>        #  see look 
    <h2> THIS IS DEVOPS CLASS ZZZZZZXXXX<h2>    ##  see  look 
kind: ConfigMap
metadata:
  creationTimestamp: "2023-01-02T08:14:50Z"
  name: indexmap
  namespace: default
  resourceVersion: "292980"
  uid: 0d3e9b51-dc20-4401-92e3-4fc397c724443
=====---- "
anji@master:~$ kubectl get all,configmap  -o wide
NAME                         READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES
pod/"nginx-6bf598886b-dpfkb "  1/1     Running   0          26m   10.44.0.3   worker1   <none>           <none>

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE    SELECTOR
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   5d1h   <none>

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES   SELECTOR
"deployment.apps/nginx "  1/1     1            1           26m   nginx        nginx    app=nginx

NAME                               DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES   SELECTOR
replicaset.apps/nginx-6bf598886b   1         1         1       26m   nginx        nginx    app=nginx,pod-template-hash=6bf598886b

NAME                         DATA   AGE
configmap/"indexmap "          1      25m
configmap/kube-root-ca.crt   1      5d1h
======="
anji@master:~$ kubectl set env deployment nginx  --from=configmap/indexmap
Warning: key index.html transferred to INDEX_HTML
deployment.apps/nginx env updated                                  ##  restart the pod  see look 
+++++
anji@master:~$ kubectl get all,configmap  -o wide
NAME                        READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES
pod/"nginx-cc7957db6-5mzj4 "  1/1     Running   0          45s   10.44.0.4   worker1   <none>           <none>

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE    SELECTOR
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   5d2h   <none>

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES   SELECTOR
deployment.apps/nginx   1/1     1            1           31m   nginx        nginx    app=nginx

NAME                               DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES   SELECTOR
replicaset.apps/nginx-6bf598886b   0         0         0       31m   nginx        nginx    app=nginx,pod-template-hash=6bf598886b
replicaset.apps/nginx-cc7957db6    1         1         1       45s   nginx        nginx    app=nginx,pod-template-hash=cc7957db6

NAME                         DATA   AGE
configmap/indexmap           1      29m
configmap/kube-root-ca.crt   1      5d2h
##################################################################################################################
https://www.youtube.com/watch?v=mkksJZZmpgE&list=PL8SR-mMnL9f6KKYDnrG3saVnyLqNDz1GP&index=18

Kubernetes-Day-18
Kubernetes Probes, Introduction to HELM

Probe is health chesk , that can be configured to check the health of the container running in the pod,

https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/

a probe may return the following results:
1.success
2.failure
3.unknown

types of probes: 
 1. Liveness Probe
     this probe is used to deternmine if the particular container is running or not , if a container fails  the liveness probe , then controller will 
       try to restart the pod in the same node , it is all based on restart policy -- , the default restart policy is always 

2. Readiness probe:
    this probe is used to deternmine  wheather a particular container is ready to receive the request or not , if this fails then
     kubenretes controller will ensure the pod does't receive any request 


implementaion of the probes:
 1. command probes : in this probe the controller will get the  container to execute specific command in order to perform probe in the  container

2.HTTP REQUEST probes: in this probe , the controller will send GET-HTTP request to the given address to either port or hostname 
   to perform the probe on the container , we can set the following fields to configure the htttp probe
   * port number
   * hostname
   *http header 
   * tcp socket probe  "

apiVersion: v1
kind: Pod
metadata: 
  name: liveness-alwaysrestart
spec:  
  restartPolicy: Always        # see look
  containers:
    - name: ubuntu
      image: ubuntu
      command:
        - /bin/bash
        - -ec
        - touch /tmp/anji;sleep 30; rm /tmp/anji;sleep 600
      livenessProbe:
         exec:
           command:
             - cat
             - /tmp/anji
         initialDelaySeconds: 5
         periodSeconds: 5         "
anji@master:~$ kubectl get pod -o wide 
NAME                     READY   STATUS              RESTARTS   AGE   IP       NODE      NOMINATED NODE   READINESS GATES
liveness-alwaysrestart   0/1     ContainerCreating    " 0 "         2s    <none>   worker1   <none>           <none>

anji@master:~$ kubectl get pod -o wide 
NAME                     READY   STATUS    RESTARTS     AGE   IP          NODE      NOMINATED NODE   READINESS GATES
liveness-alwaysrestart   1/1     Running   "1 "(4s ago)   79s   10.44.0.3   worker1   <none>           <none>


anji@master:~$ kubectl get pod -o wide 
NAME                     READY   STATUS    RESTARTS     AGE     IP          NODE      NOMINATED NODE   READINESS GATES
liveness-alwaysrestart   1/1     Running   "2 "(5s ago)   2m35s   10.44.0.3   worker1   <none>           <none>

anji@master:~$ kubectl get pod -o wide 
NAME                     READY   STATUS    RESTARTS      AGE     IP          NODE      NOMINATED NODE   READINESS GATES
liveness-alwaysrestart   1/1     Running   "3 "(39s ago)   4m24s   10.44.0.3   worker1   <none>           <none>

##+++++++++++++++++++++++++++++ 
"
apiVersion: v1
kind: Pod
metadata: 
   name: liveness-norestart
spec:
  restartPolicy: Never               # see look 
  containers: 
    - name: ubuntu
      image: ubuntu
      command:
         - /bin/bash
         - -ec
         - touch /tmp/anji;sleep 30; rm /tmp/anji;sleep 600
      livenessProbe:
        exec: 
          command:
             - cat
             - /tmp/anji
        initialDelaySeconds: 5
        periodSeconds: 5
                   
anji@master:~$ kubectl get pods   -o wide
NAME                 READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES
liveness-norestart   1/1     "Running "  0        "  3s  "  10.44.0.3   worker1   <none>           <none>

anji@master:~$ kubectl get pods   -o wide
NAME                 READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES
liveness-norestart   1/1     Running   0          76s   10.44.0.3   worker1   <none>           <none>

anji@master:~$ kubectl get pods   -o wide
NAME                 READY   STATUS   RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES
liveness-norestart   0/1    "Error  "  0          "77s  " 10.44.0.3   worker1   <none>           <none>

==========++++++++++++++++++++++++++++++++++++++++++++"
apiVersion: v1
kind: Pod
metadata: 
  name: readiness-command-probe
spec: 
  containers: 
    - name: ubuntu
      image: ubuntu
      command: 
        - /bin/bash
        - -ec
        - sleep 30; touch  /tmp/anji; sleep 600
      readinessProbe:        # see look  
        exec: 
          command:
            - cat
            - /tmp/anji
        initialDelaySeconds: 10
        periodSeconds: 5        
"        
anji@master:~$ kubectl get pods -o wide 
NAME                      READY   STATUS              RESTARTS   AGE   IP       NODE      NOMINATED NODE   READINESS GATES
readiness-command-probe   0/1     ContainerCreating   0          2s    <none>   worker1   <none>           <none>

======================++++++++++++++++++++++++++++++++++++"
---
apiVersion: v1 
kind: Pod 
metadata: 
  name: liveness-request-nginx
spec: 
  containers: 
  - name: liveness 
    image: nginx
    ports: 
      - containerPort: 80 
    livenessProbe: 
      httpGet: 
        path: / 
        port: 80 
      initialDelaySeconds: 2
      periodSeconds: 2
      timeoutSeconds: 1 
      successThreshold: 1 
      failureThreshold: 3
---
apiVersion: v1 
kind: Pod 
metadata: 
  name: liveness-request-httpd
spec: 
  containers: 
  - name: liveness 
    image: httpd
    ports: 
      - containerPort: 80 
    livenessProbe: 
      httpGet: 
        path: / 
        port: 80 
      initialDelaySeconds: 2
      periodSeconds: 2
      timeoutSeconds: 1 
      successThreshold: 1 
      failureThreshold: 3

===========

---
apiVersion: v1
kind: Pod
metadata: 
   name: http-probe-111
spec: 
  containers: 
  - name: http
    image: httpd
    ports: 
      - containerPort: 80
    livenessProbe:
      httpGet: 
        path: /
        port: 80
      initialDelaySeconds: 2
      periodSeconds: 2
      timeoutSeconds: 1 
      successThreshold: 1
      failureThreshold: 3

-------"
anji@master:~$ kubectl get po -o wide 
NAME             READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES
http-probe-111   1/1     Running   0          4s    10.44.0.3   worker1   <none>           <none>
---=="
nji@master:~$ kubectl describe pod http-probe-111  
Name:             http-probe-111
Containers:
  http:
    Image:          httpd
    Restart Count:  0
    Liveness:    "   http-get http://:80/ delay=2s timeout=1s period=2s #success=1 #failure=3"
    
#########################################################################################################################
HELM CHARTS "
https://www.youtube.com/watch?v=2dqQcou_MCU&list=PL7iMyoQPMtANm_35XWjkNzDCcsw9vy01b&index=2

https://helm.sh/docs/intro/install/
apt-get update -y

root@master:~# wget https://get.helm.sh/helm-v3.10.3-linux-amd64.tar.gz

root@master:~# tar -xvzf helm-v3.10.3-linux-amd64.tar.gz 

root@master:~# mv linux-amd64/helm  /usr/local/bin/helm

========-----------------"
https://jhooq.com/getting-start-with-helm-chart/
https://www.youtube.com/watch?v=jP_PF0mqUHk&list=PL7iMyoQPMtANm_35XWjkNzDCcsw9vy01b&index=3

anji@master:~$ helm create helloworld 
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/anji/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/anji/.kube/config
Creating helloworld

anji@master:~$ ls -lrt | grep helloworld 
drwxr-xr-x 4 anji anji 4096 Jan  3 14:02 helloworld

anji@master:~/helloworld$ ll
total 28
drwxr-xr-x  4 anji anji 4096 Jan  3 14:02 ./
drwxr-xr-x 18 anji anji 4096 Jan  3 14:02 ../
drwxr-xr-x  2 anji anji 4096 Jan  3 14:02 charts/
-rw-r--r--  1 anji anji 1146 Jan  3 14:02 Chart.yaml
-rw-r--r--  1 anji anji  349 Jan  3 14:02 .helmignore
drwxr-xr-x  3 anji anji 4096 Jan  3 14:02 templates/
-rw-r--r--  1 anji anji 1877 Jan  3 14:02 values.yaml
anji@master:~/helloworld$ tree
.
â”œâ”€â”€ charts
â”œâ”€â”€ Chart.yaml
â”œâ”€â”€ templates
â”‚Â Â  â”œâ”€â”€ deployment.yaml
â”‚Â Â  â”œâ”€â”€ _helpers.tpl
â”‚Â Â  â”œâ”€â”€ hpa.yaml
â”‚Â Â  â”œâ”€â”€ ingress.yaml
â”‚Â Â  â”œâ”€â”€ NOTES.txt
â”‚Â Â  â”œâ”€â”€ serviceaccount.yaml
â”‚Â Â  â”œâ”€â”€ service.yaml
â”‚Â Â  â””â”€â”€ tests
â”‚Â Â      â””â”€â”€ test-connection.yaml
â””â”€â”€ values.yaml
3 directories, 10 files
--------+++++++++++++========"
anji@master:~/helloworld$ cat values.yaml 
# Default values for helloworld.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
replicaCount: 1
image:
  repository: nginx
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: ""
imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""
serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""
podAnnotations: {}
podSecurityContext: {}
  # fsGroup: 2000
securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000
service:
  type: ClusterIP                  ## changed to clusterip =>> nodeport see look 
  port: 80

ingress:
  enabled: false
  className: ""
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: chart-example.local
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi
autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80
nodeSelector: {}
tolerations: []
affinity: {}
==============="
anji@master:~/helloworld$ cat values.yaml 
# Default values for helloworld.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  repository: nginx
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: ""

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podAnnotations: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

service:
  type: NodePort   ### see look  service
  port: 80

ingress:
  enabled: false
  className: ""
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: chart-example.local
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

nodeSelector: {}

tolerations: []

affinity: {}
======================="
helm install <FIRST_ARGUMENT_RELEASE_NAME> <SECOND_ARGUMENT_CHART_NAME>

anji@master:~$ helm install myhworld helloworld
NAME: myhworld
LAST DEPLOYED: Tue Jan  3 14:45:05 2023
NAMESPACE: default
STATUS: "deployed"
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
  export NODE_PORT=$(kubectl get --namespace default -o jsonpath="{.spec.ports[0].nodePort}" services myhworld-helloworld)
  export NODE_IP=$(kubectl get nodes --namespace default -o jsonpath="{.items[0].status.addresses[0].address}")
  echo http://$NODE_IP:$NODE_PORT
------===="
anji@master:~$ helm list -a 
NAME    	NAMESPACE	REVISION	UPDATED                                	STATUS  	CHART           	APP VERSION
myhworld	default  	1       	2023-01-03 14:45:05.517588358 +0530 IST	"deployed"	"helloworld-0.1.0"	1.16.0     

anji@master:~$ kubectl get pod,deployments  -o wide 
NAME                                      READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES
pod/myhworld-helloworld-9f5d78d85-2w4f4   1/1     Running   0          11m   10.44.0.1   worker1   <none>           <none>

NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES         SELECTOR
deployment.apps/"myhworld-helloworld  " 1/1     1            1           11m   "helloworld "  nginx:1.16.0   app.kubernetes.io/instance=myhworld,app.kubernetes.io/name=helloworld
---==+++++++++++++"
anji@master:~$ kubectl get service  -o wide 
NAME                  TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE    SELECTOR
kubernetes            ClusterIP   10.96.0.1      <none>        443/TCP        111m   <none>
myhworld-helloworld   "NodePort "   10.104.51.12   <none>        "80:32114/TCP  " 18m    app.kubernetes.io/instance=myhworld,app.kubernetes.io/name=helloworld
"===/////\\
nji@master:~$ kubectl describe pod myhworld-helloworld-9f5d78d85-2w4f4 
Name:             myhworld-helloworld-9f5d78d85-2w4f4
Service Account:  myhworld-helloworld
Labels:           app.kubernetes.io/instance=myhworld
                  app.kubernetes.io/name=helloworld
                  pod-template-hash=9f5d78d85
Controlled By:  ReplicaSet/myhworld-helloworld-9f5d78d85
Containers:
  helloworld:
    Image:          nginx:1.16.0
  "  Liveness:  "     http-get http://:http/ delay=0s timeout=1s period=10s #success=1 #failure=3
    "Readiness:  "    http-get http://:http/ delay=0s timeout=1s period=10s #success=1 #failure=3

http://192.168.122.15:32114/
anji@master:~$ curl http://192.168.122.15:32114/
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
anji@mast
##########################################3
What are Helm Chart Repositories and how to work with it - Part 3
https://bitnami.com/
https://bitnami.com/stack/redis/helm
https://bitnami.com/stacks

anji@master:~$ helm repo list 
Error: no repositories to show

Usage:  helm repo add [NAME] [URL] [flags]
anji@master:~$ helm repo add bitnami https://charts.bitnami.com/bitnami
"bitnami" has been added to your repositories

anji@master:~$ helm repo list 
NAME   	URL                               
bitnami	https://charts.bitnami.com/bitnami

anji@master:~$ helm search repo bitnami
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/anji/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/anji/.kube/config
NAME                                        	CHART VERSION	APP VERSION  	DESCRIPTION                                       
bitnami/airflow                             	14.0.6       	2.5.0        	Apache Airflow is a tool to express and execute...
bitnami/apache                              	9.2.9        	2.4.54       	Apache HTTP Server is an open-source HTTP serve...
bitnami/appsmith                            	0.1.7        	1.8.15       	Appsmith is an open source platform for buildin...
bitnami/argo-cd                             	4.4.0        	2.5.5        	Argo CD is a continuous delivery tool for Kuber...
bitnami/argo-workflows                      	5.1.1        	3.4.4        	Argo Workflows is meant to orchestrate Kubernet...
bitnami/aspnet-core                         	4.0.1        	7.0.1        	ASP.NET Core is an open-source framework for we...
bitnami/cassandra                           	10.0.0       	4.1.0        	Apache Cassandra is an open source distributed ...
bitnami/cert-manager                        	0.8.10       	1.10.1       	cert-manager is a Kubernetes add-on to automate...
bitnami/clickhouse                          	2.2.1        	22.12.1      	ClickHouse is an open-source column-oriented OL...
bitnami/common                              	2.2.2        	2.2.2        	A Library Helm Chart for grouping common logic ...
bitnami/concourse                           	2.0.1        	7.8.3        	Concourse is an automation system written in Go...
bitnami/consul                              	10.9.8       	1.14.3       	HashiCorp Consul is a tool for discovering and ...
bitnami/contour                             	10.1.3       	1.23.2       	Contour is an open source Kubernetes ingress co...
bitnami/contour-operator                    	3.0.1        	1.23.0       	The Contour Operator extends the Kubernetes API...
bitnami/dataplatform-bp2                    	12.0.5       	1.0.1        	DEPRECATED This Helm chart can be used for the ...
bitnami/discourse                           	9.0.6        	2.8.13       	Discourse is an open source discussion platform...
bitnami/dokuwiki                            	13.1.9       	20220731.1.0 	DokuWiki is a standards-compliant wiki optimize...
bitnami/drupal                              	12.5.12      	9.4.9        	Drupal is one of the most versatile open source...
bitnami/ejbca                               	6.3.9        	7.10.0-2     	EJBCA is an enterprise class PKI Certificate Au...
bitnami/elasticsearch                       	19.5.5       	8.5.3        	Elasticsearch is a distributed search and analy...
bitnami/etcd                                	8.5.11       	3.5.6        	etcd is a distributed key-value store designed ...
bitnami/external-dns                        	6.12.2       	0.13.1       	ExternalDNS is a Kubernetes addon that configur...
bitnami/fluentd                             	5.5.12       	1.15.3       	Fluentd collects events from various data sourc...
bitnami/geode                               	1.1.8        	1.15.1       	DEPRECATED Apache Geode is a data management pl...
bitnami/ghost                               	19.1.53      	5.26.4       	Ghost is an open source publishing platform des...
bitnami/gitea                               	0.1.3        	1.18.0       	Gitea is a lightweight code hosting solution. W...
bitnami/grafana                             	8.2.21       	9.3.2        	Grafana is an open source metric analytics and ...
bitnami/grafana-loki                        	2.5.2        	2.7.1        	Grafana Loki is a horizontally scalable, highly...
bitnami/grafana-operator                    	2.7.12       	4.8.0        	Grafana Operator is a Kubernetes operator that ...
bitnami/grafana-tempo                       	1.4.6        	1.5.0        	Grafana Tempo is a distributed tracing system t...
bitnami/haproxy                             	0.6.5        	2.7.1        	HAProxy is a TCP proxy and a HTTP reverse proxy...
bitnami/haproxy-intel                       	0.2.10       	2.7.1        	HAProxy is a high-performance, open-source load...
bitnami/harbor                              	16.1.2       	2.7.0        	Harbor is an open source trusted cloud-native r...
bitnami/influxdb                            	5.4.14       	2.6.1        	InfluxDB(TM) is an open source time-series data...
bitnami/jaeger                              	0.1.2        	1.40.0       	Jaeger is a distributed tracing system. It is u...
bitnami/jasperreports                       	14.3.6       	8.1.0        	JasperReports Server is a stand-alone and embed...
bitnami/jenkins                             	11.0.10      	2.375.1      	Jenkins is an open source Continuous Integratio...
bitnami/joomla                              	13.3.12      	4.2.6        	Joomla! is an award winning open source CMS pla...
bitnami/jupyterhub                          	3.0.4        	3.0.0        	JupyterHub brings the power of notebooks to gro...
bitnami/kafka                               	20.0.2       	3.3.1        	Apache Kafka is a distributed streaming platfor...
bitnami/keycloak                            	13.0.0       	20.0.2       	Keycloak is a high performance Java-based ident...
bitnami/kiam                                	1.1.6        	4.2.0        	kiam is a proxy that captures AWS Metadata API ...
bitnami/kibana                              	10.2.10      	8.5.3        	Kibana is an open source, browser based analyti...
bitnami/kong                                	8.0.25       	3.1.1        	Kong is an open source Microservice API gateway...
bitnami/kube-prometheus                     	8.3.1        	0.61.1       	Prometheus Operator provides easy monitoring de...
bitnami/kube-state-metrics                  	3.2.7        	2.7.0        	kube-state-metrics is a simple service that lis...
bitnami/kubeapps                            	12.1.3       	2.6.2        	Kubeapps is a web-based UI for launching and ma...
bitnami/kubernetes-event-exporter           	2.1.4        	1.1.0        	Kubernetes Event Exporter makes it easy to expo...
bitnami/logstash                            	5.1.9        	8.5.3        	Logstash is an open source data processing engi...
bitnami/magento                             	21.1.11      	2.4.5-p1     	Magento is a powerful open source e-commerce pl...
bitnami/mariadb                             	11.4.2       	10.6.11      	MariaDB is an open source, community-developed ...
bitnami/mariadb-galera                      	7.4.10       	10.6.11      	MariaDB Galera is a multi-primary database clus...
bitnami/mastodon                            	0.1.2        	4.0.2        	Mastodon is self-hosted social network server b...
bitnami/matomo                              	0.2.15       	4.13.0       	Matomo, formerly known as Piwik, is a real time...
bitnami/mediawiki                           	14.3.11      	1.39.1       	MediaWiki is the free and open source wiki soft...
bitnami/memcached                           	6.3.2        	1.6.17       	Memcached is an high-performance, distributed m...
bitnami/metallb                             	4.1.13       	0.13.7       	MetalLB is a load-balancer implementation for b...
bitnami/metrics-server                      	6.2.6        	0.6.2        	Metrics Server aggregates resource usage data, ...
bitnami/minio                               	11.10.24     	2022.12.12   	MinIO(R) is an object storage server, compatibl...
bitnami/mongodb                             	13.6.2       	6.0.3        	MongoDB(R) is a relational open source NoSQL da...
bitnami/mongodb-sharded                     	6.2.1        	6.0.3        	MongoDB(R) is an open source NoSQL database tha...
bitnami/moodle                              	14.3.4       	4.1.0        	Moodle(TM) LMS is an open source online Learnin...
bitnami/mxnet                               	3.1.8        	1.9.1        	Apache MXNet (Incubating) is a flexible and eff...
bitnami/mysql                               	9.4.5        	8.0.31       	MySQL is a fast, reliable, scalable, and easy t...
bitnami/nats                                	7.5.5        	2.9.10       	NATS is an open source, lightweight and high-pe...
bitnami/nginx                               	13.2.20      	1.23.3       	NGINX Open Source is a web server that can be a...
bitnami/nginx-ingress-controller            	9.3.24       	1.6.0        	NGINX Ingress Controller is an Ingress controll...
bitnami/nginx-intel                         	2.1.13       	0.4.9        	NGINX Open Source for Intel is a lightweight se...
bitnami/node                                	19.1.7       	16.18.0      	DEPRECATED Node.js is a runtime environment bui...
bitnami/node-exporter                       	3.2.6        	1.5.0        	Prometheus exporter for hardware and OS metrics...
bitnami/oauth2-proxy                        	3.4.2        	7.4.0        	A reverse proxy and static file server that pro...
bitnami/odoo                                	23.0.4       	16.0.20221115	Odoo is an open source ERP and CRM platform, fo...
bitnami/opencart                            	13.0.6       	4.0.1-1      	OpenCart is free open source ecommerce platform...
bitnami/osclass                             	14.2.8       	8.0.2        	Osclass allows you to easily create a classifie...
bitnami/owncloud                            	12.2.10      	10.11.0      	ownCloud is an open source content collaboratio...
bitnami/parse                               	19.1.12      	5.4.0        	Parse is a platform that enables users to add a...
bitnami/phpbb                               	12.3.9       	3.3.9        	phpBB is a popular bulletin board that features...
bitnami/phpmyadmin                          	10.3.8       	5.2.0        	phpMyAdmin is a free software tool written in P...
bitnami/pinniped                            	0.4.7        	0.21.0       	Pinniped is an identity service provider for Ku...
bitnami/postgresql                          	12.1.6       	15.1.0       	PostgreSQL (Postgres) is an open source object-...
bitnami/postgresql-ha                       	10.0.7       	15.1.0       	This PostgreSQL cluster solution includes the P...
bitnami/prestashop                          	16.0.1       	8.0.0        	PrestaShop is a powerful open source eCommerce ...
bitnami/pytorch                             	2.5.11       	1.13.1       	PyTorch is a deep learning platform that accele...
bitnami/rabbitmq                            	11.3.0       	3.11.5       	RabbitMQ is an open source general-purpose mess...
bitnami/rabbitmq-cluster-operator           	3.1.5        	2.0.0        	The RabbitMQ Cluster Kubernetes Operator automa...
bitnami/redis                               	17.4.0       	7.0.7        	Redis(R) is an open source, advanced key-value ...
bitnami/redis-cluster                       	8.3.3        	7.0.7        	Redis(R) is an open source, scalable, distribut...
bitnami/redmine                             	21.0.4       	5.0.4        	Redmine is an open source management applicatio...
bitnami/schema-registry                     	8.0.2        	7.3.1        	Confluent Schema Registry provides a RESTful in...
bitnami/sealed-secrets                      	1.2.1        	0.19.3       	Sealed Secrets are "one-way" encrypted K8s Secr...
bitnami/solr                                	7.1.0        	9.1.0        	Apache Solr is an extremely powerful, open sour...
bitnami/sonarqube                           	2.0.5        	9.8.0        	SonarQube(TM) is an open source quality managem...
bitnami/spark                               	6.3.13       	3.3.1        	Apache Spark is a high-performance engine for l...
bitnami/spring-cloud-dataflow               	15.0.1       	2.9.6        	Spring Cloud Data Flow is a microservices-based...
bitnami/suitecrm                            	11.2.7       	7.12.8       	SuiteCRM is a completely open source, enterpris...
bitnami/tensorflow-resnet                   	3.6.10       	2.11.0       	TensorFlow ResNet is a client utility for use w...
bitnami/thanos                              	11.6.6       	0.29.0       	Thanos is a highly available metrics system tha...
bitnami/tomcat                              	10.5.6       	10.1.4       	Apache Tomcat is an open-source web server desi...
bitnami/wavefront                           	4.2.8        	1.13.0       	Wavefront is a high-performance streaming analy...
bitnami/wavefront-adapter-for-istio         	2.0.6        	0.1.5        	DEPRECATED Wavefront Adapter for Istio is an ad...
bitnami/wavefront-hpa-adapter               	1.3.7        	0.9.9        	Wavefront HPA Adapter for Kubernetes is a Kuber...
bitnami/wavefront-prometheus-storage-adapter	2.1.6        	1.0.5        	Wavefront Storage Adapter is a Prometheus integ...
bitnami/wildfly                             	14.0.2       	27.0.1       	Wildfly is a lightweight, open source applicati...
bitnami/wordpress                           	15.2.22      	6.1.1        	WordPress is the world's most popular blogging ...
bitnami/wordpress-intel                     	2.1.28       	6.1.1        	WordPress for Intel is the most popular bloggin...
bitnami/zookeeper                           	11.0.2       	3.8.0        	Apache ZooKeeper provides a reliable, centraliz...
----==="
anji@master:~$ helm repo update 
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "bitnami" chart repository
Update Complete. âŽˆHappy Helming!âŽˆ "

anji@master:~$ ls
Desktop  Documents  Downloads  "helloworld " Music  Pictures  Public  Templates  Videos
anji@master:~$ tree helloworld/
helloworld/
â”œâ”€â”€ charts         # see look 
â”œâ”€â”€ Chart.yaml
â”œâ”€â”€ templates
â”‚Â Â  â”œâ”€â”€ deployment.yaml
â”‚Â Â  â”œâ”€â”€ _helpers.tpl
â”‚Â Â  â”œâ”€â”€ hpa.yaml
â”‚Â Â  â”œâ”€â”€ ingress.yaml
â”‚Â Â  â”œâ”€â”€ NOTES.txt
â”‚Â Â  â”œâ”€â”€ serviceaccount.yaml
â”‚Â Â  â”œâ”€â”€ service.yaml
â”‚Â Â  â””â”€â”€ tests
â”‚Â Â      â””â”€â”€ test-connection.yaml
â””â”€â”€ values.yaml
3 directories, 10 files

anji@master:~$ helm repo index helloworld 

anji@master:~$ tree helloworld/
helloworld/
â”œâ”€â”€ charts
â”œâ”€â”€ Chart.yaml
â”œâ”€â”€ index.yaml                ## see look  index 
â”œâ”€â”€ templates
â”‚Â Â  â”œâ”€â”€ deployment.yaml
â”‚Â Â  â”œâ”€â”€ _helpers.tpl
â”‚Â Â  â”œâ”€â”€ hpa.yaml
â”‚Â Â  â”œâ”€â”€ ingress.yaml
â”‚Â Â  â”œâ”€â”€ NOTES.txt
â”‚Â Â  â”œâ”€â”€ serviceaccount.yaml
â”‚Â Â  â”œâ”€â”€ service.yaml
â”‚Â Â  â””â”€â”€ tests
â”‚Â Â      â””â”€â”€ test-connection.yaml
â””â”€â”€ values.yaml
3 directories, 11 files "

anji@master:~$ cat helloworld/index.yaml 
apiVersion: v1
entries: {}
generated: "2023-01-03T16:34:04.958380726+05:30"   "

anji@master:~$ helm repo remove bitnami 
"bitnami" has been removed from your repositories

anji@master:~$ helm repo list 
Error: no repositories to show
##############################################################################""
"What is Helm Chart ï¼‚Pluginsï¼‚ and how to use it. - Part 4 
https://jhooq.com/helm-chart-plugin/
 https://github.com/databus23/helm-diff

helm plugin install [options] <path|url>... [flags]

anji@master:~$ helm plugin list 
NAME	VERSION	DESCRIPTION

anji@master:~$ helm plugin install https://github.com/databus23/helm-diff
 Use "diff [command] --help" for more information about a command.
  Installed plugin: diff

anji@master:~$ helm plugin list 
NAME	VERSION	DESCRIPTION                           
diff	3.6.0  	Preview helm upgrade changes as a diff  "

anji@master:~$ helm create helloworld 
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/anji/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/anji/.kube/config
Creating helloworld
WARNING: File "/home/anji/helloworld/Chart.yaml" already exists. Overwriting.
WARNING: File "/home/anji/helloworld/values.yaml" already exists. Overwriting.
WARNING: File "/home/anji/helloworld/.helmignore" already exists. Overwriting.
WARNING: File "/home/anji/helloworld/templates/ingress.yaml" already exists. Overwriting.
WARNING: File "/home/anji/helloworld/templates/deployment.yaml" already exists. Overwriting.
WARNING: File "/home/anji/helloworld/templates/service.yaml" already exists. Overwriting.
WARNING: File "/home/anji/helloworld/templates/serviceaccount.yaml" already exists. Overwriting.
WARNING: File "/home/anji/helloworld/templates/hpa.yaml" already exists. Overwriting.
WARNING: File "/home/anji/helloworld/templates/NOTES.txt" already exists. Overwriting.
WARNING: File "/home/anji/helloworld/templates/_helpers.tpl" already exists. Overwriting.
WARNING: File "/home/anji/helloworld/templates/tests/test-connection.yaml" already exists. Overwriting"

anji@master:~$ helm create helloworld/
Creating helloworld/

anji@master:~$ tree helloworld/
helloworld/
â”œâ”€â”€ charts
â”œâ”€â”€ Chart.yaml
â”œâ”€â”€ templates
â”‚Â Â  â”œâ”€â”€ deployment.yaml
â”‚Â Â  â”œâ”€â”€ _helpers.tpl
â”‚Â Â  â”œâ”€â”€ hpa.yaml
â”‚Â Â  â”œâ”€â”€ ingress.yaml
â”‚Â Â  â”œâ”€â”€ NOTES.txt
â”‚Â Â  â”œâ”€â”€ serviceaccount.yaml
â”‚Â Â  â”œâ”€â”€ service.yaml
â”‚Â Â  â””â”€â”€ tests
â”‚Â Â      â””â”€â”€ test-connection.yaml
â””â”€â”€ values.yaml
3 directories, 10 files "
======-------
anji@master:~$ helm install release1 helloworld/
NAME: release1
LAST DEPLOYED: Tue Jan  3 17:26:29 2023
NAMESPACE: default
STATUS: "deployed"
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=helloworld,app.kubernetes.io/instance=release1" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT "

anji@master:~$ helm list -a 
NAME    	NAMESPACE	REVISION	UPDATED                                	STATUS  	CHART           	APP VERSION
myhworld	default  	1       	2023-01-03 14:45:05.517588358 +0530 IST	deployed	helloworld-0.1.0	1.16.0     
"release1	"default  	"1  "     	2023-01-03 17:26:29.598397743 +0530 IST	deployed	helloworld-0.1.0	1.16.0     

anji@master:~$ helm delete myhworld
release "myhworld" uninstalled

anji@master:~$ helm list -a 
NAME    	NAMESPACE	REVISION	UPDATED                                	STATUS  	CHART           	APP VERSION
"release1	"default  	"1  "     	2023-01-03 17:26:29.598397743 +0530 IST	deployed	helloworld-0.1.0	1.16.0      "


anji@master:~$ kubectl get pod,deployment   -o wide 
NAME                                       READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES
pod/release1-helloworld-7d4d79f6f9-dlk9s   1/1     Running   0          34s   10.44.0.1   worker1   <none>           <none>

NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES         SELECTOR
deployment.apps/release1-helloworld   1/1     1            1           34s   helloworld   nginx:1.16.0   app.kubernetes.io/instance=release1,app.kubernetes.io/name=helloworld

=====================\\\\\\///////////////
anji@master:~$ ls
Desktop  Documents  Downloads  helloworld  Music  Pictures  Public  Templates  Videos

anji@master:~$ cd helloworld/
anji@master:~/helloworld$ ls
charts  Chart.yaml  templates  values.yaml

anji@master:~/helloworld$ nano values.yaml 
# Default values for helloworld.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
replicaCount: 3             " ## to increase the replicas count 1 to 3 "  see look 
image:
  repository: nginx
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: "

anji@master:~/helloworld$ helm upgrade release1 . 
Release "release1" has been upgraded. Happy Helming!
NAME: release1
LAST DEPLOYED: Tue Jan  3 17:47:48 2023
NAMESPACE: default
STATUS: "deployed"
REVISION:" 2"
NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=helloworld,app.kubernetes.io/instance=release1" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT
===-////\\\

anji@master:~$ helm list -a 
NAME    	NAMESPACE	REVISION	UPDATED                               	STATUS  	CHART           	APP VERSION
release1	default  "	2    "   	2023-01-03 17:47:48.50720335 +0530 IST	deployed	helloworld-0.1.0	1.16.0     

anji@master:~$ kubectl get pods -o wide 
NAME                                   READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES
release1-helloworld-7d4d79f6f9-dlk9s   1/1     Running   0          23m     10.44.0.1   worker1   <none>           <none>
release1-helloworld-7d4d79f6f9-qkvnv   1/1     Running   0          2m21s   10.44.0.3   worker1   <none>           <none>
release1-helloworld-7d4d79f6f9-wx7pk   1/1     Running   0          2m21s   10.44.0.2   worker1   <none>           <none>

--===--//\\

anji@master:~$ helm diff revision release1  1 2
default, release1-helloworld, Deployment (apps) has changed:
  # Source: helloworld/templates/deployment.yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: release1-helloworld
    labels:
      helm.sh/chart: helloworld-0.1.0
      app.kubernetes.io/name: helloworld
      app.kubernetes.io/instance: release1
      app.kubernetes.io/version: "1.16.0"
      app.kubernetes.io/managed-by: Helm
  spec:
-   replicas: 1    ### see look 
+   replicas: 3   ###  see look 
    selector:
      matchLabels:
        app.kubernetes.io/name: helloworld
        app.kubernetes.io/instance: release1
    template:
      metadata:
        labels:
          app.kubernetes.io/name: helloworld
          app.kubernetes.io/instance: release1
      spec:
        serviceAccountName: release1-helloworld
        securityContext:
          {}
        containers:
          - name: helloworld
            securityContext:
              {}
            image: "nginx:1.16.0"
            imagePullPolicy: IfNotPresent
            ports:
              - name: http
                containerPort: 80
                protocol: TCP
            livenessProbe:
              httpGet:
                path: /
                port: http
            readinessProbe:
              httpGet:
                path: /
                port: http
            resources:
              {}
=========\\\\////////\\\/\/\//\//\\/ "
"
https://www.youtube.com/watch?v=8hyXDFrWi9w&list=PL7iMyoQPMtANm_35XWjkNzDCcsw9vy01b&index=6   
Setup Wordpress using Helm Chart on Kubernetes - Part 5

Usage:  helm repo add [NAME] [URL] [flags]
anji@master:~$ helm repo add bitnami https://charts.bitnami.com/bitnami
"bitnami" has been added to your repositories

anji@master:~$ helm repo list 
NAME   	URL                               
bitnami	https://charts.bitnami.com/bitnami

anji@master:~/helloworld$ helm search repo bitnami

anji@master:~/helloworld$ helm search repo wordpress --versions
NAME                   	CHART VERSION	APP VERSION	DESCRIPTION                                       
bitnami/wordpress      	15.2.22      	6.1.1      	WordPress is the world's most popular blogging ...
bitnami/wordpress      	15.2.21      	6.1.1      	WordPress is the world's most popular blogging ...
bitnami/wordpress      	15.2.20      	6.1.1      	WordPress is the world's most popular blogging ...
bitnami/wordpress      	15.2.19      	6.1.1      	WordPress is the world's most popular blogging ...
bitnami/wordpress      	15.2.18      	6.1.1      	WordPress is the world's most popular blogging ... "+++++++++++++{}{}{{{{{{{{{{{{{}{}}}}}}}}}}}}}
===+++/////\\\\
anji@master:~$ helm show readme bitnami/wordpress  --version 15.2.22                               {}{{[][[][{}{{{{{][{{}{{}{{{}[]}}\}}]}}}}}]]}}
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/anji/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/anji/.kube/config
<!--- app-name: WordPress -->
# WordPress packaged by Bitnami
WordPress is the world's most popular blogging and content management platform. Powerful yet simple, everyone from students to global corporations use it to build beautiful, functional websites.
[Overview of WordPress](http://www.wordpress.org)
## TL;DR
```console
$ helm repo add my-repo https://charts.bitnami.com/bitnami
$ helm install my-release my-repo/wordpress
``
## Introduction
This chart bootstraps a [WordPress](https://github.com/bitnami/containers/tree/main/bitnami/wordpress) deployment on a [Kubernetes](https://kubernetes.io) cluster using the [Helm](https://helm.sh) package manager.
It also packages the [Bitnami MariaDB chart](https://github.com/bitnami/charts/tree/main/bitnami/mariadb) which is required for bootstrapping a MariaDB deployment for the database requirements of the WordPress application, and the [Bitnami Memcached chart](https://github.com/bitnami/charts/tree/main/bitnami/memcached) that can be used to cache database queries.
Bitnami charts can be used with [Kubeapps](https://kubeapps.dev/) for deployment and management of Helm Charts in clusters.
## Prerequisites
- Kubernetes 1.19+
- Helm 3.2.0+
- PV provisioner support in the underlying infrastructure
- ReadWriteMany volumes for deployment scaling
## Installing the Chart
To install the chart with the release name `my-release`:
```console
helm install my-release my-repo/wordpress
`
The command deploys WordPress on the Kubernetes cluster in the default configuration. The [Parameters](#parameters) section lists the parameters that can be configured during installation.
> **Tip**: List all releases using `helm list`
## Uninstalling the Chart
To uninstall/delete the `my-release` deployment:
```console
helm delete my-release
The command removes all the Kubernetes components associated with the chart and deletes the release.
## Parameters
### Global parameters
| Name                      | Description                                     | Value |
| ------------------------- | ----------------------------------------------- | ----- |
| `global.imageRegistry`    | Global Docker image registry                    | `""`  |
| `global.imagePullSecrets` | Global Docker registry secret names as an array | `[]`  |
| `global.storageClass`     | Global StorageClass for Persistent Volume(s)    | `""`  |
### Common parameters
| Name                     | Description                                                                                  | Value           |
| ------------------------ | -------------------------------------------------------------------------------------------- | --------------- |
| `kubeVersion`            | Override Kubernetes version                                                                  | `""`            |
| `nameOverride`           | String to partially override common.names.fullname template (will maintain the release name) | `""`            |
| `fullnameOverride`       | String to fully override common.names.fullname template                                      | `""`            |
| `commonLabels`           | Labels to add to all deployed resources                                                      | `{}`            |
| `commonAnnotations`      | Annotations to add to all deployed resources                                                 | `{}`            |
| `clusterDomain`          | Kubernetes Cluster Domain                                                                    | `cluster.local` |
| `extraDeploy`            | Array of extra objects to deploy with the release                                            | `[]`            |
| `diagnosticMode.enabled` | Enable diagnostic mode (all probes will be disabled and the command will be overridden)      | `false`         |
| `diagnosticMode.command` | Command to override all containers in the deployment                                         | `["sleep"]`     |
| `diagnosticMode.args`    | Args to override all containers in the deployment                                            | `["infinity"]`  |
### WordPress Image parameters
| Name                | Description                                                                                               | Value                 |
| ------------------- | --------------------------------------------------------------------------------------------------------- | --------------------- |
| `image.registry`    | WordPress image registry                                                                                  | `docker.io`           |
| `image.repository`  | WordPress image repository                                                                                | `bitnami/wordpress`   |
| `image.tag`         | WordPress image tag (immutable tags are recommended)                                                      | `6.1.1-debian-11-r15` |
| `image.digest`      | WordPress image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag | `""`                  |
| `image.pullPolicy`  | WordPress image pull policy                                                                               | `IfNotPresent`        |
| `image.pullSecrets` | WordPress image pull secrets                                                                              | `[]`                  |
| `image.debug`       | Specify if debug values should be set                                                                     | `false`               |
### WordPress Configuration parameters
| Name                                   | Description                                                                           | Value              |
| -------------------------------------- | ------------------------------------------------------------------------------------- | ------------------ |
| `wordpressUsername`                    | WordPress username                                                                    | `user`             |
| `wordpressPassword`                    | WordPress user password                                                               | `""`               |
| `existingSecret`                       | Name of existing secret containing WordPress credentials                              | `""`               |
| `wordpressEmail`                       | WordPress user email                                                                  | `user@example.com` |
| `wordpressFirstName`                   | WordPress user first name                                                             | `FirstName`        |
| `wordpressLastName`                    | WordPress user last name                                                              | `LastName`         |
| `wordpressBlogName`                    | Blog name                                                                             | `User's Blog!`     |
| `wordpressTablePrefix`                 | Prefix to use for WordPress database tables                                           | `wp_`              |
| `wordpressScheme`                      | Scheme to use to generate WordPress URLs                                              | `http`             |
| `wordpressSkipInstall`                 | Skip wizard installation                                                              | `false`            |
| `wordpressExtraConfigContent`          | Add extra content to the default wp-config.php file                                   | `""`               |
| `wordpressConfiguration`               | The content for your custom wp-config.php file (advanced feature)                     | `""`               |
| `existingWordPressConfigurationSecret` | The name of an existing secret with your custom wp-config.php file (advanced feature) | `""`               |
| `wordpressConfigureCache`              | Enable W3 Total Cache plugin and configure cache settings                             | `false`            |
| `wordpressPlugins`                     | Array of plugins to install and activate. Can be specified as `all` or `none`.        | `none`             |
| `apacheConfiguration`                  | The content for your custom httpd.conf file (advanced feature)                        | `""`               |
| `existingApacheConfigurationConfigMap` | The name of an existing secret with your custom httpd.conf file (advanced feature)    | `""`               |
| `customPostInitScripts`                | Custom post-init.d user scripts                                                       | `{}`               |
| `smtpHost`                             | SMTP server host                                                                      | `""`               |
| `smtpPort`                             | SMTP server port                                                                      | `""`               |
| `smtpUser`                             | SMTP username                                                                         | `""`               |
| `smtpPassword`                         | SMTP user password                                                                    | `""`               |
| `smtpProtocol`                         | SMTP protocol                                                                         | `""`               |
| `smtpExistingSecret`                   | The name of an existing secret with SMTP credentials                                  | `""`               |
| `allowEmptyPassword`                   | Allow the container to be started with blank passwords                                | `true`             |
| `allowOverrideNone`                    | Configure Apache to prohibit overriding directives with htaccess files                | `false`            |
| `overrideDatabaseSettings`             | Allow overriding the database settings persisted in wp-config.php                     | `false`            |
| `htaccessPersistenceEnabled`           | Persist custom changes on htaccess files                                              | `false`            |
| `customHTAccessCM`                     | The name of an existing ConfigMap with custom htaccess rules                          | `""`               |
| `command`                              | Override default container command (useful when using custom images)                  | `[]`               |
| `args`                                 | Override default container args (useful when using custom images)                     | `[]`               |
| `extraEnvVars`                         | Array with extra environment variables to add to the WordPress container              | `[]`               |
| `extraEnvVarsCM`                       | Name of existing ConfigMap containing extra env vars                                  | `""`               |
| `extraEnvVarsSecret`                   | Name of existing Secret containing extra env vars                                     | `""`               |

### WordPress Multisite Configuration parameters
| Name                            | Description                                                                                                                        | Value       |
| ------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------- | ----------- |
| `multisite.enable`              | Whether to enable WordPress Multisite configuration.                                                                               | `false`     |
| `multisite.host`                | WordPress Multisite hostname/address. This value is mandatory when enabling Multisite mode.                                        | `""`        |
| `multisite.networkType`         | WordPress Multisite network type to enable. Allowed values: `subfolder`, `subdirectory` or `subdomain`.                            | `subdomain` |
| `multisite.enableNipIoRedirect` | Whether to enable IP address redirection to nip.io wildcard DNS. Useful when running on an IP address with subdomain network type. | `false`     |

### WordPress deployment parameters
| Name                                                | Description                                                                                                              | Value            |
| --------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------ | ---------------- |
| `replicaCount`                                      | Number of WordPress replicas to deploy                                                                                   | `1`              |
| `updateStrategy.type`                               | WordPress deployment strategy type                                                                                       | `RollingUpdate`  |
| `updateStrategy.rollingUpdate`                      | WordPress deployment rolling update configuration parameters                                                             | `{}`             |
| `schedulerName`                                     | Alternate scheduler                                                                                                      | `""`             |
| `topologySpreadConstraints`                         | Topology Spread Constraints for pod assignment spread across your cluster among failure-domains. Evaluated as a template | `[]`             |
| `priorityClassName`                                 | Name of the existing priority class to be used by WordPress pods, priority class needs to be created beforehand          | `""`             |
| `hostAliases`                                       | WordPress pod host aliases                                                                                               | `[]`             |
| `extraVolumes`                                      | Optionally specify extra list of additional volumes for WordPress pods                                                   | `[]`             |
| `extraVolumeMounts`                                 | Optionally specify extra list of additional volumeMounts for WordPress container(s)                                      | `[]`             |
| `sidecars`                                          | Add additional sidecar containers to the WordPress pod                                                                   | `[]`             |
| `initContainers`                                    | Add additional init containers to the WordPress pods                                                                     | `[]`             |
| `podLabels`                                         | Extra labels for WordPress pods                                                                                          | `{}`             |
| `podAnnotations`                                    | Annotations for WordPress pods                                                                                           | `{}`             |
| `podAffinityPreset`                                 | Pod affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`                                      | `""`             |
| `podAntiAffinityPreset`                             | Pod anti-affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`                                 | `soft`           |
| `nodeAffinityPreset.type`                           | Node affinity preset type. Ignored if `affinity` is set. Allowed values: `soft` or `hard`                                | `""`             |
| `nodeAffinityPreset.key`                            | Node label key to match. Ignored if `affinity` is set                                                                    | `""`             |
| `nodeAffinityPreset.values`                         | Node label values to match. Ignored if `affinity` is set                                                                 | `[]`             |
| `affinity`                                          | Affinity for pod assignment                                                                                              | `{}`             |
| `nodeSelector`                                      | Node labels for pod assignment                                                                                           | `{}`             |
| `tolerations`                                       | Tolerations for pod assignment                                                                                           | `[]`             |
| `resources.limits`                                  | The resources limits for the WordPress containers                                                                        | `{}`             |
| `resources.requests.memory`                         | The requested memory for the WordPress containers                                                                        | `512Mi`          |
| `resources.requests.cpu`                            | The requested cpu for the WordPress containers                                                                           | `300m`           |
| `containerPorts.http`                               | WordPress HTTP container port                                                                                            | `8080`           |
| `containerPorts.https`                              | WordPress HTTPS container port                                                                                           | `8443`           |
| `extraContainerPorts`                               | Optionally specify extra list of additional ports for WordPress container(s)                                             | `[]`             |
| `podSecurityContext.enabled`                        | Enabled WordPress pods' Security Context                                                                                 | `true`           |
| `podSecurityContext.fsGroup`                        | Set WordPress pod's Security Context fsGroup                                                                             | `1001`           |
| `podSecurityContext.seccompProfile.type`            | Set WordPress container's Security Context seccomp profile                                                               | `RuntimeDefault` |
| `containerSecurityContext.enabled`                  | Enabled WordPress containers' Security Context                                                                           | `true`           |
| `containerSecurityContext.runAsUser`                | Set WordPress container's Security Context runAsUser                                                                     | `1001`           |
| `containerSecurityContext.runAsNonRoot`             | Set WordPress container's Security Context runAsNonRoot                                                                  | `true`           |
| `containerSecurityContext.allowPrivilegeEscalation` | Set WordPress container's privilege escalation                                                                           | `false`          |
| `containerSecurityContext.capabilities.drop`        | Set WordPress container's Security Context runAsNonRoot                                                                  | `["ALL"]`        |
| `livenessProbe.enabled`                             | Enable livenessProbe on WordPress containers                                                                             | `true`           |
| `livenessProbe.initialDelaySeconds`                 | Initial delay seconds for livenessProbe                                                                                  | `120`            |
| `livenessProbe.periodSeconds`                       | Period seconds for livenessProbe                                                                                         | `10`             |
| `livenessProbe.timeoutSeconds`                      | Timeout seconds for livenessProbe                                                                                        | `5`              |
| `livenessProbe.failureThreshold`                    | Failure threshold for livenessProbe                                                                                      | `6`              |
| `livenessProbe.successThreshold`                    | Success threshold for livenessProbe                                                                                      | `1`              |
| `readinessProbe.enabled`                            | Enable readinessProbe on WordPress containers                                                                            | `true`           |
| `readinessProbe.initialDelaySeconds`                | Initial delay seconds for readinessProbe                                                                                 | `30`             |
| `readinessProbe.periodSeconds`                      | Period seconds for readinessProbe                                                                                        | `10`             |
| `readinessProbe.timeoutSeconds`                     | Timeout seconds for readinessProbe                                                                                       | `5`              |
| `readinessProbe.failureThreshold`                   | Failure threshold for readinessProbe                                                                                     | `6`              |
| `readinessProbe.successThreshold`                   | Success threshold for readinessProbe                                                                                     | `1`              |
| `startupProbe.enabled`                              | Enable startupProbe on WordPress containers                                                                              | `false`          |
| `startupProbe.initialDelaySeconds`                  | Initial delay seconds for startupProbe                                                                                   | `30`             |
| `startupProbe.periodSeconds`                        | Period seconds for startupProbe                                                                                          | `10`             |
| `startupProbe.timeoutSeconds`                       | Timeout seconds for startupProbe                                                                                         | `5`              |
| `startupProbe.failureThreshold`                     | Failure threshold for startupProbe                                                                                       | `6`              |
| `startupProbe.successThreshold`                     | Success threshold for startupProbe                                                                                       | `1`              |
| `customLivenessProbe`                               | Custom livenessProbe that overrides the default one                                                                      | `{}`             |
| `customReadinessProbe`                              | Custom readinessProbe that overrides the default one                                                                     | `{}`             |
| `customStartupProbe`                                | Custom startupProbe that overrides the default one                                                                       | `{}`             |
| `lifecycleHooks`                                    | for the WordPress container(s) to automate configuration before or after startup                                         | `{}`             |

### Traffic Exposure Parameters
| Name                               | Description                                                                                                                      | Value                    |
| ---------------------------------- | -------------------------------------------------------------------------------------------------------------------------------- | ------------------------ |
| `service.type`                     | WordPress service type                                                                                                           | `LoadBalancer`           |
| `service.ports.http`               | WordPress service HTTP port                                                                                                      | `80`                     |
| `service.ports.https`              | WordPress service HTTPS port                                                                                                     | `443`                    |
| `service.httpsTargetPort`          | Target port for HTTPS                                                                                                            | `https`                  |
| `service.nodePorts.http`           | Node port for HTTP                                                                                                               | `""`                     |
| `service.nodePorts.https`          | Node port for HTTPS                                                                                                              | `""`                     |
| `service.sessionAffinity`          | Control where client requests go, to the same pod or round-robin                                                                 | `None`                   |
| `service.sessionAffinityConfig`    | Additional settings for the sessionAffinity                                                                                      | `{}`                     |
| `service.clusterIP`                | WordPress service Cluster IP                                                                                                     | `""`                     |
| `service.loadBalancerIP`           | WordPress service Load Balancer IP                                                                                               | `""`                     |
| `service.loadBalancerSourceRanges` | WordPress service Load Balancer sources                                                                                          | `[]`                     |
| `service.externalTrafficPolicy`    | WordPress service external traffic policy                                                                                        | `Cluster`                |
| `service.annotations`              | Additional custom annotations for WordPress service                                                                              | `{}`                     |
| `service.extraPorts`               | Extra port to expose on WordPress service                                                                                        | `[]`                     |
| `ingress.enabled`                  | Enable ingress record generation for WordPress                                                                                   | `false`                  |
| `ingress.pathType`                 | Ingress path type                                                                                                                | `ImplementationSpecific` |
| `ingress.apiVersion`               | Force Ingress API version (automatically detected if not set)                                                                    | `""`                     |
| `ingress.ingressClassName`         | IngressClass that will be be used to implement the Ingress (Kubernetes 1.18+)                                                    | `""`                     |
| `ingress.hostname`                 | Default host for the ingress record                                                                                              | `wordpress.local`        |
| `ingress.path`                     | Default path for the ingress record                                                                                              | `/`                      |
| `ingress.annotations`              | Additional annotations for the Ingress resource. To enable certificate autogeneration, place here your cert-manager annotations. | `{}`                     |
| `ingress.tls`                      | Enable TLS configuration for the host defined at `ingress.hostname` parameter                                                    | `false`                  |
| `ingress.selfSigned`               | Create a TLS secret for this ingress record using self-signed certificates generated by Helm                                     | `false`                  |
| `ingress.extraHosts`               | An array with additional hostname(s) to be covered with the ingress record                                                       | `[]`                     |
| `ingress.extraPaths`               | An array with additional arbitrary paths that may need to be added to the ingress under the main host                            | `[]`                     |
| `ingress.extraTls`                 | TLS configuration for additional hostname(s) to be covered with this ingress record                                              | `[]`                     |
| `ingress.secrets`                  | Custom TLS certificates as secrets                                                                                               | `[]`                     |
| `ingress.extraRules`               | Additional rules to be covered with this ingress record                                                                          | `[]`                     |

### Persistence Parameters
| Name                                                   | Description                                                                                                   | Value                   |
| ------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------- | ----------------------- |
| `persistence.enabled`                                  | Enable persistence using Persistent Volume Claims                                                             | `true`                  |
| `persistence.storageClass`                             | Persistent Volume storage class                                                                               | `""`                    |
| `persistence.accessModes`                              | Persistent Volume access modes                                                                                | `[]`                    |
| `persistence.accessMode`                               | Persistent Volume access mode (DEPRECATED: use `persistence.accessModes` instead)                             | `ReadWriteOnce`         |
| `persistence.size`                                     | Persistent Volume size                                                                                        | `10Gi`                  |
| `persistence.dataSource`                               | Custom PVC data source                                                                                        | `{}`                    |
| `persistence.existingClaim`                            | The name of an existing PVC to use for persistence                                                            | `""`                    |
| `persistence.selector`                                 | Selector to match an existing Persistent Volume for WordPress data PVC                                        | `{}`                    |
| `persistence.annotations`                              | Persistent Volume Claim annotations                                                                           | `{}`                    |
| `volumePermissions.enabled`                            | Enable init container that changes the owner/group of the PV mount point to `runAsUser:fsGroup`               | `false`                 |
| `volumePermissions.image.registry`                     | Bitnami Shell image registry                                                                                  | `docker.io`             |
| `volumePermissions.image.repository`                   | Bitnami Shell image repository                                                                                | `bitnami/bitnami-shell` |
| `volumePermissions.image.tag`                          | Bitnami Shell image tag (immutable tags are recommended)                                                      | `11-debian-11-r63`      |
| `volumePermissions.image.digest`                       | Bitnami Shell image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag | `""`                    |
| `volumePermissions.image.pullPolicy`                   | Bitnami Shell image pull policy                                                                               | `IfNotPresent`          |
| `volumePermissions.image.pullSecrets`                  | Bitnami Shell image pull secrets                                                                              | `[]`                    |
| `volumePermissions.resources.limits`                   | The resources limits for the init container                                                                   | `{}`                    |
| `volumePermissions.resources.requests`                 | The requested resources for the init container                                                                | `{}`                    |
| `volumePermissions.containerSecurityContext.runAsUser` | User ID for the init container                                                                                | `0`                     |

### Other Parameters

| Name                                          | Description                                                            | Value   |
| --------------------------------------------- | ---------------------------------------------------------------------- | ------- |
| `serviceAccount.create`                       | Enable creation of ServiceAccount for WordPress pod                    | `false` |
| `serviceAccount.name`                         | The name of the ServiceAccount to use.                                 | `""`    |
| `serviceAccount.automountServiceAccountToken` | Allows auto mount of ServiceAccountToken on the serviceAccount created | `true`  |
| `serviceAccount.annotations`                  | Additional custom annotations for the ServiceAccount                   | `{}`    |
| `pdb.create`                                  | Enable a Pod Disruption Budget creation                                | `false` |
| `pdb.minAvailable`                            | Minimum number/percentage of pods that should remain scheduled         | `1`     |
| `pdb.maxUnavailable`                          | Maximum number/percentage of pods that may be made unavailable         | `""`    |
| `autoscaling.enabled`                         | Enable Horizontal POD autoscaling for WordPress                        | `false` |
| `autoscaling.minReplicas`                     | Minimum number of WordPress replicas                                   | `1`     |
| `autoscaling.maxReplicas`                     | Maximum number of WordPress replicas                                   | `11`    |
| `autoscaling.targetCPU`                       | Target CPU utilization percentage                                      | `50`    |
| `autoscaling.targetMemory`                    | Target Memory utilization percentage                                   | `50`    |

### Metrics Parameters
| Name                                         | Description                                                                                                     | Value                     |
| -------------------------------------------- | --------------------------------------------------------------------------------------------------------------- | ------------------------- |
| `metrics.enabled`                            | Start a sidecar prometheus exporter to expose metrics                                                           | `false`                   |
| `metrics.image.registry`                     | Apache exporter image registry                                                                                  | `docker.io`               |
| `metrics.image.repository`                   | Apache exporter image repository                                                                                | `bitnami/apache-exporter` |
| `metrics.image.tag`                          | Apache exporter image tag (immutable tags are recommended)                                                      | `0.11.0-debian-11-r73`    |
| `metrics.image.digest`                       | Apache exporter image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag | `""`                      |
| `metrics.image.pullPolicy`                   | Apache exporter image pull policy                                                                               | `IfNotPresent`            |
| `metrics.image.pullSecrets`                  | Apache exporter image pull secrets                                                                              | `[]`                      |
| `metrics.containerPorts.metrics`             | Prometheus exporter container port                                                                              | `9117`                    |
| `metrics.livenessProbe.enabled`              | Enable livenessProbe on Prometheus exporter containers                                                          | `true`                    |
| `metrics.livenessProbe.initialDelaySeconds`  | Initial delay seconds for livenessProbe                                                                         | `15`                      |
| `metrics.livenessProbe.periodSeconds`        | Period seconds for livenessProbe                                                                                | `10`                      |
| `metrics.livenessProbe.timeoutSeconds`       | Timeout seconds for livenessProbe                                                                               | `5`                       |
| `metrics.livenessProbe.failureThreshold`     | Failure threshold for livenessProbe                                                                             | `3`                       |
| `metrics.livenessProbe.successThreshold`     | Success threshold for livenessProbe                                                                             | `1`                       |
| `metrics.readinessProbe.enabled`             | Enable readinessProbe on Prometheus exporter containers                                                         | `true`                    |
| `metrics.readinessProbe.initialDelaySeconds` | Initial delay seconds for readinessProbe                                                                        | `5`                       |
| `metrics.readinessProbe.periodSeconds`       | Period seconds for readinessProbe                                                                               | `10`                      |
| `metrics.readinessProbe.timeoutSeconds`      | Timeout seconds for readinessProbe                                                                              | `3`                       |
| `metrics.readinessProbe.failureThreshold`    | Failure threshold for readinessProbe                                                                            | `3`                       |
| `metrics.readinessProbe.successThreshold`    | Success threshold for readinessProbe                                                                            | `1`                       |
| `metrics.startupProbe.enabled`               | Enable startupProbe on Prometheus exporter containers                                                           | `false`                   |
| `metrics.startupProbe.initialDelaySeconds`   | Initial delay seconds for startupProbe                                                                          | `10`                      |
| `metrics.startupProbe.periodSeconds`         | Period seconds for startupProbe                                                                                 | `10`                      |
| `metrics.startupProbe.timeoutSeconds`        | Timeout seconds for startupProbe                                                                                | `1`                       |
| `metrics.startupProbe.failureThreshold`      | Failure threshold for startupProbe                                                                              | `15`                      |
| `metrics.startupProbe.successThreshold`      | Success threshold for startupProbe                                                                              | `1`                       |
| `metrics.customLivenessProbe`                | Custom livenessProbe that overrides the default one                                                             | `{}`                      |
| `metrics.customReadinessProbe`               | Custom readinessProbe that overrides the default one                                                            | `{}`                      |
| `metrics.customStartupProbe`                 | Custom startupProbe that overrides the default one                                                              | `{}`                      |
| `metrics.resources.limits`                   | The resources limits for the Prometheus exporter container                                                      | `{}`                      |
| `metrics.resources.requests`                 | The requested resources for the Prometheus exporter container                                                   | `{}`                      |
| `metrics.service.ports.metrics`              | Prometheus metrics service port                                                                                 | `9150`                    |
| `metrics.service.annotations`                | Additional custom annotations for Metrics service                                                               | `{}`                      |
| `metrics.serviceMonitor.enabled`             | Create ServiceMonitor Resource for scraping metrics using Prometheus Operator                                   | `false`                   |
| `metrics.serviceMonitor.namespace`           | Namespace for the ServiceMonitor Resource (defaults to the Release Namespace)                                   | `""`                      |
| `metrics.serviceMonitor.interval`            | Interval at which metrics should be scraped.                                                                    | `""`                      |
| `metrics.serviceMonitor.scrapeTimeout`       | Timeout after which the scrape is ended                                                                         | `""`                      |
| `metrics.serviceMonitor.labels`              | Additional labels that can be used so ServiceMonitor will be discovered by Prometheus                           | `{}`                      |
| `metrics.serviceMonitor.selector`            | Prometheus instance selector labels                                                                             | `{}`                      |
| `metrics.serviceMonitor.relabelings`         | RelabelConfigs to apply to samples before scraping                                                              | `[]`                      |
| `metrics.serviceMonitor.metricRelabelings`   | MetricRelabelConfigs to apply to samples before ingestion                                                       | `[]`                      |
| `metrics.serviceMonitor.honorLabels`         | Specify honorLabels parameter to add the scrape endpoint                                                        | `false`                   |
| `metrics.serviceMonitor.jobLabel`            | The name of the label on the target service to use as the job name in prometheus.                               | `""`                      |

### NetworkPolicy parameters

| Name                                                          | Description                                                                                                                  | Value   |
| ------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------- | ------- |
| `networkPolicy.enabled`                                       | Enable network policies                                                                                                      | `false` |
| `networkPolicy.metrics.enabled`                               | Enable network policy for metrics (prometheus)                                                                               | `false` |
| `networkPolicy.metrics.namespaceSelector`                     | Monitoring namespace selector labels. These labels will be used to identify the prometheus' namespace.                       | `{}`    |
| `networkPolicy.metrics.podSelector`                           | Monitoring pod selector labels. These labels will be used to identify the Prometheus pods.                                   | `{}`    |
| `networkPolicy.ingress.enabled`                               | Enable network policy for Ingress Proxies                                                                                    | `false` |
| `networkPolicy.ingress.namespaceSelector`                     | Ingress Proxy namespace selector labels. These labels will be used to identify the Ingress Proxy's namespace.                | `{}`    |
| `networkPolicy.ingress.podSelector`                           | Ingress Proxy pods selector labels. These labels will be used to identify the Ingress Proxy pods.                            | `{}`    |
| `networkPolicy.ingressRules.backendOnlyAccessibleByFrontend`  | Enable ingress rule that makes the backend (mariadb) only accessible by testlink's pods.                                     | `false` |
| `networkPolicy.ingressRules.customBackendSelector`            | Backend selector labels. These labels will be used to identify the backend pods.                                             | `{}`    |
| `networkPolicy.ingressRules.accessOnlyFrom.enabled`           | Enable ingress rule that makes testlink only accessible from a particular origin                                             | `false` |
| `networkPolicy.ingressRules.accessOnlyFrom.namespaceSelector` | Namespace selector label that is allowed to access testlink. This label will be used to identified the allowed namespace(s). | `{}`    |
| `networkPolicy.ingressRules.accessOnlyFrom.podSelector`       | Pods selector label that is allowed to access testlink. This label will be used to identified the allowed pod(s).            | `{}`    |
| `networkPolicy.ingressRules.customRules`                      | Custom network policy ingress rule                                                                                           | `{}`    |
| `networkPolicy.egressRules.denyConnectionsToExternal`         | Enable egress rule that denies outgoing traffic outside the cluster, except for DNS (port 53).                               | `false` |
| `networkPolicy.egressRules.customRules`                       | Custom network policy rule                                                                                                   | `{}`    |

## Database Parameters
| Name                                       | Description                                                                       | Value               |
| ------------------------------------------ | --------------------------------------------------------------------------------- | ------------------- |
| `mariadb.enabled`                          | Deploy a MariaDB server to satisfy the applications database requirements         | `true`              |
| `mariadb.architecture`                     | MariaDB architecture. Allowed values: `standalone` or `replication`               | `standalone`        |
| `mariadb.auth.rootPassword`                | MariaDB root password                                                             | `""`                |
| `mariadb.auth.database`                    | MariaDB custom database                                                           | `bitnami_wordpress` |
| `mariadb.auth.username`                    | MariaDB custom user name                                                          | `bn_wordpress`      |
| `mariadb.auth.password`                    | MariaDB custom user password                                                      | `""`                |
| `mariadb.primary.persistence.enabled`      | Enable persistence on MariaDB using PVC(s)                                        | `true`              |
| `mariadb.primary.persistence.storageClass` | Persistent Volume storage class                                                   | `""`                |
| `mariadb.primary.persistence.accessModes`  | Persistent Volume access modes                                                    | `[]`                |
| `mariadb.primary.persistence.size`         | Persistent Volume size                                                            | `8Gi`               |
| `externalDatabase.host`                    | External Database server host                                                     | `localhost`         |
| `externalDatabase.port`                    | External Database server port                                                     | `3306`              |
| `externalDatabase.user`                    | External Database username                                                        | `bn_wordpress`      |
| `externalDatabase.password`                | External Database user password                                                   | `""`                |
| `externalDatabase.database`                | External Database database name                                                   | `bitnami_wordpress` |
| `externalDatabase.existingSecret`          | The name of an existing secret with database credentials. Evaluated as a template | `""`                |
| `memcached.enabled`                        | Deploy a Memcached server for caching database queries                            | `false`             |
| `memcached.auth.enabled`                   | Enable Memcached authentication                                                   | `false`             |
| `memcached.auth.username`                  | Memcached admin user                                                              | `""`                |
| `memcached.auth.password`                  | Memcached admin password                                                          | `""`                |
| `memcached.service.port`                   | Memcached service port                                                            | `11211`             |
| `externalCache.host`                       | External cache server host                                                        | `localhost`         |
| `externalCache.port`                       | External cache server port                                                        | `11211`             |

The above parameters map to the env variables defined in [bitnami/wordpress](https://github.com/bitnami/containers/tree/main/bitnami/wordpress). For more information please refer to the [bitnami/wordpress](https://github.com/bitnami/containers/tree/main/bitnami/wordpress) image documentation.

Specify each parameter using the `--set key=value[,key=value]` argument to `helm install`. For example,

```console
helm install my-release \
  --set wordpressUsername=admin \
  --set wordpressPassword=password \
  --set mariadb.auth.rootPassword=secretpassword \
    my-repo/wordpress
``
The above command sets the WordPress administrator account username and password to `admin` and `password` respectively. Additionally, it sets the MariaDB `root` user password to `secretpassword`.

> NOTE: Once this chart is deployed, it is not possible to change the application's access credentials, such as usernames or passwords, using Helm. To change these application credentials after deployment, delete any persistent volumes (PVs) used by the chart and re-deploy it, or use the application's built-in administrative tools if available.

Alternatively, a YAML file that specifies the values for the above parameters can be provided while installing the chart. For example,

```console
helm install my-release -f values.yaml my-repo/wordpress
```
> **Tip**: You can use the default [values.yaml](values.yaml)

## Configuration and installation details

### [Rolling VS Immutable tags](https://docs.bitnami.com/containers/how-to/understand-rolling-tags-containers/)

It is strongly recommended to use immutable tags in a production environment. This ensures your deployment does not change automatically if the same tag is updated with a different image.

Bitnami will release a new chart updating its containers if a new version of the main container, significant changes, or critical vulnerabilities exist.

### Known limitations

When performing admin operations that require activating the maintenance mode (such as updating a plugin or theme), it's activated in only one replica (see: [bug report](https://core.trac.wordpress.org/ticket/50797)). This implies that WP could be attending requests on other replicas while performing admin operations, with unpredictable consequences.

To avoid that, you can manually activate/deactivate the maintenance mode on every replica using the WP CLI. For instance, if you installed WP with three replicas, you can run the commands below to activate the maintenance mode in all of them (assuming that the release name is `wordpress`):

```console
kubectl exec $(kubectl get pods -l app.kubernetes.io/name=wordpress -o jsonpath='{.items[0].metadata.name}') -c wordpress -- wp maintenance-mode activate
kubectl exec $(kubectl get pods -l app.kubernetes.io/name=wordpress -o jsonpath='{.items[1].metadata.name}') -c wordpress -- wp maintenance-mode activate
kubectl exec $(kubectl get pods -l app.kubernetes.io/name=wordpress -o jsonpath='{.items[2].metadata.name}') -c wordpress -- wp maintenance-mode activate
``
### External database support

You may want to have WordPress connect to an external database rather than installing one inside your cluster. Typical reasons for this are to use a managed database service, or to share a common database server for all your applications. To achieve this, the chart allows you to specify credentials for an external database with the [`externalDatabase` parameter](#database-parameters). You should also disable the MariaDB installation with the `mariadb.enabled` option. Here is an example:

```console
mariadb.enabled=false
externalDatabase.host=myexternalhost
externalDatabase.user=myuser
externalDatabase.password=mypassword
externalDatabase.database=mydatabase
externalDatabase.port=3306
```
Refer to the [documentation on using an external database with WordPress](https://docs.bitnami.com/kubernetes/apps/wordpress/configuration/use-external-database/) and the [tutorial on integrating WordPress with a managed cloud database](https://docs.bitnami.com/tutorials/secure-wordpress-kubernetes-managed-database-ssl-upgrades/) for more information.
### Memcached
This chart provides support for using Memcached to cache database queries and objects improving the website performance. To enable this feature, set `wordpressConfigureCache` and `memcached.enabled` parameters to `true`.
When this feature is enabled, a Memcached server will be deployed in your K8s cluster using the Bitnami Memcached chart and the [W3 Total Cache](https://wordpress.org/plugins/w3-total-cache/) plugin will be activated and configured to use the Memcached server for database caching.
It is also possible to use an external cache server rather than installing one inside your cluster. To achieve this, the chart allows you to specify credentials for an external cache server with the [`externalCache` parameter](#database-parameters). You should also disable the Memcached installation with the `memcached.enabled` option. Here is an example:
```console
wordpressConfigureCache=true
memcached.enabled=false
externalCache.host=myexternalcachehost
externalCache.port=11211
```
### Ingress
This chart provides support for Ingress resources. If an Ingress controller, such as nginx-ingress or traefik, that Ingress controller can be used to serve WordPress.
To enable Ingress integration, set `ingress.enabled` to `true`. The `ingress.hostname` property can be used to set the host name. The `ingress.tls` parameter can be used to add the TLS configuration for this host. It is also possible to have more than one host, with a separate TLS configuration for each host. [Learn more about configuring and using Ingress](https://docs.bitnami.com/kubernetes/apps/wordpress/configuration/configure-ingress/).
### TLS secrets

The chart also facilitates the creation of TLS secrets for use with the Ingress controller, with different options for certificate management. [Learn more about TLS secrets](https://docs.bitnami.com/kubernetes/apps/wordpress/administration/enable-tls-ingress/).

### `.htaccess` files

For performance and security reasons, it is a good practice to configure Apache with the `AllowOverride None` directive. Instead of using `.htaccess` files, Apache will load the same directives at boot time. These directives are located in `/opt/bitnami/wordpress/wordpress-htaccess.conf`.

By default, the container image includes all the default `.htaccess` files in WordPress (together with the default plugins). To enable this feature, install the chart with the value `allowOverrideNone=yes`.

[Learn more about working with `.htaccess` files](https://docs.bitnami.com/kubernetes/apps/wordpress/configuration/understand-htaccess/).

## Persistence

The [Bitnami WordPress](https://github.com/bitnami/containers/tree/main/bitnami/wordpress) image stores the WordPress data and configurations at the `/bitnami` path of the container. Persistent Volume Claims are used to keep the data across deployments.

If you encounter errors when working with persistent volumes, refer to our [troubleshooting guide for persistent volumes](https://docs.bitnami.com/kubernetes/faq/troubleshooting/troubleshooting-persistence-volumes/).

### Additional environment variables

In case you want to add extra environment variables (useful for advanced operations like custom init scripts), you can use the `extraEnvVars` property.

```yaml
wordpress:
  extraEnvVars:
    - name: LOG_LEVEL
      value: error
```

Alternatively, you can use a ConfigMap or a Secret with the environment variables. To do so, use the `extraEnvVarsCM` or the `extraEnvVarsSecret` values.

### Sidecars

If additional containers are needed in the same pod as WordPress (such as additional metrics or logging exporters), they can be defined using the `sidecars` parameter. If these sidecars export extra ports, extra port definitions can be added using the `service.extraPorts` parameter. [Learn more about configuring and using sidecar containers](https://docs.bitnami.com/kubernetes/apps/wordpress/configuration/configure-sidecar-init-containers/).

### Pod affinity

This chart allows you to set your custom affinity using the `affinity` parameter. Learn more about Pod affinity in the [kubernetes documentation](https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity).

As an alternative, use one of the preset configurations for pod affinity, pod anti-affinity, and node affinity available at the [bitnami/common](https://github.com/bitnami/charts/tree/main/bitnami/common#affinities) chart. To do so, set the `podAffinityPreset`, `podAntiAffinityPreset`, or `nodeAffinityPreset` parameters.

## Troubleshooting

Find more information about how to deal with common errors related to Bitnami's Helm charts in [this troubleshooting guide](https://docs.bitnami.com/general/how-to/troubleshoot-helm-chart-issues).

## Notable changes

### 13.2.0

Removed support for limiting auto-updates to WordPress core via the `wordpressAutoUpdateLevel` option. To update WordPress core, we recommend you use the `helm upgrade` command to update your deployment instead of using the built-in update functionality.

### 11.0.0

The [Bitnami WordPress](https://github.com/bitnami/containers/tree/main/bitnami/wordpress) image was refactored and now the source code is published in GitHub in the `rootfs` folder of the container image.

In addition, several new features have been implemented:

- Multisite mode is now supported via `multisite.*` options.
- Plugins can be installed and activated on the first deployment via the `wordpressPlugins` option.
- Added support for limiting auto-updates to WordPress core via the `wordpressAutoUpdateLevel` option. In addition, auto-updates have been disabled by default. To update WordPress core, we recommend to swap the container image version for your deployment instead of using the built-in update functionality.

To enable the new features, it is not possible to do it by upgrading an existing deployment. Instead, it is necessary to perform a fresh deploy.

## Upgrading

### To 14.0.0

This major release bumps the MariaDB version to 10.6. Follow the [upstream instructions](https://mariadb.com/kb/en/upgrading-from-mariadb-105-to-mariadb-106/) for upgrading from MariaDB 10.5 to 10.6. No major issues are expected during the upgrade.

### To 13.0.0

This major release renames several values in this chart and adds missing features, in order to be inline with the rest of assets in the Bitnami charts repository.

- `service.port` and `service.httpsPort` have been regrouped under the `service.ports` map.
- `metrics.service.port` has been regrouped under the `metrics.service.ports` map.
- `serviceAccountName` has been deprecated in favor of `serviceAccount` map.

Additionally updates the MariaDB & Memcached subcharts to their newest major `10.x.x` and `6.x.x`, respectively, which contain similar changes.

### To 12.0.0

WordPress version was bumped to its latest major, `5.8.x`. Though no incompatibilities are expected while upgrading from previous versions, WordPress recommends backing up your application first.

Site backups can be easily performed using tools such as [VaultPress](https://vaultpress.com/) or [All-in-One WP Migration](https://wordpress.org/plugins/all-in-one-wp-migration/).

### To 11.0.0

The [Bitnami WordPress](https://github.com/bitnami/containers/tree/main/bitnami/wordpress) image was refactored and now the source code is published in GitHub in the `rootfs` folder of the container image.

Compatibility is not guaranteed due to the amount of involved changes, however no breaking changes are expected.

### To 10.0.0

[On November 13, 2020, Helm v2 support was formally finished](https://github.com/helm/charts#status-of-the-project), this major version is the result of the required changes applied to the Helm Chart to be able to incorporate the different features added in Helm v3 and to be consistent with the Helm project itself regarding the Helm v2 EOL.

[Learn more about this change and related upgrade considerations](https://docs.bitnami.com/kubernetes/apps/wordpress/administration/upgrade-helm3/).

#### Additional upgrade notes

- MariaDB dependency version was bumped to a new major version that introduces several incompatibilities. Therefore, backwards compatibility is not guaranteed unless an external database is used. Check [MariaDB Upgrading Notes](https://github.com/bitnami/charts/tree/main/bitnami/mariadb#to-800) for more information.
- If you want to upgrade to this version from a previous one installed with Helm v3, there are two alternatives:
  - Install a new WordPress chart, and migrate your WordPress site using backup/restore tools such as [VaultPress](https://vaultpress.com/) or [All-in-One WP Migration](https://wordpress.org/plugins/all-in-one-wp-migration/).
  - Reuse the PVC used to hold the MariaDB data on your previous release. To do so, follow the instructions below (the following example assumes that the release name is `wordpress`).

> Warning: please create a backup of your database before running any of these actions. The steps below would be only valid if your application (e.g. any plugins or custom code) is compatible with MariaDB 10.5.

Obtain the credentials and the name of the PVC used to hold the MariaDB data on your current release:

```console
$ export WORDPRESS_PASSWORD=$(kubectl get secret --namespace default wordpress -o jsonpath="{.data.wordpress-password}" | base64 -d)
$ export MARIADB_ROOT_PASSWORD=$(kubectl get secret --namespace default wordpress-mariadb -o jsonpath="{.data.mariadb-root-password}" | base64 -d)
$ export MARIADB_PASSWORD=$(kubectl get secret --namespace default wordpress-mariadb -o jsonpath="{.data.mariadb-password}" | base64 -d)
$ export MARIADB_PVC=$(kubectl get pvc -l app.kubernetes.io/instance=wordpress,app.kubernetes.io/name=mariadb,app.kubernetes.io/component=primary -o jsonpath="{.items[0].metadata.name}")
```

Upgrade your release (maintaining the version) disabling MariaDB and scaling WordPress replicas to 0:

```console
$ helm upgrade wordpress my-repo/wordpress --set wordpressPassword=$WORDPRESS_PASSWORD --set replicaCount=0 --set mariadb.enabled=false --version 9.6.4
```

Finally, upgrade you release to `10.0.0` reusing the existing PVC, and enabling back MariaDB:

```console
$ helm upgrade wordpress my-repo/wordpress --set mariadb.primary.persistence.existingClaim=$MARIADB_PVC --set mariadb.auth.rootPassword=$MARIADB_ROOT_PASSWORD --set mariadb.auth.password=$MARIADB_PASSWORD --set wordpressPassword=$WORDPRESS_PASSWORD
```

You should see the lines below in MariaDB container logs:

```console
$ kubectl logs $(kubectl get pods -l app.kubernetes.io/instance=wordpress,app.kubernetes.io/name=mariadb,app.kubernetes.io/component=primary -o jsonpath="{.items[0].metadata.name}")
...
mariadb 12:13:24.98 INFO  ==> Using persisted data
mariadb 12:13:25.01 INFO  ==> Running mysql_upgrade
...
```

### To 9.0.0

The [Bitnami WordPress](https://github.com/bitnami/containers/tree/main/bitnami/wordpress) image was migrated to a "non-root" user approach. Previously the container ran as the `root` user and the Apache daemon was started as the `daemon` user. From now on, both the container and the Apache daemon run as user `1001`. You can revert this behavior by setting the parameters `securityContext.runAsUser`, and `securityContext.fsGroup` to `0`.
Chart labels and Ingress configuration were also adapted to follow the Helm charts best practices.

Consequences:

- The HTTP/HTTPS ports exposed by the container are now `8080/8443` instead of `80/443`.
- No writing permissions will be granted on `wp-config.php` by default.
- Backwards compatibility is not guaranteed.

To upgrade to `9.0.0`, it's recommended to install a new WordPress chart, and migrate your WordPress site using backup/restore tools such as [VaultPress](https://vaultpress.com/) or [All-in-One WP Migration](https://wordpress.org/plugins/all-in-one-wp-migration/).

### To 8.0.0

Helm performs a lookup for the object based on its group (apps), version (v1), and kind (Deployment). Also known as its GroupVersionKind, or GVK. Changing the GVK is considered a compatibility breaker from Kubernetes' point of view, so you cannot "upgrade" those objects to the new GVK in-place. Earlier versions of Helm 3 did not perform the lookup correctly which has since been fixed to match the spec.

In https://github.com/helm/charts/pulls/12642 the `apiVersion` of the deployment resources was updated to `apps/v1` in tune with the API's deprecated, resulting in compatibility breakage.

This major version signifies this change.

### To 3.0.0

Backwards compatibility is not guaranteed unless you modify the labels used on the chart's deployments.
Use the workaround below to upgrade from versions previous to `3.0.0`. The following example assumes that the release name is `wordpress`:

```console
kubectl patch deployment wordpress-wordpress --type=json -p='[{"op": "remove", "path": "/spec/selector/matchLabels/chart"}]'
kubectl delete statefulset wordpress-mariadb --cascade=false
```

## License

Copyright &copy; 2022 Bitnami

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License  
##########################################################################################################################################################
"
anji@master:~$  helm show values bitnami/wordpress --version  15.2.22 
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/anji/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/anji/.kube/config
## @section Global parameters
## Global Docker image parameters
## Please, note that this will override the image parameters, including dependencies, configured to use the global value
## Current available global Docker image parameters: imageRegistry, imagePullSecrets and storageClass
##

## @param global.imageRegistry Global Docker image registry
## @param global.imagePullSecrets Global Docker registry secret names as an array
## @param global.storageClass Global StorageClass for Persistent Volume(s)
##
global:
  imageRegistry: ""
  ## E.g.
  ## imagePullSecrets:
  ##   - myRegistryKeySecretName
  ##
  imagePullSecrets: []
  storageClass: ""

## @section Common parameters
##

## @param kubeVersion Override Kubernetes version
##
kubeVersion: ""
## @param nameOverride String to partially override common.names.fullname template (will maintain the release name)
##
nameOverride: ""
## @param fullnameOverride String to fully override common.names.fullname template
##
fullnameOverride: ""
## @param commonLabels Labels to add to all deployed resources
##
commonLabels: {}
## @param commonAnnotations Annotations to add to all deployed resources
##
commonAnnotations: {}
## @param clusterDomain Kubernetes Cluster Domain
##
clusterDomain: cluster.local
## @param extraDeploy Array of extra objects to deploy with the release
##
extraDeploy: []

## Enable diagnostic mode in the deployment
##
diagnosticMode:
  ## @param diagnosticMode.enabled Enable diagnostic mode (all probes will be disabled and the command will be overridden)
  ##
  enabled: false
  ## @param diagnosticMode.command Command to override all containers in the deployment
  ##
  command:
    - sleep
  ## @param diagnosticMode.args Args to override all containers in the deployment
  ##
  args:
    - infinity

## @section WordPress Image parameters
##

## Bitnami WordPress image
## ref: https://hub.docker.com/r/bitnami/wordpress/tags/
## @param image.registry WordPress image registry
## @param image.repository WordPress image repository
## @param image.tag WordPress image tag (immutable tags are recommended)
## @param image.digest WordPress image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag
## @param image.pullPolicy WordPress image pull policy
## @param image.pullSecrets WordPress image pull secrets
## @param image.debug Specify if debug values should be set
##
image:
  registry: docker.io
  repository: bitnami/wordpress
  tag: 6.1.1-debian-11-r15
  digest: ""
  ## Specify a imagePullPolicy
  ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'
  ## ref: https://kubernetes.io/docs/user-guide/images/#pre-pulling-images
  ##
  pullPolicy: IfNotPresent
  ## Optionally specify an array of imagePullSecrets.
  ## Secrets must be manually created in the namespace.
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
  ## e.g:
  ## pullSecrets:
  ##   - myRegistryKeySecretName
  ##
  pullSecrets: []
  ## Enable debug mode
  ##
  debug: false

## @section WordPress Configuration parameters
## WordPress settings based on environment variables
## ref: https://github.com/bitnami/containers/tree/main/bitnami/wordpress#environment-variables
##

## @param wordpressUsername WordPress username
##
wordpressUsername: user
## @param wordpressPassword WordPress user password
## Defaults to a random 10-character alphanumeric string if not set
##
wordpressPassword: ""
## @param existingSecret Name of existing secret containing WordPress credentials
## NOTE: Must contain key `wordpress-password`
## NOTE: When it's set, the `wordpressPassword` parameter is ignored
##
existingSecret: ""
## @param wordpressEmail WordPress user email
##
wordpressEmail: user@example.com
## @param wordpressFirstName WordPress user first name
##
wordpressFirstName: FirstName
## @param wordpressLastName WordPress user last name
##
wordpressLastName: LastName
## @param wordpressBlogName Blog name
##
wordpressBlogName: User's Blog!
## @param wordpressTablePrefix Prefix to use for WordPress database tables
##
wordpressTablePrefix: wp_
## @param wordpressScheme Scheme to use to generate WordPress URLs
##
wordpressScheme: http
## @param wordpressSkipInstall Skip wizard installation
## NOTE: useful if you use an external database that already contains WordPress data
## ref: https://github.com/bitnami/containers/tree/main/bitnami/wordpress#connect-wordpress-docker-container-to-an-existing-database
##
wordpressSkipInstall: false
## @param wordpressExtraConfigContent Add extra content to the default wp-config.php file
## e.g:
## wordpressExtraConfigContent: |
##   @ini_set( 'post_max_size', '128M');
##   @ini_set( 'memory_limit', '256M' );
##
wordpressExtraConfigContent: ""
## @param wordpressConfiguration The content for your custom wp-config.php file (advanced feature)
## NOTE: This will override configuring WordPress based on environment variables (including those set by the chart)
## NOTE: Currently only supported when `wordpressSkipInstall=true`
##
wordpressConfiguration: ""
## @param existingWordPressConfigurationSecret The name of an existing secret with your custom wp-config.php file (advanced feature)
## NOTE: When it's set the `wordpressConfiguration` parameter is ignored
##
existingWordPressConfigurationSecret: ""
## @param wordpressConfigureCache Enable W3 Total Cache plugin and configure cache settings
## NOTE: useful if you deploy Memcached for caching database queries or you use an external cache server
##
wordpressConfigureCache: false
## @param wordpressPlugins Array of plugins to install and activate. Can be specified as `all` or `none`.
## NOTE: If set to all, only plugins that are already installed will be activated, and if set to none, no plugins will be activated
##
wordpressPlugins: none
## @param apacheConfiguration The content for your custom httpd.conf file (advanced feature)
##
apacheConfiguration: ""
## @param existingApacheConfigurationConfigMap The name of an existing secret with your custom httpd.conf file (advanced feature)
## NOTE: When it's set the `apacheConfiguration` parameter is ignored
##
existingApacheConfigurationConfigMap: ""
## @param customPostInitScripts Custom post-init.d user scripts
## ref: https://github.com/bitnami/containers/tree/main/bitnami/wordpress
## NOTE: supported formats are `.sh`, `.sql` or `.php`
## NOTE: scripts are exclusively executed during the 1st boot of the container
## e.g:
## customPostInitScripts:
##   enable-multisite.sh: |
##     #!/bin/bash
##     chmod +w /bitnami/wordpress/wp-config.php
##     wp core multisite-install --url=example.com --title="Welcome to the WordPress Multisite" --admin_user="doesntmatternotreallyused" --admin_password="doesntmatternotreallyused" --admin_email="user@example.com"
##     cat /docker-entrypoint-init.d/.htaccess > /bitnami/wordpress/.htaccess
##     chmod -w bitnami/wordpress/wp-config.php
##   .htaccess: |
##     RewriteEngine On
##     RewriteBase /
##     ...
##
customPostInitScripts: {}
## SMTP mail delivery configuration
## ref: https://github.com/bitnami/containers/tree/main/bitnami/wordpress/#smtp-configuration
## @param smtpHost SMTP server host
## @param smtpPort SMTP server port
## @param smtpUser SMTP username
## @param smtpPassword SMTP user password
## @param smtpProtocol SMTP protocol
##
smtpHost: ""
smtpPort: ""
smtpUser: ""
smtpPassword: ""
smtpProtocol: ""
## @param smtpExistingSecret The name of an existing secret with SMTP credentials
## NOTE: Must contain key `smtp-password`
## NOTE: When it's set, the `smtpPassword` parameter is ignored
##
smtpExistingSecret: ""
## @param allowEmptyPassword Allow the container to be started with blank passwords
##
allowEmptyPassword: true
## @param allowOverrideNone Configure Apache to prohibit overriding directives with htaccess files
##
allowOverrideNone: false
## @param overrideDatabaseSettings Allow overriding the database settings persisted in wp-config.php
##
overrideDatabaseSettings: false
## @param htaccessPersistenceEnabled Persist custom changes on htaccess files
## If `allowOverrideNone` is `false`, it will persist `/opt/bitnami/wordpress/wordpress-htaccess.conf`
## If `allowOverrideNone` is `true`, it will persist `/opt/bitnami/wordpress/.htaccess`
##
htaccessPersistenceEnabled: false
## @param customHTAccessCM The name of an existing ConfigMap with custom htaccess rules
## NOTE: Must contain key `wordpress-htaccess.conf` with the file content
## NOTE: Requires setting `allowOverrideNone=false`
##
customHTAccessCM: ""
## @param command Override default container command (useful when using custom images)
##
command: []
## @param args Override default container args (useful when using custom images)
##
args: []
## @param extraEnvVars Array with extra environment variables to add to the WordPress container
## e.g:
## extraEnvVars:
##   - name: FOO
##     value: "bar"
##
extraEnvVars: []
## @param extraEnvVarsCM Name of existing ConfigMap containing extra env vars
##
extraEnvVarsCM: ""
## @param extraEnvVarsSecret Name of existing Secret containing extra env vars
##
extraEnvVarsSecret: ""

## @section WordPress Multisite Configuration parameters
## ref: https://github.com/bitnami/containers/tree/main/bitnami/wordpress#multisite-configuration
##

## @param multisite.enable Whether to enable WordPress Multisite configuration.
## @param multisite.host WordPress Multisite hostname/address. This value is mandatory when enabling Multisite mode.
## @param multisite.networkType WordPress Multisite network type to enable. Allowed values: `subfolder`, `subdirectory` or `subdomain`.
## @param multisite.enableNipIoRedirect Whether to enable IP address redirection to nip.io wildcard DNS. Useful when running on an IP address with subdomain network type.
##
multisite:
  enable: false
  host: ""
  networkType: subdomain
  enableNipIoRedirect: false

## @section WordPress deployment parameters
##

## @param replicaCount Number of WordPress replicas to deploy
## NOTE: ReadWriteMany PVC(s) are required if replicaCount > 1
##
replicaCount: 1
## @param updateStrategy.type WordPress deployment strategy type
## @param updateStrategy.rollingUpdate WordPress deployment rolling update configuration parameters
## ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy
## NOTE: Set it to `Recreate` if you use a PV that cannot be mounted on multiple pods
## e.g:
## updateStrategy:
##  type: RollingUpdate
##  rollingUpdate:
##    maxSurge: 25%
##    maxUnavailable: 25%
##
updateStrategy:
  type: RollingUpdate
  rollingUpdate: {}
## @param schedulerName Alternate scheduler
## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
##
schedulerName: ""
## @param topologySpreadConstraints Topology Spread Constraints for pod assignment spread across your cluster among failure-domains. Evaluated as a template
## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods
##
topologySpreadConstraints: []
## @param priorityClassName Name of the existing priority class to be used by WordPress pods, priority class needs to be created beforehand
## Ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/
##
priorityClassName: ""
## @param hostAliases [array] WordPress pod host aliases
## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
##
hostAliases:
  ## Required for Apache exporter to work
  ##
  - ip: "127.0.0.1"
    hostnames:
      - "status.localhost"
## @param extraVolumes Optionally specify extra list of additional volumes for WordPress pods
##
extraVolumes: []
## @param extraVolumeMounts Optionally specify extra list of additional volumeMounts for WordPress container(s)
##
extraVolumeMounts: []
## @param sidecars Add additional sidecar containers to the WordPress pod
## e.g:
## sidecars:
##   - name: your-image-name
##     image: your-image
##     imagePullPolicy: Always
##     ports:
##       - name: portname
##         containerPort: 1234
##
sidecars: []
## @param initContainers Add additional init containers to the WordPress pods
## ref: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
## e.g:
## initContainers:
##  - name: your-image-name
##    image: your-image
##    imagePullPolicy: Always
##    command: ['sh', '-c', 'copy themes and plugins from git and push to /bitnami/wordpress/wp-content. Should work with extraVolumeMounts and extraVolumes']
##
initContainers: []
## @param podLabels Extra labels for WordPress pods
## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
##
podLabels: {}
## @param podAnnotations Annotations for WordPress pods
## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
##
podAnnotations: {}
## @param podAffinityPreset Pod affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
##
podAffinityPreset: ""
## @param podAntiAffinityPreset Pod anti-affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
##
podAntiAffinityPreset: soft
## Node affinity preset
## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
##
nodeAffinityPreset:
  ## @param nodeAffinityPreset.type Node affinity preset type. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
  ##
  type: ""
  ## @param nodeAffinityPreset.key Node label key to match. Ignored if `affinity` is set
  ##
  key: ""
  ## @param nodeAffinityPreset.values Node label values to match. Ignored if `affinity` is set
  ## E.g.
  ## values:
  ##   - e2e-az1
  ##   - e2e-az2
  ##
  values: []
## @param affinity Affinity for pod assignment
## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
## NOTE: podAffinityPreset, podAntiAffinityPreset, and nodeAffinityPreset will be ignored when it's set
##
affinity: {}
## @param nodeSelector Node labels for pod assignment
## ref: https://kubernetes.io/docs/user-guide/node-selection/
##
nodeSelector: {}
## @param tolerations Tolerations for pod assignment
## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
##
tolerations: []
## WordPress containers' resource requests and limits
## ref: https://kubernetes.io/docs/user-guide/compute-resources/
## @param resources.limits The resources limits for the WordPress containers
## @param resources.requests.memory The requested memory for the WordPress containers
## @param resources.requests.cpu The requested cpu for the WordPress containers
##
resources:
  limits: {}
  requests:
    memory: 512Mi
    cpu: 300m
## Container ports
## @param containerPorts.http WordPress HTTP container port
## @param containerPorts.https WordPress HTTPS container port
##
containerPorts:
  http: 8080
  https: 8443
## @param extraContainerPorts Optionally specify extra list of additional ports for WordPress container(s)
## e.g:
## extraContainerPorts:
##   - name: myservice
##     containerPort: 9090
##
extraContainerPorts: []
## Configure Pods Security Context
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
## @param podSecurityContext.enabled Enabled WordPress pods' Security Context
## @param podSecurityContext.fsGroup Set WordPress pod's Security Context fsGroup
## @param podSecurityContext.seccompProfile.type Set WordPress container's Security Context seccomp profile
##
podSecurityContext:
  enabled: true
  fsGroup: 1001
  seccompProfile:
    type: "RuntimeDefault"
## Configure Container Security Context (only main container)
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
## @param containerSecurityContext.enabled Enabled WordPress containers' Security Context
## @param containerSecurityContext.runAsUser Set WordPress container's Security Context runAsUser
## @param containerSecurityContext.runAsNonRoot Set WordPress container's Security Context runAsNonRoot
## @param containerSecurityContext.allowPrivilegeEscalation Set WordPress container's privilege escalation
## @param containerSecurityContext.capabilities.drop Set WordPress container's Security Context runAsNonRoot
##
containerSecurityContext:
  enabled: true
  runAsUser: 1001
  runAsNonRoot: true
  allowPrivilegeEscalation: false
  capabilities:
    drop: ["ALL"]
## Configure extra options for WordPress containers' liveness, readiness and startup probes
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes
## @param livenessProbe.enabled Enable livenessProbe on WordPress containers
## @skip livenessProbe.httpGet
## @param livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
## @param livenessProbe.periodSeconds Period seconds for livenessProbe
## @param livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
## @param livenessProbe.failureThreshold Failure threshold for livenessProbe
## @param livenessProbe.successThreshold Success threshold for livenessProbe
##
livenessProbe:
  enabled: true
  httpGet:
    path: /wp-admin/install.php
    port: '{{ .Values.wordpressScheme }}'
    scheme: '{{ .Values.wordpressScheme | upper }}'
    ## If using an HTTPS-terminating load-balancer, the probes may need to behave
    ## like the balancer to prevent HTTP 302 responses. According to the Kubernetes
    ## docs, 302 should be considered "successful", but this issue on GitHub
    ## (https://github.com/kubernetes/kubernetes/issues/47893) shows that it isn't.
    ## E.g.
    ## httpHeaders:
    ## - name: X-Forwarded-Proto
    ##   value: https
    ##
    httpHeaders: []
  initialDelaySeconds: 120
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 6
  successThreshold: 1
## @param readinessProbe.enabled Enable readinessProbe on WordPress containers
## @skip readinessProbe.httpGet
## @param readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
## @param readinessProbe.periodSeconds Period seconds for readinessProbe
## @param readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
## @param readinessProbe.failureThreshold Failure threshold for readinessProbe
## @param readinessProbe.successThreshold Success threshold for readinessProbe
##
readinessProbe:
  enabled: true
  httpGet:
    path: /wp-login.php
    port: '{{ .Values.wordpressScheme }}'
    scheme: '{{ .Values.wordpressScheme | upper }}'
    ## If using an HTTPS-terminating load-balancer, the probes may need to behave
    ## like the balancer to prevent HTTP 302 responses. According to the Kubernetes
    ## docs, 302 should be considered "successful", but this issue on GitHub
    ## (https://github.com/kubernetes/kubernetes/issues/47893) shows that it isn't.
    ## E.g.
    ## httpHeaders:
    ## - name: X-Forwarded-Proto
    ##   value: https
    ##
    httpHeaders: []
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 6
  successThreshold: 1
## @param startupProbe.enabled Enable startupProbe on WordPress containers
## @skip startupProbe.httpGet
## @param startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
## @param startupProbe.periodSeconds Period seconds for startupProbe
## @param startupProbe.timeoutSeconds Timeout seconds for startupProbe
## @param startupProbe.failureThreshold Failure threshold for startupProbe
## @param startupProbe.successThreshold Success threshold for startupProbe
##
startupProbe:
  enabled: false
  httpGet:
    path: /wp-login.php
    port: '{{ .Values.wordpressScheme }}'
    scheme: '{{ .Values.wordpressScheme | upper }}'
    ## If using an HTTPS-terminating load-balancer, the probes may need to behave
    ## like the balancer to prevent HTTP 302 responses. According to the Kubernetes
    ## docs, 302 should be considered "successful", but this issue on GitHub
    ## (https://github.com/kubernetes/kubernetes/issues/47893) shows that it isn't.
    ## E.g.
    ## httpHeaders:
    ## - name: X-Forwarded-Proto
    ##   value: https
    ##
    httpHeaders: []
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 6
  successThreshold: 1
## @param customLivenessProbe Custom livenessProbe that overrides the default one
##
customLivenessProbe: {}
## @param customReadinessProbe Custom readinessProbe that overrides the default one
##
customReadinessProbe: {}
## @param customStartupProbe Custom startupProbe that overrides the default one
##
customStartupProbe: {}
## @param lifecycleHooks for the WordPress container(s) to automate configuration before or after startup
##
lifecycleHooks: {}

## @section Traffic Exposure Parameters
##

## WordPress service parameters
##
service:
  ## @param service.type WordPress service type
  ##
  type: LoadBalancer
  ## @param service.ports.http WordPress service HTTP port
  ## @param service.ports.https WordPress service HTTPS port
  ##
  ports:
    http: 80
    https: 443
  ## @param service.httpsTargetPort Target port for HTTPS
  ##
  httpsTargetPort: https
  ## Node ports to expose
  ## @param service.nodePorts.http Node port for HTTP
  ## @param service.nodePorts.https Node port for HTTPS
  ## NOTE: choose port between <30000-32767>
  ##
  nodePorts:
    http: ""
    https: ""
  ## @param service.sessionAffinity Control where client requests go, to the same pod or round-robin
  ## Values: ClientIP or None
  ## ref: https://kubernetes.io/docs/user-guide/services/
  ##
  sessionAffinity: None
  ## @param service.sessionAffinityConfig Additional settings for the sessionAffinity
  ## sessionAffinityConfig:
  ##   clientIP:
  ##     timeoutSeconds: 300
  ##
  sessionAffinityConfig: {}
  ## @param service.clusterIP WordPress service Cluster IP
  ## e.g.:
  ## clusterIP: None
  ##
  clusterIP: ""
  ## @param service.loadBalancerIP WordPress service Load Balancer IP
  ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer
  ##
  loadBalancerIP: ""
  ## @param service.loadBalancerSourceRanges WordPress service Load Balancer sources
  ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
  ## e.g:
  ## loadBalancerSourceRanges:
  ##   - 10.10.10.0/24
  ##
  loadBalancerSourceRanges: []
  ## @param service.externalTrafficPolicy WordPress service external traffic policy
  ## ref https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
  ##
  externalTrafficPolicy: Cluster
  ## @param service.annotations Additional custom annotations for WordPress service
  ##
  annotations: {}
  ## @param service.extraPorts Extra port to expose on WordPress service
  ##
  extraPorts: []
## Configure the ingress resource that allows you to access the WordPress installation
## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/
##
ingress:
  ## @param ingress.enabled Enable ingress record generation for WordPress
  ##
  enabled: false
  ## @param ingress.pathType Ingress path type
  ##
  pathType: ImplementationSpecific
  ## @param ingress.apiVersion Force Ingress API version (automatically detected if not set)
  ##
  apiVersion: ""
  ## @param ingress.ingressClassName IngressClass that will be be used to implement the Ingress (Kubernetes 1.18+)
  ## This is supported in Kubernetes 1.18+ and required if you have more than one IngressClass marked as the default for your cluster .
  ## ref: https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/
  ##
  ingressClassName: ""
  ## @param ingress.hostname Default host for the ingress record
  ##
  hostname: wordpress.local
  ## @param ingress.path Default path for the ingress record
  ## NOTE: You may need to set this to '/*' in order to use this with ALB ingress controllers
  ##
  path: /
  ## @param ingress.annotations Additional annotations for the Ingress resource. To enable certificate autogeneration, place here your cert-manager annotations.
  ## For a full list of possible ingress annotations, please see
  ## ref: https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md
  ## Use this parameter to set the required annotations for cert-manager, see
  ## ref: https://cert-manager.io/docs/usage/ingress/#supported-annotations
  ##
  ## e.g:
  ## annotations:
  ##   kubernetes.io/ingress.class: nginx
  ##   cert-manager.io/cluster-issuer: cluster-issuer-name
  ##
  annotations: {}
  ## @param ingress.tls Enable TLS configuration for the host defined at `ingress.hostname` parameter
  ## TLS certificates will be retrieved from a TLS secret with name: `{{- printf "%s-tls" .Values.ingress.hostname }}`
  ## You can:
  ##   - Use the `ingress.secrets` parameter to create this TLS secret
  ##   - Rely on cert-manager to create it by setting the corresponding annotations
  ##   - Rely on Helm to create self-signed certificates by setting `ingress.selfSigned=true`
  ##
  tls: false
  ## @param ingress.selfSigned Create a TLS secret for this ingress record using self-signed certificates generated by Helm
  ##
  selfSigned: false
  ## @param ingress.extraHosts An array with additional hostname(s) to be covered with the ingress record
  ## e.g:
  ## extraHosts:
  ##   - name: wordpress.local
  ##     path: /
  ##
  extraHosts: []
  ## @param ingress.extraPaths An array with additional arbitrary paths that may need to be added to the ingress under the main host
  ## e.g:
  ## extraPaths:
  ## - path: /*
  ##   backend:
  ##     serviceName: ssl-redirect
  ##     servicePort: use-annotation
  ##
  extraPaths: []
  ## @param ingress.extraTls TLS configuration for additional hostname(s) to be covered with this ingress record
  ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#tls
  ## e.g:
  ## extraTls:
  ## - hosts:
  ##     - wordpress.local
  ##   secretName: wordpress.local-tls
  ##
  extraTls: []
  ## @param ingress.secrets Custom TLS certificates as secrets
  ## NOTE: 'key' and 'certificate' are expected in PEM format
  ## NOTE: 'name' should line up with a 'secretName' set further up
  ## If it is not set and you're using cert-manager, this is unneeded, as it will create a secret for you with valid certificates
  ## If it is not set and you're NOT using cert-manager either, self-signed certificates will be created valid for 365 days
  ## It is also possible to create and manage the certificates outside of this helm chart
  ## Please see README.md for more information
  ## e.g:
  ## secrets:
  ##   - name: wordpress.local-tls
  ##     key: |-
  ##       -----BEGIN RSA PRIVATE KEY-----
  ##       ...
  ##       -----END RSA PRIVATE KEY-----
  ##     certificate: |-
  ##       -----BEGIN CERTIFICATE-----
  ##       ...
  ##       -----END CERTIFICATE-----
  ##
  secrets: []
  ## @param ingress.extraRules Additional rules to be covered with this ingress record
  ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-rules
  ## e.g:
  ## extraRules:
  ## - host: wordpress.local
  ##     http:
  ##       path: /
  ##       backend:
  ##         service:
  ##           name: wordpress-svc
  ##           port:
  ##             name: http
  ##
  extraRules: []

## @section Persistence Parameters
##

## Persistence Parameters
## ref: https://kubernetes.io/docs/user-guide/persistent-volumes/
##
persistence:
  ## @param persistence.enabled Enable persistence using Persistent Volume Claims
  ##
  enabled: true
  ## @param persistence.storageClass Persistent Volume storage class
  ## If defined, storageClassName: <storageClass>
  ## If set to "-", storageClassName: "", which disables dynamic provisioning
  ## If undefined (the default) or set to null, no storageClassName spec is set, choosing the default provisioner
  ##
  storageClass: ""
  ## @param persistence.accessModes [array] Persistent Volume access modes
  ##
  accessModes:
    - ReadWriteOnce
  ## @param persistence.accessMode Persistent Volume access mode (DEPRECATED: use `persistence.accessModes` instead)
  ##
  accessMode: ReadWriteOnce
  ## @param persistence.size Persistent Volume size
  ##
  size: 10Gi
  ## @param persistence.dataSource Custom PVC data source
  ##
  dataSource: {}
  ## @param persistence.existingClaim The name of an existing PVC to use for persistence
  ##
  existingClaim: ""
  ## @param persistence.selector Selector to match an existing Persistent Volume for WordPress data PVC
  ## If set, the PVC can't have a PV dynamically provisioned for it
  ## E.g.
  ## selector:
  ##   matchLabels:
  ##     app: my-app
  ##
  selector: {}
  ## @param persistence.annotations Persistent Volume Claim annotations
  ##
  annotations: {}

## Init containers parameters:
## volumePermissions: Change the owner and group of the persistent volume(s) mountpoint(s) to 'runAsUser:fsGroup' on each node
##
volumePermissions:
  ## @param volumePermissions.enabled Enable init container that changes the owner/group of the PV mount point to `runAsUser:fsGroup`
  ##
  enabled: false
  ## Bitnami Shell image
  ## ref: https://hub.docker.com/r/bitnami/bitnami-shell/tags/
  ## @param volumePermissions.image.registry Bitnami Shell image registry
  ## @param volumePermissions.image.repository Bitnami Shell image repository
  ## @param volumePermissions.image.tag Bitnami Shell image tag (immutable tags are recommended)
  ## @param volumePermissions.image.digest Bitnami Shell image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag
  ## @param volumePermissions.image.pullPolicy Bitnami Shell image pull policy
  ## @param volumePermissions.image.pullSecrets Bitnami Shell image pull secrets
  ##
  image:
    registry: docker.io
    repository: bitnami/bitnami-shell
    tag: 11-debian-11-r63
    digest: ""
    pullPolicy: IfNotPresent
    ## Optionally specify an array of imagePullSecrets.
    ## Secrets must be manually created in the namespace.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
    ## e.g:
    ## pullSecrets:
    ##   - myRegistryKeySecretName
    ##
    pullSecrets: []
  ## Init container's resource requests and limits
  ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
  ## @param volumePermissions.resources.limits The resources limits for the init container
  ## @param volumePermissions.resources.requests The requested resources for the init container
  ##
  resources:
    limits: {}
    requests: {}
  ## Init container' Security Context
  ## Note: the chown of the data folder is done to containerSecurityContext.runAsUser
  ## and not the below volumePermissions.containerSecurityContext.runAsUser
  ## @param volumePermissions.containerSecurityContext.runAsUser User ID for the init container
  ##
  containerSecurityContext:
    runAsUser: 0

## @section Other Parameters
##

## WordPress Service Account
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
##
serviceAccount:
  ## @param serviceAccount.create Enable creation of ServiceAccount for WordPress pod
  ##
  create: false
  ## @param serviceAccount.name The name of the ServiceAccount to use.
  ## If not set and create is true, a name is generated using the common.names.fullname template
  ##
  name: ""
  ## @param serviceAccount.automountServiceAccountToken Allows auto mount of ServiceAccountToken on the serviceAccount created
  ## Can be set to false if pods using this serviceAccount do not need to use K8s API
  ##
  automountServiceAccountToken: true
  ## @param serviceAccount.annotations Additional custom annotations for the ServiceAccount
  ##
  annotations: {}
## WordPress Pod Disruption Budget configuration
## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/
## @param pdb.create Enable a Pod Disruption Budget creation
## @param pdb.minAvailable Minimum number/percentage of pods that should remain scheduled
## @param pdb.maxUnavailable Maximum number/percentage of pods that may be made unavailable
##
pdb:
  create: false
  minAvailable: 1
  maxUnavailable: ""
## WordPress Autoscaling configuration
## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
## @param autoscaling.enabled Enable Horizontal POD autoscaling for WordPress
## @param autoscaling.minReplicas Minimum number of WordPress replicas
## @param autoscaling.maxReplicas Maximum number of WordPress replicas
## @param autoscaling.targetCPU Target CPU utilization percentage
## @param autoscaling.targetMemory Target Memory utilization percentage
##
autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 11
  targetCPU: 50
  targetMemory: 50

## @section Metrics Parameters
##

## Prometheus Exporter / Metrics configuration
##
metrics:
  ## @param metrics.enabled Start a sidecar prometheus exporter to expose metrics
  ##
  enabled: false
  ## Bitnami Apache exporter image
  ## ref: https://hub.docker.com/r/bitnami/apache-exporter/tags/
  ## @param metrics.image.registry Apache exporter image registry
  ## @param metrics.image.repository Apache exporter image repository
  ## @param metrics.image.tag Apache exporter image tag (immutable tags are recommended)
  ## @param metrics.image.digest Apache exporter image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag
  ## @param metrics.image.pullPolicy Apache exporter image pull policy
  ## @param metrics.image.pullSecrets Apache exporter image pull secrets
  ##
  image:
    registry: docker.io
    repository: bitnami/apache-exporter
    tag: 0.11.0-debian-11-r73
    digest: ""
    pullPolicy: IfNotPresent
    ## Optionally specify an array of imagePullSecrets.
    ## Secrets must be manually created in the namespace.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
    ## e.g:
    ## pullSecrets:
    ##   - myRegistryKeySecretName
    ##
    pullSecrets: []
  ## @param metrics.containerPorts.metrics Prometheus exporter container port
  ##
  containerPorts:
    metrics: 9117
  ## Configure extra options for Prometheus exporter containers' liveness, readiness and startup probes
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes
  ## @param metrics.livenessProbe.enabled Enable livenessProbe on Prometheus exporter containers
  ## @param metrics.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
  ## @param metrics.livenessProbe.periodSeconds Period seconds for livenessProbe
  ## @param metrics.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
  ## @param metrics.livenessProbe.failureThreshold Failure threshold for livenessProbe
  ## @param metrics.livenessProbe.successThreshold Success threshold for livenessProbe
  ##
  livenessProbe:
    enabled: true
    initialDelaySeconds: 15
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 3
    successThreshold: 1
  ## @param metrics.readinessProbe.enabled Enable readinessProbe on Prometheus exporter containers
  ## @param metrics.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
  ## @param metrics.readinessProbe.periodSeconds Period seconds for readinessProbe
  ## @param metrics.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
  ## @param metrics.readinessProbe.failureThreshold Failure threshold for readinessProbe
  ## @param metrics.readinessProbe.successThreshold Success threshold for readinessProbe
  ##
  readinessProbe:
    enabled: true
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 3
    failureThreshold: 3
    successThreshold: 1
  ## @param metrics.startupProbe.enabled Enable startupProbe on Prometheus exporter containers
  ## @param metrics.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
  ## @param metrics.startupProbe.periodSeconds Period seconds for startupProbe
  ## @param metrics.startupProbe.timeoutSeconds Timeout seconds for startupProbe
  ## @param metrics.startupProbe.failureThreshold Failure threshold for startupProbe
  ## @param metrics.startupProbe.successThreshold Success threshold for startupProbe
  ##
  startupProbe:
    enabled: false
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 1
    failureThreshold: 15
    successThreshold: 1
  ## @param metrics.customLivenessProbe Custom livenessProbe that overrides the default one
  ##
  customLivenessProbe: {}
  ## @param metrics.customReadinessProbe Custom readinessProbe that overrides the default one
  ##
  customReadinessProbe: {}
  ## @param metrics.customStartupProbe Custom startupProbe that overrides the default one
  ##
  customStartupProbe: {}
  ## Prometheus exporter container's resource requests and limits
  ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
  ## @param metrics.resources.limits The resources limits for the Prometheus exporter container
  ## @param metrics.resources.requests The requested resources for the Prometheus exporter container
  ##
  resources:
    limits: {}
    requests: {}
  ## Prometheus exporter service parameters
  ##
  service:
    ## @param metrics.service.ports.metrics Prometheus metrics service port
    ##
    ports:
      metrics: 9150
    ## @param metrics.service.annotations [object] Additional custom annotations for Metrics service
    ##
    annotations:
      prometheus.io/scrape: "true"
      prometheus.io/port: "{{ .Values.metrics.containerPorts.metrics }}"
  ## Prometheus Operator ServiceMonitor configuration
  ##
  serviceMonitor:
    ## @param metrics.serviceMonitor.enabled Create ServiceMonitor Resource for scraping metrics using Prometheus Operator
    ##
    enabled: false
    ## @param metrics.serviceMonitor.namespace Namespace for the ServiceMonitor Resource (defaults to the Release Namespace)
    ##
    namespace: ""
    ## @param metrics.serviceMonitor.interval Interval at which metrics should be scraped.
    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
    ##
    interval: ""
    ## @param metrics.serviceMonitor.scrapeTimeout Timeout after which the scrape is ended
    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
    ##
    scrapeTimeout: ""
    ## @param metrics.serviceMonitor.labels Additional labels that can be used so ServiceMonitor will be discovered by Prometheus
    ##
    labels: {}
    ## @param metrics.serviceMonitor.selector Prometheus instance selector labels
    ## ref: https://github.com/bitnami/charts/tree/main/bitnami/prometheus-operator#prometheus-configuration
    ##
    selector: {}
    ## @param metrics.serviceMonitor.relabelings RelabelConfigs to apply to samples before scraping
    ##
    relabelings: []
    ## @param metrics.serviceMonitor.metricRelabelings MetricRelabelConfigs to apply to samples before ingestion
    ##
    metricRelabelings: []
    ## @param metrics.serviceMonitor.honorLabels Specify honorLabels parameter to add the scrape endpoint
    ##
    honorLabels: false
    ## @param metrics.serviceMonitor.jobLabel The name of the label on the target service to use as the job name in prometheus.
    ##
    jobLabel: ""

## @section NetworkPolicy parameters
##

## Add networkpolicies
##
networkPolicy:
  ## @param networkPolicy.enabled Enable network policies
  ## If ingress.enabled or metrics.enabled are true, configure networkPolicy.ingress and networkPolicy.metrics selectors respectively to allow communication
  ##
  enabled: false
  ## @param networkPolicy.metrics.enabled Enable network policy for metrics (prometheus)
  ## @param networkPolicy.metrics.namespaceSelector [object] Monitoring namespace selector labels. These labels will be used to identify the prometheus' namespace.
  ## @param networkPolicy.metrics.podSelector [object] Monitoring pod selector labels. These labels will be used to identify the Prometheus pods.
  ##
  metrics:
    enabled: false
    ## e.g:
    ## podSelector:
    ##   label: monitoring
    ##
    podSelector: {}
    ## e.g:
    ## namespaceSelector:
    ##   label: monitoring
    ##
    namespaceSelector: {}
  ## @param networkPolicy.ingress.enabled Enable network policy for Ingress Proxies
  ## @param networkPolicy.ingress.namespaceSelector [object] Ingress Proxy namespace selector labels. These labels will be used to identify the Ingress Proxy's namespace.
  ## @param networkPolicy.ingress.podSelector [object] Ingress Proxy pods selector labels. These labels will be used to identify the Ingress Proxy pods.
  ##
  ingress:
    enabled: false
    ## e.g:
    ## podSelector:
    ##   label: ingress
    ##
    podSelector: {}
    ## e.g:
    ## namespaceSelector:
    ##   label: ingress
    ##
    namespaceSelector: {}
  ## @param networkPolicy.ingressRules.backendOnlyAccessibleByFrontend Enable ingress rule that makes the backend (mariadb) only accessible by testlink's pods.
  ## @param networkPolicy.ingressRules.customBackendSelector [object] Backend selector labels. These labels will be used to identify the backend pods.
  ## @param networkPolicy.ingressRules.accessOnlyFrom.enabled Enable ingress rule that makes testlink only accessible from a particular origin
  ## @param networkPolicy.ingressRules.accessOnlyFrom.namespaceSelector [object] Namespace selector label that is allowed to access testlink. This label will be used to identified the allowed namespace(s).
  ## @param networkPolicy.ingressRules.accessOnlyFrom.podSelector [object] Pods selector label that is allowed to access testlink. This label will be used to identified the allowed pod(s).
  ## @param networkPolicy.ingressRules.customRules [object] Custom network policy ingress rule
  ##
  ingressRules:
    ## mariadb backend only can be accessed from testlink
    ##
    backendOnlyAccessibleByFrontend: false
    ## Additional custom backend selector
    ## e.g:
    ## customBackendSelector:
    ##   - to:
    ##       - namespaceSelector:
    ##           matchLabels:
    ##             label: example
    ##
    customBackendSelector: {}
    ## Allow only from the indicated:
    ##
    accessOnlyFrom:
      enabled: false
      ## e.g:
      ## podSelector:
      ##   label: access
      ##
      podSelector: {}
      ## e.g:
      ## namespaceSelector:
      ##   label: access
      ##
      namespaceSelector: {}
    ## custom ingress rules
    ## e.g:
    ## customRules:
    ##   - from:
    ##       - namespaceSelector:
    ##           matchLabels:
    ##             label: example
    ##
    customRules: {}
  ## @param networkPolicy.egressRules.denyConnectionsToExternal Enable egress rule that denies outgoing traffic outside the cluster, except for DNS (port 53).
  ## @param networkPolicy.egressRules.customRules [object] Custom network policy rule
  ##
  egressRules:
    # Deny connections to external. This is not compatible with an external database.
    denyConnectionsToExternal: false
    ## Additional custom egress rules
    ## e.g:
    ## customRules:
    ##   - to:
    ##       - namespaceSelector:
    ##           matchLabels:
    ##             label: example
    ##
    customRules: {}

## @section Database Parameters
##

## MariaDB chart configuration
## ref: https://github.com/bitnami/charts/blob/main/bitnami/mariadb/values.yaml
##
mariadb:
  ## @param mariadb.enabled Deploy a MariaDB server to satisfy the applications database requirements
  ## To use an external database set this to false and configure the `externalDatabase.*` parameters
  ##
  enabled: true
  ## @param mariadb.architecture MariaDB architecture. Allowed values: `standalone` or `replication`
  ##
  architecture: standalone
  ## MariaDB Authentication parameters
  ## @param mariadb.auth.rootPassword MariaDB root password
  ## @param mariadb.auth.database MariaDB custom database
  ## @param mariadb.auth.username MariaDB custom user name
  ## @param mariadb.auth.password MariaDB custom user password
  ## ref: https://github.com/bitnami/containers/tree/main/bitnami/mariadb#setting-the-root-password-on-first-run
  ##      https://github.com/bitnami/containers/blob/main/bitnami/mariadb/README.md#creating-a-database-on-first-run
  ##      https://github.com/bitnami/containers/blob/main/bitnami/mariadb/README.md#creating-a-database-user-on-first-run
  ##
  auth:
    rootPassword: ""
    database: bitnami_wordpress
    username: bn_wordpress
    password: ""
  ## MariaDB Primary configuration
  ##
  primary: 
    ## MariaDB Primary Persistence parameters
    ## ref: https://kubernetes.io/docs/user-guide/persistent-volumes/
    ## @param mariadb.primary.persistence.enabled Enable persistence on MariaDB using PVC(s)
    ## @param mariadb.primary.persistence.storageClass Persistent Volume storage class
    ## @param mariadb.primary.persistence.accessModes [array] Persistent Volume access modes
    ## @param mariadb.primary.persistence.size Persistent Volume size
    ##
    persistence:
      enabled: true
      storageClass: ""
      accessModes:
        - ReadWriteOnce
      size: 8Gi
## External Database Configuration
## All of these values are only used if `mariadb.enabled=false`
##
externalDatabase:
  ## @param externalDatabase.host External Database server host
  ##
  host: localhost
  ## @param externalDatabase.port External Database server port
  ##
  port: 3306
  ## @param externalDatabase.user External Database username
  ##
  user: bn_wordpress
  ## @param externalDatabase.password External Database user password
  ##
  password: ""
  ## @param externalDatabase.database External Database database name
  ##
  database: bitnami_wordpress
  ## @param externalDatabase.existingSecret The name of an existing secret with database credentials. Evaluated as a template
  ## NOTE: Must contain key `mariadb-password`
  ## NOTE: When it's set, the `externalDatabase.password` parameter is ignored
  ##
  existingSecret: ""
## Memcached chart configuration
## ref: https://github.com/bitnami/charts/blob/main/bitnami/memcached/values.yaml
##
memcached:
  ## @param memcached.enabled Deploy a Memcached server for caching database queries
  ##
  enabled: false
  ## Authentication parameters
  ## ref: https://github.com/bitnami/containers/tree/main/bitnami/memcached#creating-the-memcached-admin-user
  ##
  auth:
    ## @param memcached.auth.enabled Enable Memcached authentication
    ##
    enabled: false
    ## @param memcached.auth.username Memcached admin user
    ##
    username: ""
    ## @param memcached.auth.password Memcached admin password
    ##
    password: ""
  ## Service parameters
  ##
  service:
    ## @param memcached.service.port Memcached service port
    ##
    port: 11211
## External Memcached Configuration
## All of these values are only used if `memcached.enabled=false`
##
externalCache:
  ## @param externalCache.host External cache server host
  ##
  host: localhost
  ## @param externalCache.port External cache server port
  ##
  port: 11211 
  ############################################################################################################################################################
  
  "anji@master:~$ mkdir wordpress
  anji@master:~$ cd wordpress/
anji@master:~/wordpress$ ll
anji@master:~/wordpress$ touch wordpress.yaml

anji@master:~/wordpress$ nano wordpress.yaml 
wordpressUsername: admin
wordpressPassword: admin
wordpressEmail: admin@gmail.com
wordpressFirstName: anji
wordpressLastName: reddy
wordpressBlogName: anji.com
service: 
  type: LoadBalancer                 
---==--" 
anji@master:~/wordpress$ kubectl create namespace wordpress
namespace/wordpress created
anji@master:~/wordpress$ kubectl get ns
NAME              STATUS   AGE
default           Active   6h48m
kube-node-lease   Active   6h48m
kube-public       Active   6h48m
kube-system       Active   6h48m
"wordpress  "       Active   10s
===----------"
anji@master:~/wordpress$ helm install wordpress bitnami/wordpress --values=wordpress.yaml --namespace wordpress --version 15.2.22
NAME: wordpress
LAST DEPLOYED: Tue Jan  3 20:18:32 2023
"NAMESPACE": wordpress
STATUS: "deployed"
REVISION: 1
TEST SUITE: None
NOTES:
CHART NAME: wordpress
CHART VERSION: 15.2.22
APP VERSION: 6.1.1
** Please be patient while the chart is being deployed **
Your WordPress site can be accessed through the following DNS name from within your cluster:
    wordpress.wordpress.svc.cluster.local (port 80)
To access your WordPress site from outside the cluster follow the steps below:
1. Get the WordPress URL by running these commands:
  NOTE: It may take a few minutes for the LoadBalancer IP to be available.
        Watch the status with: 'kubectl get svc --namespace wordpress -w wordpress'
   export SERVICE_IP=$(kubectl get svc --namespace wordpress wordpress --include "{{ range (index .status.loadBalancer.ingress 0) }}{{ . }}{{ end }}")
   echo "WordPress URL: http://$SERVICE_IP/"
   echo "WordPress Admin URL: http://$SERVICE_IP/admin"
2. Open a browser and access WordPress using the obtained URL.
3. Login with the following credentials below to see your blog:
  echo Username: admin
  echo Password: $(kubectl get secret --namespace wordpress wordpress -o jsonpath="{.data.wordpress-password}" | base64 -d)

===\\\\\\//////////
anji@master:~$ kubectl get po -n wordpress
NAME                        READY   STATUS    RESTARTS   AGE
wordpress-8b854d97f-6bkh4   0/1     Pending   0          8m58s
wordpress-mariadb-0         0/1     Pending   0          8m58s

anji@master:~/wordpress$ kubectl get secret --namespace wordpress
NAME                              TYPE                 DATA   AGE
sh.helm.release.v1.wordpress.v1   helm.sh/release.v1   1      9m4s
wordpress                         Opaque               1      9m4s
wordpress-mariadb                 "Opaque  "             2      9m4s  "
\\\\\\///////////===

anji@master:~$ kubectl get svc  -n wordpress -o wide 
NAME                TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE   SELECTOR
wordpress          " LoadBalancer   10.97.108.43    <pending>  "   80:30121/TCP,443:30120/TCP   28m   app.kubernetes.io/instance=wordpress,app.kubernetes.io/name=wordpress
wordpress-mariadb   ClusterIP      10.109.65.243   <none>        3306/TCP                     28m   app.kubernetes.io/component=primary,app.kubernetes.io/instance=wordpress,app.kubernetes.io/name=mariadb
anji@master:~$ 


----
anji@master:~/wordpress$ kubectl describe pod  wordpress-8b854d97f-6bkh4  -n  wordpress
Name:             wordpress-8b854d97f-6bkh4
Namespace:        wordpress
Node:             <none>
Labels:           app.kubernetes.io/instance=wordpress
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=wordpress
                  helm.sh/chart=wordpress-15.2.22
                  pod-template-hash=8b854d97f
Annotations:      <none>
Status:           Pending
Controlled By:    ReplicaSet/wordpress-8b854d97f
Containers:
  wordpress:
    Image:       docker.io/bitnami/wordpress:6.1.1-debian-11-r15
    Ports:       8080/TCP, 8443/TCP
    Host Ports:  0/TCP, 0/TCP
    "Requests:"
      cpu:      300m
      memory:   512Mi
  "  Liveness:"   http-get http://:http/wp-admin/install.php delay=120s timeout=5s period=10s #success=1 #failure=6
   " Readiness:  h"ttp-get http://:http/wp-login.php delay=30s timeout=5s period=10s #success=1 #failure=6
    Environment:
      BITNAMI_DEBUG:                          false
      ALLOW_EMPTY_PASSWORD:                   yes
      MARIADB_HOST:                           wordpress-mariadb
      MARIADB_PORT_NUMBER:                   " 3306"
      WORDPRESS_DATABASE_NAME:                bitnami_wordpress
      WORDPRESS_DATABASE_USER:                bn_wordpress
      WORDPRESS_DATABASE_PASSWORD:            <set to the key 'mariadb-password' in secret 'wordpress-mariadb'>  Optional: false
      WORDPRESS_"USERNAME:                     admin"
      WORDPRESS_PASSWORD:                     <set to the key 'wordpress-password' in secret 'wordpress'>  Optional: false
      WORDPRESS_EMAIL:                        admin@gmail.com
      WORDPRESS_FIRST_NAME:                   anji
      WORDPRESS_LAST_NAME:                    reddy
      WORDPRESS_HTACCESS_OVERRIDE_NONE:       no
      WORDPRESS_ENABLE_HTACCESS_PERSISTENCE:  no
      WORDPRESS_BLOG_NAME:                    anji.com
      WORDPRESS_SKIP_BOOTSTRAP:               no
      WORDPRESS_TABLE_PREFIX:                 wp_
      WORDPRESS_SCHEME:                       http
      WORDPRESS_EXTRA_WP_CONFIG_CONTENT:      
      WORDPRESS_PLUGINS:                      none
      APACHE_HTTP_PORT_NUMBER:                8080
      APACHE_HTTPS_PORT_NUMBER:               8443
    Mounts:
      /"bitnami/wordpress from wordpress-data (rw,path="wordpress")"
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-k4cm5 (ro)
Conditions:
  Type           Status
  PodScheduled   False 
Volumes:
  wordpress-data:
    Type:     "  PersistentVolumeClaim (a reference to a" PersistentVolumeClaim in the same namespace)
   " ClaimName:  wordpress"
    ReadOnly:   false
  kube-api-access-k4cm5:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age                  From               Message
  ----     ------            ----                 ----               -------
  Warning  FailedScheduling  2m17s (x3 over 12m)  default-scheduler  0/2 nodes are available: 2 pod has unbound immediate PersistentVolumeClaims. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling.
\\\\\\/\/\\/\/\/\\\////////////////////\\\\\\\\\\\\\\\\\\\\\\\\\\\//\/\

anji@master:~/wordpress$ kubectl describe pod  wordpress-mariadb-0   -n  wordpress
Name:             wordpress-mariadb-0
Namespace:        wordpress
Service Account:  wordpress-mariadb
Labels:           app.kubernetes.io/component=primary
                  app.kubernetes.io/instance=wordpress
                  app.kubernetes.io/managed-by=Helm
                  app.kubernetes.io/name=mariadb
                  controller-revision-hash=wordpress-mariadb-7b6748bdff
                  helm.sh/chart=mariadb-11.4.2
                  statefulset.kubernetes.io/pod-name=wordpress-mariadb-0
Controlled By:    "StatefulSet/wordpress-mariadb"
Containers:
 " mariadb:"
    Image:      docker.io/bitnami/mariadb:10.6.11-debian-11-r12
    Port:       3306/TCP
    Host Port:  0/TCP
    Liveness:   exec [/bin/bash -ec password_aux="${MARIADB_ROOT_PASSWORD:-}"
if [[ -f "${MARIADB_ROOT_PASSWORD_FILE:-}" ]]; then
    password_aux=$(cat "$MARIADB_ROOT_PASSWORD_FILE")
fi
mysqladmin status -uroot -p"${password_aux}"
] delay=120s timeout=1s period=10s #success=1 #failure=3
    Readiness:  exec [/bin/bash -ec password_aux="${MARIADB_ROOT_PASSWORD:-}"
if [[ -f "${MARIADB_ROOT_PASSWORD_FILE:-}" ]]; then
    password_aux=$(cat "$MARIADB_ROOT_PASSWORD_FILE")
fi
mysqladmin status -uroot -p"${password_aux}"
] delay=30s timeout=1s period=10s #success=1 #failure=3
    Environment:
      BITNAMI_DEBUG:          false
      MARIADB_ROOT_PASSWORD:  <set to the key 'mariadb-root-password' in secret 'wordpress-mariadb'>  Optional: false
      MARIADB_USER:           bn_wordpress
      MARIADB_PASSWORD:       <set to the key 'mariadb-password' in secret 'wordpress-mariadb'>  Optional: false
      MARIADB_DATABASE:       bitnami_wordpress
    Mounts:
      /bitnami/mariadb from data (rw)
      /opt/bitnami/mariadb/conf/my.cnf from config (rw,path="my.cnf")
Conditions:
  Type           Status
  PodScheduled   False 
Volumes:
  data:
    "Type:       PersistentVolumeClaim" (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  data-wordpress-mariadb-0
    ReadOnly:   false
  config:
    Type:        ConfigMap (a volume populated by a ConfigMap)
    Name:        wordpress-mariadb
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age                  From               Message
  ----     ------            ----                 ----               -------
  Warning  FailedScheduling  3m23s (x4 over 18m)  default-scheduler  0/2 nodes are available: 2 pod has unbound immediate PersistentVolumeClaims. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling.
"=====================================================================
Helm Chart Demo - How to create your first Helm Chart? - Part 6
https://www.youtube.com/watch?v=2dqQcou_MCU&list=PL7iMyoQPMtANm_35XWjkNzDCcsw9vy01b&index=2
https://jhooq.com/building-first-helm-chart-with-spring-boot/

anji@master:~$ helm create springboot
Creating springboot

anji@master:~$ ls
Desktop  Documents  Downloads  Music  Pictures  Public  springboot  Templates  Videos  wordpress
anji@master:~$ cd springboot/
anji@master:~/springboot$ ls
charts  Chart.yaml  templates  values.yaml
anji@master:~/springboot$ tree
.
â”œâ”€â”€ charts
â”œâ”€â”€ Chart.yaml
â”œâ”€â”€ templates
â”‚Â Â  â”œâ”€â”€ deployment.yaml
â”‚Â Â  â”œâ”€â”€ _helpers.tpl
â”‚Â Â  â”œâ”€â”€ hpa.yaml
â”‚Â Â  â”œâ”€â”€ ingress.yaml
â”‚Â Â  â”œâ”€â”€ NOTES.txt
â”‚Â Â  â”œâ”€â”€ serviceaccount.yaml
â”‚Â Â  â”œâ”€â”€ service.yaml
â”‚Â Â  â””â”€â”€ tests
â”‚Â Â      â””â”€â”€ test-connection.yaml
â””â”€â”€ values.yaml
3 directories, 10 files
---====//\\\============== 
anji@master:~/springboot$ cat Chart.yaml 
apiVersion: v2   ##  importent 
name: springboot
description: A Helm chart for Kubernetes
type: application
version: 0.1.0
appVersion: "1.16.0"      ##  importent 

====#############\/\/\/\/\/\\\\\\\\// "
anji@master:~/springboot$ cat  values.yaml 
# Default values for springboot.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  "repository: rahulwagh17/kubernetes:jhooq-k8s-springboot "  # see look 
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: ""

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podAnnotations: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

service:
  type: ClusterIP
  "port: 8080     # see look 
"
ingress:
  enabled: false
  className: ""
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: chart-example.local
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

nodeSelector: {}

tolerations: []

affinity: {}   "
====--==/\/\==============================\\
anji@master:~/springboot$ cat templates/deployment.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "springboot.fullname" . }}
  labels:
    {{- include "springboot.labels" . | nindent 4 }}
spec:
  {{- if not .Values.autoscaling.enabled }}
  replicas: {{ .Values.replicaCount }}
  {{- end }}
  selector:
    matchLabels:
      {{- include "springboot.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      {{- with .Values.podAnnotations }}
      annotations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      labels:
        {{- include "springboot.selectorLabels" . | nindent 8 }}
    spec:
      {{- with .Values.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      serviceAccountName: {{ include "springboot.serviceAccountName" . }}
      securityContext:
        {{- toYaml .Values.podSecurityContext | nindent 8 }}
      containers:
        - name: {{ .Chart.Name }}
          securityContext:
            {{- toYaml .Values.securityContext | nindent 12 }}
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          ports:
            - name: http
              containerPort: {{ .Values.service.port }}
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: http
          readinessProbe:
            httpGet:
              path: /
              port: http
          resources:
            {{- toYaml .Values.resources | nindent 12 }}
      {{- with .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
======++++++++++++++++++++++++++++++++"
anji@master:~/springboot/templates$ cat service.yaml 
apiVersion: v1
kind: Service
metadata:
  name: {{ include "springboot.fullname" . }}
  labels:
    {{- include "springboot.labels" . | nindent 4 }}
spec:
  type: {{ .Values.service.type }}
  ports:
    - port: {{ .Values.service.port }}
      targetPort: http
      protocol: TCP
      name: http
  selector:
    {{- include "springboot.selectorLabels" . | nindent 4 }}  "

############################################################################333
anji@master:~$ helm template springboot/
---
# Source: springboot/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-springboot
  labels:
    helm.sh/chart: springboot-0.1.0
    app.kubernetes.io/name: springboot
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: springboot/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-springboot
  labels:
    helm.sh/chart: springboot-0.1.0
    app.kubernetes.io/name: springboot
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: springboot
    app.kubernetes.io/instance: release-name
---
# Source: springboot/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-springboot
  labels:
    helm.sh/chart: springboot-0.1.0
    app.kubernetes.io/name: springboot
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: springboot
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      labels:
        app.kubernetes.io/name: springboot
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: release-name-springboot
      securityContext:
        {}
      containers:
        - name: springboot
          securityContext:
            {}
          image: "rahulwagh17/kubernetes:jhooq-k8s-springboot"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
#          livenessProbe:
 #           httpGet:
  #            path: /
   #           port: http
    #      readinessProbe:
     #       httpGet:
      #        path: /
       #       port: http
          resources:
            {}
---
# Source: springboot/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "release-name-springboot-test-connection"
  labels:
    helm.sh/chart: springboot-0.1.0
    app.kubernetes.io/name: springboot
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args: ['release-name-springboot:8080']
  restartPolicy: Never
###############################################################3
"
anji@master:~$ helm lint springboot/
==> Linting springboot/
[INFO] Chart.yaml: icon is recommended
1 chart(s) linted, 0 chart(s) failed   ## see look 
"###############################################################33
anji@master:~$ helm install mydebug --debug --dry-run springboot/
install.go:209: [debug] CHART PATH: /home/anji/springboot
NAME: mydebug
LAST DEPLOYED: Wed Jan  4 12:10:41 2023
NAMESPACE: default
STATUS:" pending-install"
REVISION: 1
USER-SUPPLIED VALUES:
{}

COMPUTED VALUES:
affinity: {}
autoscaling:
  enabled: false
  maxReplicas: 100
  minReplicas: 1
  targetCPUUtilizationPercentage: 80
fullnameOverride: ""
image:
  pullPolicy: IfNotPresent
  repository: rahulwagh17/kubernetes:jhooq-k8s-springboot
  tag: ""
imagePullSecrets: []
ingress:
  annotations: {}
  className: ""
  enabled: false
  hosts:
  - host: chart-example.local
    paths:
    - path: /
      pathType: ImplementationSpecific
  tls: []
nameOverride: ""
nodeSelector: {}
podAnnotations: {}
podSecurityContext: {}
replicaCount: 1
resources: {}
securityContext: {}
service:
  port: 8080
  type: ClusterIP
serviceAccount:
  annotations: {}
  create: true
  name: ""
tolerations: []

HOOKS:
---
# Source: springboot/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "mydebug-springboot-test-connection"
  labels:
    helm.sh/chart: springboot-0.1.0
    app.kubernetes.io/name: springboot
    app.kubernetes.io/instance: mydebug
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args: ['mydebug-springboot:8080']
  restartPolicy: Never
MANIFEST:
---
# Source: springboot/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: mydebug-springboot
  labels:
    helm.sh/chart: springboot-0.1.0
    app.kubernetes.io/name: springboot
    app.kubernetes.io/instance: mydebug
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: springboot/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: mydebug-springboot
  labels:
    helm.sh/chart: springboot-0.1.0
    app.kubernetes.io/name: springboot
    app.kubernetes.io/instance: mydebug
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: springboot
    app.kubernetes.io/instance: mydebug
---
# Source: springboot/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mydebug-springboot
  labels:
    helm.sh/chart: springboot-0.1.0
    app.kubernetes.io/name: springboot
    app.kubernetes.io/instance: mydebug
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: springboot
      app.kubernetes.io/instance: mydebug
  template:anji@master:~$ helm install mydebug --debug --dry-run springboot/
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/anji/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/anji/.kube/config
install.go:192: [debug] Original chart version: ""
install.go:209: [debug] CHART PATH: /home/anji/springboot

NAME: mydebug
LAST DEPLOYED: Wed Jan  4 12:10:41 2023
NAMESPACE: default
STATUS: pending-install
REVISION: 1
USER-SUPPLIED VALUES:
{}

COMPUTED VALUES:
affinity: {}
autoscaling:
  enabled: false
  maxReplicas: 100
  minReplicas: 1
  targetCPUUtilizationPercentage: 80
fullnameOverride: ""
image:
  pullPolicy: IfNotPresent
  repository: rahulwagh17/kubernetes:jhooq-k8s-springboot
  tag: ""
imagePullSecrets: []
ingress:
  annotations: {}
  className: ""
  enabled: false
  hosts:
  - host: chart-example.local
    paths:
    - path: /
      pathType: ImplementationSpecific
  tls: []
nameOverride: ""
nodeSelector: {}
podAnnotations: {}
podSecurityContext: {}
replicaCount: 1
resources: {}
securityContext: {}
service:
  port: 8080
  type: ClusterIP
serviceAccount:
  annotations: {}
  create: true
  name: ""
tolerations: []

HOOKS:
---
# Source: springboot/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "mydebug-springboot-test-connection"
  labels:
    helm.sh/chart: springboot-0.1.0
    app.kubernetes.io/name: springboot
    app.kubernetes.io/instance: mydebug
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args: ['mydebug-springboot:8080']
  restartPolicy: Never
MANIFEST:
---
# Source: springboot/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: mydebug-springboot
  labels:
    helm.sh/chart: springboot-0.1.0
    app.kubernetes.io/name: springboot
    app.kubernetes.io/instance: mydebug
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: springboot/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: mydebug-springboot
  labels:
    helm.sh/chart: springboot-0.1.0
    app.kubernetes.io/name: springboot
    app.kubernetes.io/instance: mydebug
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: springboot
    app.kubernetes.io/instance: mydebug
---
# Source: springboot/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mydebug-springboot
  labels:
    helm.sh/chart: springboot-0.1.0
    app.kubernetes.io/name: springboot
    app.kubernetes.io/instance: mydebug
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: springboot
      app.kubernetes.io/instance: mydebug
  template:
    metadata:
      labels:
        app.kubernetes.io/name: springboot
        app.kubernetes.io/instance: mydebug
    spec:
      serviceAccountName: mydebug-springboot
      securityContext:
        {}
      containers:
        - name: springboot
          securityContext:
            {}
          image: "rahulwagh17/kubernetes:jhooq-k8s-springboot"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
#          livenessProbe:
 #           httpGet:
  #            path: /
   #           port: http
    #      readinessProbe:
     #       httpGet:
      #        path: /
       #       port: http
          resources:
            {}
NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=springboot,app.kubernetes.io/instance=mydebug" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT
anji@master:~$ 
    metadata:
      labels:
        app.kubernetes.io/name: springboot
        app.kubernetes.io/instance: mydebug
    spec:
      serviceAccountName: mydebug-springboot
      securityContext:
        {}
      containers:
        - name: springboot
          securityContext:
            {}
          image: "rahulwagh17/kubernetes:jhooq-k8s-springboot"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
#          livenessProbe:
 #           httpGet:
  #            path: /
   #           port: http
    #      readinessProbe:
     #       httpGet:
      #        path: /
       #       port: http
          resources:
            {}
NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=springboot,app.kubernetes.io/instance=mydebug" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT
########################################################################################33
"anji@master:~$ helm install release1-1  springboot/
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/anji/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/anji/.kube/config
NAME: "release1-1"
LAST DEPLOYED: Wed Jan  4 12:37:03 2023
NAMESPACE: default
STATUS: "deployed"
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=springboot,app.kubernetes.io/instance=release1-1" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT 
=======++++++++++++++++++///\//
anji@master:~$ kubectl get pod,service,deployment  -o wide 
NAME                                         READY   STATUS    RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES
pod/"release1-1-"springboot-85f9fcc4c5-v4cg2   0/1     Pending   0          5s    <none>   <none>   <none>           <none>

NAME                            TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE   SELECTOR
service/kubernetes              ClusterIP   10.96.0.1      <none>        443/TCP    17h   <none>
service/release1-1-springboot   ClusterIP   10.98.185.59   <none>        "8080/TCP"   5s    app.kubernetes.io/instance=release1-1,app.kubernetes.io/name=springboot

NAME                                    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES                                        SELECTOR
deployment.apps/release1-1-springboot   0/1     1            0           5s    springboot   "rahulwagh17/kubernetes:jhooq-k8s-springboot "  app.kubernetes.io/instance=release1-1,app.kubernetes.io/name=springboot "
==\\/\//####
anji@master:~$ helm list -a 
NAME      	NAMESPACE	REVISION	UPDATED                                	STATUS  	CHART           	APP VERSION
release1  	default  	2       	2023-01-03 17:47:48.50720335 +0530 IST 	deployed	helloworld-0.1.0	1.16.0     
"release1-1	"default  	"1  "     	2023-01-04 12:42:18.744331019 +0530 IST	deployed	springboot-0.1.0	1.16.0     "
########################################### +++++   BEFORE  BEFORE =====
anji@master:~$ cat springboot/Chart.yaml 
apiVersion: v2
name: springboot
description: A Helm chart for Kubernetes
type: application
version: 0.1.0   ##  SEE LOOK  CHANGED THE VERSION 
appVersion: "1.16.0"
====++++++++

anji@master:~$ cat springboot/Chart.yaml 
apiVersion: v2
name: springboot
description: A Helm chart for Kubernetes
type: application
version: 0.1.1     ## see look changed the value 
appVersion: "1.16.0"
######################################   before  BEFORE 
anji@master:~$ cd  springboot/
anji@master:~/springboot$ cat values.yaml 
# Default values for springboot.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
replicaCount: 1    ===  SEE LOOK 
image:
  repository: rahulwagh17/kubernetes:jhooq-k8s-springboot 
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: ""
==================
anji@master:~$ cd  springboot/
anji@master:~/springboot$ cat values.yaml 
# Default values for springboot.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
replicaCount: 2     ===  SEE LOOK 
image:
  repository: rahulwagh17/kubernetes:jhooq-k8s-springboot 
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: ""
============+++
anji@master:~$ helm upgrade release1-1 springboot/
Release "release1-1" has been upgraded. Happy Helming!
NAME: release1-1
LAST DEPLOYED: Wed Jan  4 13:23:14 2023
NAMESPACE: default
STATUS: deployed
REVISION: 2   # see look changed 
NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=springboot,app.kubernetes.io/instance=release1-1" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT
                         
anji@master:~$ kubectl get all -o wide  --show-labels
NAME                                         READY   STATUS    RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES   LABELS
pod/release1-1-springboot-85f9fcc4c5-v4cg2   0/1     Pending   0          43m   <none>   <none>   <none>           <none>            app.kubernetes.io/instance=release1-1,app.kubernetes.io/name=springboot,pod-template-hash=85f9fcc4c5

NAME                            TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE   SELECTOR                                                                  LABELS
service/kubernetes              ClusterIP   10.96.0.1      "<none>"        443/TCP    18h   <none>                                                                    component=apiserver,provider=kubernetes
service/release1-1-springboot   ClusterIP   10.98.185.59  " <none>   "     8080/TCP   43m   app.kubernetes.io/instance=release1-1,app.kubernetes.io/name=springboot   app.kubernetes.io/instance=release1-1,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=springboot,app.kubernetes.io/version=1.16.0,helm.sh/chart=springboot-0.1.1

NAME                                    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES                                        SELECTOR                                                                  LABELS
deployment.apps/release1-1-springboot   0/1     1            0           43m   springboot   rahulwagh17/kubernetes:jhooq-k8s-springboot   app.kubernetes.io/instance=release1-1,app.kubernetes.io/name=springboot   app.kubernetes.io/instance=release1-1,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=springboot,app.kubernetes.io/version=1.16.0,helm.sh/chart=springboot-0.1.1

NAME                                               DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES                                        SELECTOR                                                                                               LABELS
replicaset.apps/release1-1-springboot-85f9fcc4c5   1         1         0       43m   springboot   rahulwagh17/kubernetes:jhooq-k8s-springboot   app.kubernetes.io/instance=release1-1,app.kubernetes.io/name=springboot,pod-template-hash=85f9fcc4c5   app.kubernetes.io/instance=release1-1,app.kubernetes.io/name=springboot,pod-template-hash=85f9fcc4c5
################################################################ -roleback 

anji@master:~$ helm list -a 
NAME      	NAMESPACE	REVISION	UPDATED                                	STATUS  	CHART           	  APP VERSION
release1-1	default  	"2  "     	2023-01-04 13:23:14.402246047 +0530 IST	deployed	"springboot-0.1.1	"1.16.0     

anji@master:~$ helm rollback release1-1  1
Rollback was a "success!" Happy Helming!

anji@master:~$ helm list -a 
NAME      	NAMESPACE	REVISION	UPDATED                                	STATUS  	CHART           	APP VERSION
release1-1	default  	3       	2023-01-04 13:50:28.742369186 +0530 IST	deployed	s"pringboot-0.1.0"	1.16.0     
===========+++++++++++++++++=================
anji@master:~$ helm diff revision release1-1   1  2 
default, release1-1-springboot, Deployment (apps) has changed:
  # Source: springboot/templates/deployment.yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: release1-1-springboot
    labels:
-     helm.sh/chart: springboot-0.1.0    ### see look 
+     helm.sh/chart: springboot-0.1.1
      app.kubernetes.io/name: springboot
      app.kubernetes.io/instance: release1-1
      app.kubernetes.io/version: "1.16.0"
      app.kubernetes.io/managed-by: Helm
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/name: springboot
        app.kubernetes.io/instance: release1-1
    template:
      metadata:
        labels:
          app.kubernetes.io/name: springboot
          app.kubernetes.io/instance: release1-1
      spec:
        serviceAccountName: release1-1-springboot
        securityContext:
          {}
        containers:
          - name: springboot
            securityContext:
              {}
            image: "rahulwagh17/kubernetes:jhooq-k8s-springboot"
            imagePullPolicy: IfNotPresent
            ports:
              - name: http
                containerPort: 8080
                protocol: TCP
  #          livenessProbe:
   #           httpGet:
    #            path: /
     #           port: http
      #      readinessProbe:
       #       httpGet:
        #        path: /
         #       port: http
            resources:
              {}
default, release1-1-springboot, Service (v1) has changed:
  # Source: springboot/templates/service.yaml
  apiVersion: v1
  kind: Service
  metadata:
    name: release1-1-springboot
    labels:
-     helm.sh/chart: springboot-0.1.0
+     helm.sh/chart: springboot-0.1.1    ##  see look changed 
      app.kubernetes.io/name: springboot
      app.kubernetes.io/instance: release1-1
      app.kubernetes.io/version: "1.16.0"
      app.kubernetes.io/managed-by: Helm
  spec:
    type: ClusterIP
    ports:
      - port: 8080
        targetPort: http
        protocol: TCP
        name: http
    selector:
      app.kubernetes.io/name: springboot
      app.kubernetes.io/instance: release1-1
default, release1-1-springboot, ServiceAccount (v1) has changed:
  # Source: springboot/templates/serviceaccount.yaml
  apiVersion: v1
  kind: ServiceAccount
  metadata:
    name: release1-1-springboot
    labels:
-     helm.sh/chart: springboot-0.1.0
+     helm.sh/chart: springboot-0.1.1     ## see look 
      app.kubernetes.io/name: springboot
      app.kubernetes.io/instance: release1-1
      app.kubernetes.io/version: "1.16.0"
      app.kubernetes.io/managed-by: Helm
####################################################################################
How to convert Kubernetes yaml to Helm Chart yaml "
https://www.youtube.com/watch?v=ZZVXXEyEzAs&list=PL7iMyoQPMtANm_35XWjkNzDCcsw9vy01b&index=7
https://jhooq.com/convert-kubernetes-yaml-into-helm/

apiVersion: apps/v1
kind: Deployment
metadata: 
  name: nginx
  labels:
    app: nginx
spec:
  replicas: 1
  selector: 
     matchLabels:
       app: nginx
  strategy: {}     
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: nginx
    spec:
      containers: 
       - name: nginx
         image: rahulwagh17/kubernetes:jhooq-k8s-springboot
         ports: 
           - containerPort: 80
         resources: {}
status: {}          
 
anji@master:~$ kubectl get all -o wide  --show-labels
NAME                         READY   STATUS    RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES   LABELS
pod/nginx-7bc7874b58-tc8dr   0/1     Pending   0          78s   <none>   <none>   <none>           <none>            app=nginx,pod-template-hash=7bc7874b58

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE   SELECTOR   LABELS
service/kubernetes   ClusterIP   10.96.0.1    "<none>  "      443/TCP   87s   <none>     component=apiserver,provider=kubernetes

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES                                        SELECTOR    LABELS
deployment.apps/nginx   0/1     1            0           78s   nginx        rahulwagh17/kubernetes:jhooq-k8s-springboot   app=nginx   app=nginx

NAME                               DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES                                        SELECTOR                                 LABELS
replicaset.apps/nginx-7bc7874b58   1         1         0       78s   nginx        rahulwagh17/kubernetes:jhooq-k8s-springboot   app=nginx,pod-template-hash=7bc7874b58   app=nginx,pod-template-hash=7bc7874b58
=============++++++++++++"
apiVersion: v1
kind: Service
metadata: 
  name: service
  labels: 
    app: nginx
spec:
  ports:
    - port: 8080
      protocol: TCP
      targetPort: 8080
  selector:
    app: nginx
  type: LoadBalancer
status: 
  loadBalancer: {}
  
anji@master:~$ kubectl get all -o wide  --show-labels
NAME                         READY   STATUS    RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES   LABELS
pod/nginx-7bc7874b58-tc8dr   0/1     Pending   0          14m   <none>   <none>   <none>           <none>            app=nginx,pod-template-hash=7bc7874b58

NAME                 TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE   SELECTOR    LABELS
service/kubernetes   ClusterIP      10.96.0.1       <none>        443/TCP          14m   <none>      component=apiserver,provider=kubernetes
service/service    "  LoadBalancer "  10.107.234.72   <pending>    " 8080:32425/TCP "  5s    app=nginx   app=nginx

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES                                        SELECTOR    LABELS
deployment.apps/nginx   0/1     1            0           14m   nginx        rahulwagh17/kubernetes:jhooq-k8s-springboot   app=nginx   app=nginx

NAME                               DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES                                        SELECTOR                                 LABELS
replicaset.apps/nginx-7bc7874b58   1         1         0       14m   nginx        rahulwagh17/kubernetes:jhooq-k8s-springboot   app=nginx,pod-template-hash=7bc7874b58   app=nginx,pod-template-hash=7bc7874b58

###################################################################################
anji@master:~$ helm create velpula
Creating velpula

anji@master:~$ tree reddy/
reddy/
â”œâ”€â”€ charts
â”œâ”€â”€ Chart.yaml
â”œâ”€â”€ templates
â”‚Â Â  â”œâ”€â”€ deployment.yaml
â”‚Â Â  â”œâ”€â”€ _helpers.tpl
â”‚Â Â  â”œâ”€â”€ hpa.yaml
â”‚Â Â  â”œâ”€â”€ ingress.yaml
â”‚Â Â  â”œâ”€â”€ NOTES.txt
â”‚Â Â  â”œâ”€â”€ serviceaccount.yaml
â”‚Â Â  â”œâ”€â”€ service.yaml
â”‚Â Â  â””â”€â”€ tests
â”‚Â Â      â””â”€â”€ test-connection.yaml
â””â”€â”€ values.yaml
3 directories, 10 files
=----++++++++\\/\/
anji@master:~$ tree anji
anji
â”œâ”€â”€ charts
â”œâ”€â”€ Chart.yaml
â”œâ”€â”€ templates
â”‚Â Â  â”œâ”€â”€ deployment.yaml
â”‚Â Â  â”œâ”€â”€ _helpers.tpl
â”‚Â Â  â”œâ”€â”€ hpa.yaml
â”‚Â Â  â”œâ”€â”€ ingress.yaml
â”‚Â Â  â”œâ”€â”€ NOTES.txt
â”‚Â Â  â”œâ”€â”€ serviceaccount.yaml
â”‚Â Â  â”œâ”€â”€ service.yaml
â”‚Â Â  â””â”€â”€ tests
â”‚Â Â      â””â”€â”€ test-connection.yaml
â””â”€â”€ values.yaml
3 directories, 10 files
====================+++++++++++++==
Here is comparision of YAMLs generated by Helm Chart and Kubernetes(k8s) -
YAMLs Generated by Helm Chart	                                Kubernetes(k8s) YAMLs

1	Chart.yaml	
2	helper.tpl	
3	deployment.yaml	                                             k8s-deployment.yaml
4	hpa.yaml	
5	ingress.yaml	
6	service.yaml	                                                k8s-service.yaml
7	serviceaccount.yaml	
8	test-connection.yaml	
9	values.yaml	                                                  k8s-deployment.yaml
                                                                 1. replicas: 1
                                                                 2. docker image =rahulwagh17/kubernetes:jhooq-k8s-springboot
-----========------------++
anji@master:~$ tree velpula/
velpula/
â”œâ”€â”€ charts
â”œâ”€â”€ Chart.yaml
â”œâ”€â”€ templates
â”‚Â Â  â”œâ”€â”€ deployment.yaml
â”‚Â Â  â”œâ”€â”€ _helpers.tpl
â”‚Â Â  â”œâ”€â”€ hpa.yaml
â”‚Â Â  â”œâ”€â”€ ingress.yaml
â”‚Â Â  â”œâ”€â”€ NOTES.txt
â”‚Â Â  â”œâ”€â”€ serviceaccount.yaml
â”‚Â Â  â”œâ”€â”€ service.yaml
â”‚Â Â  â””â”€â”€ tests
â”‚Â Â      â””â”€â”€ test-connection.yaml
â””â”€â”€ values.yaml
3 directories, 10 files

===-----
anji@master:~/velpula$ cat  Chart.yaml 
apiVersion: v2
name: velpula
description: anjireddy-velpula-venkata-test-helm   ## see look  changed
type: application
version: 0.1.0
appVersion: "1.16.0"

anji@master:~/velpula/templates$ cat  deployment.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "velpula.fullname" . }}
  labels:
    {{- include "velpula.labels" . | nindent 4 }}
spec:
  {{- if not .Values.autoscaling.enabled }}
  replicas: {{ .Values.replicaCount }}
  {{- end }}
  selector:
    matchLabels:
      {{- include "velpula.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      {{- with .Values.podAnnotations }}
      annotations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      labels:
        {{- include "velpula.selectorLabels" . | nindent 8 }}
    spec:
      {{- with .Values.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      serviceAccountName: {{ include "velpula.serviceAccountName" . }}
      securityContext:
        {{- toYaml .Values.podSecurityContext | nindent 8 }}
      containers:
        - name: {{ .Chart.Name }}
          securityContext:
            {{- toYaml .Values.securityContext | nindent 12 }}
          image: "{{ .Values.image.repository }}"                 ## see look changed 
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          ports:
            - name: http
              containerPort: 8080    ## see look changed 
              protocol: TCP
          resources:
            {{- toYaml .Values.resources | nindent 12 }}
      {{- with .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
+++++++------------====       
anji@master:~/velpula/templates$ cat service.yaml   ## no changes  no  modifications 
apiVersion: v1
kind: Service
metadata:
  name: {{ include "velpula.fullname" . }}
  labels:
    {{- include "velpula.labels" . | nindent 4 }}
spec:
  type: {{ .Values.service.type }}
  ports:
    - port: {{ .Values.service.port }}
      targetPort: http
      protocol: TCP
      name: http
  selector:
    {{- include "velpula.selectorLabels" . | nindent 4 }}
===---======#######
anji@master:~/velpula$ cat  values.yaml 
# Default values for velpula.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
replicaCount: 1
image:
  repository: rahulwagh17/kubernetes:jhooq-k8s-springboot   #  see look changed 
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: ""
imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""
serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""
podAnnotations: {}
podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000
service:
  type: NoadPort    # changed see look  
  port: 8080   see look 
ingress:
  enabled: false
  className: ""
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: chart-example.local
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local
resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi
autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80
nodeSelector: {}
tolerations: []
affinity: {}
####################============+++++++++++=
anji@master:~$ helm template velpula/
---
# Source: velpula/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-velpula
  labels:
    helm.sh/chart: velpula-0.1.0
    app.kubernetes.io/name: velpula
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: velpula/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-velpula
  labels:
    helm.sh/chart: velpula-0.1.0
    app.kubernetes.io/name: velpula
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: NodePort
  ports:
    - port: 8080
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: velpula
    app.kubernetes.io/instance: release-name
---
# Source: velpula/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-velpula
  labels:
    helm.sh/chart: velpula-0.1.0
    app.kubernetes.io/name: velpula
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: velpula
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      labels:
        app.kubernetes.io/name: velpula
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: release-name-velpula
      securityContext:
        {}
      containers:
        - name: velpula
          securityContext:
            {}
          image: "rahulwagh17/kubernetes:jhooq-k8s-springboot"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          resources:
            {}
---
# Source: velpula/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "release-name-velpula-test-connection"
  labels:
    helm.sh/chart: velpula-0.1.0
    app.kubernetes.io/name: velpula
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args: ['release-name-velpula:8080']
  restartPolicy: Never
--=========++++=
anji@master:~$ helm lint velpula/
==> Linting velpula/
[INFO] Chart.yaml: icon is recommended
1 chart(s) linted, 0 chart(s) failed "
##################################################################################3
anji@master:~$ helm install sony velpula/
NAME: "sony"
LAST DEPLOYED: Wed Jan  4 16:12:21 2023
NAMESPACE: default
STATUS:" deployed"
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
  export NODE_PORT=$(kubectl get --namespace default -o jsonpath="{.spec.ports[0].nodePort}" services sony-velpula)
  export NODE_IP=$(kubectl get nodes --namespace default -o jsonpath="{.items[0].status.addresses[0].address}")
  echo http://$NODE_IP:$NODE_PORT

anji@master:~$ kubectl get all -o wide  --show-labels
NAME                                READY   STATUS    RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES   LABELS
pod/sony-velpula-5cdc66bdd6-nk78r   0/1     Pending   0          17s   <none>   <none>   <none>           <none>            app.kubernetes.io/instance=sony,app.kubernetes.io/name=velpula,pod-template-hash=5cdc66bdd6

NAME                   TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE   SELECTOR                                                         LABELS
service/kubernetes     ClusterIP   10.96.0.1       <none>        443/TCP          54m   <none>                                                           component=apiserver,provider=kubernetes
service/sony-velpula   "NodePort "   10.110.40.140   <none>        "8080:32460/TCP "  17s   app.kubernetes.io/instance=sony,app.kubernetes.io/name=velpula   app.kubernetes.io/instance=sony,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=velpula,app.kubernetes.io/version=1.16.0,helm.sh/chart=velpula-0.1.0

NAME                           READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES                                        SELECTOR                                                         LABELS
deployment.apps/sony-velpula   0/1     1            0           17s   velpula      rahulwagh17/kubernetes:jhooq-k8s-springboot   app.kubernetes.io/instance=sony,app.kubernetes.io/name=velpula   app.kubernetes.io/instance=sony,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=velpula,app.kubernetes.io/version=1.16.0,helm.sh/chart=velpula-0.1.0

NAME                                      DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES                                        SELECTOR                                                                                      LABELS
replicaset.apps/sony-velpula-5cdc66bdd6   1         1         0       17s   velpula      rahulwagh17/kubernetes:jhooq-k8s-springboot   app.kubernetes.io/instance=sony,app.kubernetes.io/name=velpula,pod-template-hash=5cdc66bdd6   app.kubernetes.io/instance=sony,app.kubernetes.io/name=velpula,pod-template-hash=5cdc66bdd6
==========++++++"
anji@master:~$ helm list -a
NAME      	NAMESPACE	REVISION	UPDATED                                	STATUS  	CHART           	APP VERSION
release1-1	default  	3       	2023-01-04 13:50:28.742369186 +0530 IST	deployed	springboot-0.1.0	1.16.0     
sony      	default  	1       	2023-01-04 16:12:21.864548832 +0530 IST	deployed	velpula-0.1.0   	1.16.0     

anji@master:~$ helm delete release1-1  sony    
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/anji/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/anji/.kube/config
release "release1-1" uninstalled
release "sony" uninstalled

anji@master:~$ helm list -a
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/anji/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/anji/.kube/config
NAME	NAMESPACE	REVISION	UPDATED	STATUS	CHART	APP VERSION
#################################################################################################################
"
anji@master:~$ helm create wipro
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/anji/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/anji/.kube/config
Creating wipro
anji@master:~$ tree wipro/
wipro/
â”œâ”€â”€ charts
â”œâ”€â”€ Chart.yaml
â”œâ”€â”€ templates
â”‚Â Â  â”œâ”€â”€ deployment.yaml
â”‚Â Â  â”œâ”€â”€ _helpers.tpl
â”‚Â Â  â”œâ”€â”€ hpa.yaml
â”‚Â Â  â”œâ”€â”€ ingress.yaml
â”‚Â Â  â”œâ”€â”€ NOTES.txt
â”‚Â Â  â”œâ”€â”€ serviceaccount.yaml
â”‚Â Â  â”œâ”€â”€ service.yaml
â”‚Â Â  â””â”€â”€ tests
â”‚Â Â      â””â”€â”€ test-connection.yaml
â””â”€â”€ values.yaml
3 directories, 10 files
========+++ "
anji@master:~/wipro$ cat values.yaml 
# Default values for wipro.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
replicaCount: 1
image:
  repository: nginx
service:
  type: NodePort
  port: 80
=====+++++ "
anji@master:~$ helm lint wipro/
==> Linting wipro/
[INFO] Chart.yaml: icon is recommended
1 chart(s) linted, 0 chart(s) failed
###########"
anji@master:~$ helm list -a 
NAME	NAMESPACE	REVISION	UPDATED	STATUS	CHART	APP VERSION
==----
anji@master:~$ helm install nginx1-1  wipro/
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/anji/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/anji/.kube/config
NAME:" nginx1-1"
LAST DEPLOYED: Wed Jan  4 17:43:49 2023
NAMESPACE: default
STATUS:" deployed"
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
  export NODE_PORT=$(kubectl get --namespace default -o jsonpath="{.spec.ports[0].nodePort}" services nginx1-1-wipro)
  export NODE_IP=$(kubectl get nodes --namespace default -o jsonpath="{.items[0].status.addresses[0].address}")
  echo http://$NODE_IP:$NODE_PORT  
==--+++++= "
anji@master:~$ kubectl get all -o wide --show-labels 
NAME                                  READY   STATUS    RESTARTS   AGE    IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/nginx1-1-wipro-5587c64dfb-2h9xl   1/1     Running   0          112s   10.44.0.1   worker1   <none>           <none>            app.kubernetes.io/instance=nginx1-1,app.kubernetes.io/name=wipro,pod-template-hash=5587c64dfb

NAME                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE    SELECTOR                                                           LABELS
service/kubernetes       ClusterIP   10.96.0.1       <none>        443/TCP        13m    <none>                                                             component=apiserver,provider=kubernetes
service/nginx1-1-wipro   "NodePort"    10.107.65.182   <none>        "80:30711/TCP "  112s   app.kubernetes.io/instance=nginx1-1,app.kubernetes.io/name=wipro   app.kubernetes.io/instance=nginx1-1,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=wipro,app.kubernetes.io/version=1.16.0,helm.sh/chart=wipro-0.1.0

NAME                             READY   UP-TO-DATE   AVAILABLE   AGE    CONTAINERS   IMAGES         SELECTOR                                                           LABELS
deployment.apps/nginx1-1-wipro   1/1     1            1           112s   wipro        nginx:1.16.0   app.kubernetes.io/instance=nginx1-1,app.kubernetes.io/name=wipro   app.kubernetes.io/instance=nginx1-1,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=wipro,app.kubernetes.io/version=1.16.0,helm.sh/chart=wipro-0.1.0

NAME                                        DESIRED   CURRENT   READY   AGE    CONTAINERS   IMAGES         SELECTOR                                                                                        LABELS
replicaset.apps/nginx1-1-wipro-5587c64dfb   1         1         1       112s   wipro       " nginx:1.16.0"   app.kubernetes.io/instance=nginx1-1,app.kubernetes.io/name=wipro,pod-template-hash=5587c64dfb   app.kubernetes.io/instance=nginx1-1,app.kubernetes.io/name=wipro,pod-template-hash=5587c64dfb

anji@master:~$ curl http://192.168.122.172:30711/   ## success 
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
=====++++++++++//
anji@master:~$ helm list -a 
NAME    	NAMESPACE	REVISION	UPDATED                                	STATUS  	CHART      	APP VERSION
nginx1-1	default  	1       	2023-01-04 19:01:14.307709222 +0530 IST	deployed	wipro-0.1.0	1.16.0   
anji@master:~$ helm delete nginx1-1
release "nginx1-1" uninstalled
=============#######################################333
anji@master:~/wipro$ cat values.yaml 
# Default values for wipro.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
replicaCount: 1
anji12image:      === modify data key  see look
  repository: nginx
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: ""
--=====----"
anji@master:~$ helm lint wipro/
==> Linting wipro/
[INFO] Chart.yaml: icon is recommended
[ERROR] templates/: template: wipro/templates/deployment.yaml:34:28: executing "wipro/templates/deployment.yaml" at <.Values.image.repository>: nil pointer evaluating interface {}.repository
Error: 1 chart(s) linted, 1 chart(s) failed

anji@master:~$ helm lint wipro/
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/anji/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/anji/.kube/config
==> Linting wipro/
[INFO] Chart.yaml: icon is recommended
[ERROR] values.yaml: unable to parse YAML: error converting YAML to JSON: yaml: line 41: mapping values are not allowed in this context
[ERROR] templates/: cannot load values.yaml: error converting YAML to JSON: yaml: line 41: mapping values are not allowed in this context
[ERROR] : unable to load chart
	cannot load values.yaml: error converting YAML to JSON: yaml: line 41: mapping values are not allowed in this context
Error: 1 chart(s) linted, 1 chart(s) failed "
##########################################################################################################
https://www.youtube.com/watch?v=VW1JCmBr8H0&list=PL7iMyoQPMtANm_35XWjkNzDCcsw9vy01b&index=9
Helm template | How to use "helm template" command for your helm chart
anji@master:~$ helm create motherboard
Creating motherboard
anji@master:~$ tree motherboard/
motherboard/
â”œâ”€â”€ charts
â”œâ”€â”€ Chart.yaml
â”œâ”€â”€ templates
â”‚Â Â  â”œâ”€â”€ deployment.yaml
â”‚Â Â  â”œâ”€â”€ _helpers.tpl
â”‚Â Â  â”œâ”€â”€ hpa.yaml
â”‚Â Â  â”œâ”€â”€ ingress.yaml
â”‚Â Â  â”œâ”€â”€ NOTES.txt
â”‚Â Â  â”œâ”€â”€ serviceaccount.yaml
â”‚Â Â  â”œâ”€â”€ service.yaml
â”‚Â Â  â””â”€â”€ tests
â”‚Â Â      â””â”€â”€ test-connection.yaml
â””â”€â”€ values.yaml
3 directories, 10 files
+++---=====
anji@master:~$ helm template motherboard/
---
# Source: motherboard/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-motherboard
  labels:
    helm.sh/chart: motherboard-0.1.0
    app.kubernetes.io/name: motherboard
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: motherboard/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-motherboard
  labels:
    helm.sh/chart: motherboard-0.1.0
    app.kubernetes.io/name: motherboard
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: motherboard
    app.kubernetes.io/instance: release-name
---
# Source: motherboard/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-motherboard
  labels:
    helm.sh/chart: motherboard-0.1.0
    app.kubernetes.io/name: motherboard
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: motherboard
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      labels:
        app.kubernetes.io/name: motherboard
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: release-name-motherboard
      securityContext:
        {}
      containers:
        - name: motherboard
          securityContext:
            {}
          image: "nginx:1.16.0"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: http
          readinessProbe:
            httpGet:
              path: /
              port: http
          resources:
            {}
---
# Source: motherboard/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "release-name-motherboard-test-connection"
  labels:
    helm.sh/chart: motherboard-0.1.0
    app.kubernetes.io/name: motherboard
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args: ['release-name-motherboard:80']
  restartPolicy: Never
==++++++++++"
anji@master:~$ helm install  intel motherboard/
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/anji/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/anji/.kube/config
NAME: intel
LAST DEPLOYED: Wed Jan  4 19:52:40 2023
NAMESPACE: default
STATUS: deployed
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=motherboard,app.kubernetes.io/instance=intel" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT
anji@master:~$ helm list -a 
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/anji/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/anji/.kube/config
NAME 	NAMESPACE	REVISION	UPDATED                                	STATUS  	CHART            	APP VERSION
intel	default  	1       	2023-01-04 19:52:40.166847269 +0530 IST	deployed	motherboard-0.1.0	1.16.0     " 

anji@master:~$ cd motherboard/
anji@master:~/motherboard$ ls
charts  Chart.yaml  templates  values.yaml
anji@master:~/motherboard$ cd templates/
anji@master:~/motherboard/templates$ ls
deployment.yaml  _helpers.tpl  hpa.yaml  ingress.yaml  NOTES.txt  serviceaccount.yaml  service.yaml  tests

anji@master:~$ helm install red motherboard/
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/anji/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/anji/.kube/config
NAME: red
LAST DEPLOYED: Wed Jan  4 20:08:36 2023
NAMESPACE: default
STATUS: deployed
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=motherboard,app.kubernetes.io/instance=red" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT

anji@master:~/motherboard/templates$  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
anji@master:~/motherboard/templates$   echo "Visit http://127.0.0.1:8080 to use your application"
Visit http://127.0.0.1:8080 to use your application
anji@master:~/motherboard/templates$   kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT
Forwarding from 127.0.0.1:8080 -> 80
Forwarding from [::1]:8080 -> 80
Handling connection for 8080

anji@master:~$ curl http://127.0.0.1:8080
+++++++++++++++++++###################################################
"
https://www.youtube.com/watch?v=gb5nYiWAIUs&list=PL7iMyoQPMtANm_35XWjkNzDCcsw9vy01b&index=10
How to use "helm --debug --dry-run" command to validate and verify kubernetes resources
https://jhooq.com/helm-dry-run-install/

anji@master:~$ helm create samsung 
WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /home/anji/.kube/config
WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /home/anji/.kube/config
Creating samsung

anji@master:~$ ls 
Desktop    Downloads                        linux-amd64  Pictures  samsung    Videos
Documents  helm-v3.10.3-linux-amd64.tar.gz  Music        Public    Templates

anji@master:~$ cd samsung/
anji@master:~/samsung$ tree
.
â”œâ”€â”€ charts
â”œâ”€â”€ Chart.yaml
â”œâ”€â”€ templates
â”‚Â Â  â”œâ”€â”€ deployment.yaml
â”‚Â Â  â”œâ”€â”€ _helpers.tpl
â”‚Â Â  â”œâ”€â”€ hpa.yaml
â”‚Â Â  â”œâ”€â”€ ingress.yaml
â”‚Â Â  â”œâ”€â”€ NOTES.txt
â”‚Â Â  â”œâ”€â”€ serviceaccount.yaml
â”‚Â Â  â”œâ”€â”€ service.yaml
â”‚Â Â  â””â”€â”€ tests
â”‚Â Â      â””â”€â”€ test-connection.yaml
â””â”€â”€ values.yaml
3 directories, 10 files
==---++++-- "
anji@master:~$  helm install apple  --debug --dry-run samsung/    #  see look importent
install.go:192: [debug] Original chart version: ""
install.go:209: [debug] CHART PATH: /home/anji/samsung
NAME: apple
LAST DEPLOYED: Wed Jan  4 20:35:08 2023
NAMESPACE: default
STATUS: pending-install
REVISION: 1
USER-SUPPLIED VALUES:
{}

COMPUTED VALUES:
affinity: {}
autoscaling:
  enabled: false
  maxReplicas: 100
  minReplicas: 1
  targetCPUUtilizationPercentage: 80
fullnameOverride: ""
image:
  pullPolicy: IfNotPresent
  repository: nginx
  tag: ""
imagePullSecrets: []
ingress:
  annotations: {}
  className: ""
  enabled: false
  hosts:
  - host: chart-example.local
    paths:
    - path: /
      pathType: ImplementationSpecific
  tls: []
nameOverride: ""
nodeSelector: {}
podAnnotations: {}
podSecurityContext: {}
replicaCount: 1
resources: {}
securityContext: {}
service:
  port: 80
  type: ClusterIP
serviceAccount:
  annotations: {}
  create: true
  name: ""
tolerations: []

HOOKS:
---
# Source: samsung/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "apple-samsung-test-connection"
  labels:
    helm.sh/chart: samsung-0.1.0
    app.kubernetes.io/name: samsung
    app.kubernetes.io/instance: apple
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args: ['apple-samsung:80']
  restartPolicy: Never
MANIFEST:
---
# Source: samsung/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: apple-samsung
  labels:
    helm.sh/chart: samsung-0.1.0
    app.kubernetes.io/name: samsung
    app.kubernetes.io/instance: apple
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: samsung/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: apple-samsung
  labels:
    helm.sh/chart: samsung-0.1.0
    app.kubernetes.io/name: samsung
    app.kubernetes.io/instance: apple
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: samsung
    app.kubernetes.io/instance: apple
---
# Source: samsung/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: apple-samsung
  labels:
    helm.sh/chart: samsung-0.1.0
    app.kubernetes.io/name: samsung
    app.kubernetes.io/instance: apple
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: samsung
      app.kubernetes.io/instance: apple
  template:
    metadata:
      labels:
        app.kubernetes.io/name: samsung
        app.kubernetes.io/instance: apple
    spec:
      serviceAccountName: apple-samsung
      securityContext:
        {}
      containers:
        - name: samsung
          securityContext:
            {}
          image: "nginx:1.16.0"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: http
          readinessProbe:
            httpGet:
              path: /
              port: http
          resources:
            {}

NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=samsung,app.kubernetes.io/instance=apple" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT

anji@master:~$ helm list -a
NAME	NAMESPACE	REVISION	UPDATED	STATUS	CHART	APP VERSION

##############################################################################################
"How to use/pull environment variables into Helm Chart
https://www.youtube.com/watch?v=dKKMJseMiGU&list=PL7iMyoQPMtANm_35XWjkNzDCcsw9vy01b&index=12

https://jhooq.com/helm-pass-environment-variables/

anji@master:~$ helm  create cpu
Creating cpu

anji@master:~$ cd cpu/
anji@master:~/cpu$ ls
charts  Chart.yaml  templates  values.yaml
----
anji@master:~$ helm install ssd cpu
NAME: ssd
LAST DEPLOYED: Thu Jan  5 11:09:24 2023
NAMESPACE: default
STATUS: deployed
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=cpu,app.kubernetes.io/instance=ssd" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT

anji@master:~$ helm list -a 
NAME	NAMESPACE	REVISION	UPDATED                                	STATUS  	CHART    	APP VERSION
ssd 	default  	1       	2023-01-05 11:09:24.442124942 +0530 IST	deployed	cpu-0.1.0	1.16.0     

anji@master:~$ kubectl get pod,deployment  -o wide  --show-labels
NAME                           READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/ssd-cpu-5dfc6bd7d7-795n8   1/1     Running   0          27s   10.44.0.1   worker1   <none>           <none>            app.kubernetes.io/instance=ssd,app.kubernetes.io/name=cpu,pod-template-hash=5dfc6bd7d7

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES         SELECTOR                                                    LABELS
deployment.apps/ssd-cpu   1/1     1            1           27s   cpu          nginx:1.16.0   app.kubernetes.io/instance=ssd,app.kubernetes.io/name=cpu   app.kubernetes.io/instance=ssd,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=cpu,app.kubernetes.io/version=1.16.0,helm.sh/chart=cpu-0.1.0
===="

anji@master:~$ helm install --set replicaCount=2  ssd cpu/
Error: INSTALLATION FAILED: cannot re-use a name that is still in use

anji@master:~$ helm install --set replicaCount=2 ssd1 cpu/
NAME: ssd1
LAST DEPLOYED: Thu Jan  5 11:17:42 2023
NAMESPACE: default
STATUS: "deployed"
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=cpu,app.kubernetes.io/instance=ssd1" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT

anji@master:~$ helm list 
NAME	NAMESPACE	REVISION	UPDATED                                	STATUS  	CHART    	APP VERSION
ssd 	default  	1       	2023-01-05 11:09:24.442124942 +0530 IST	deployed	cpu-0.1.0	1.16.0     
ssd1	default  	1       	2023-01-05 11:17:42.044590608 +0530 IST	deployed	cpu-0.1.0	1.16.0     "
anji@master:~$ kubectl get pod,deployment  -o wide  --show-labels
NAME                            READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/ssd-cpu-5dfc6bd7d7-795n8    1/1     Running   0          12m     10.44.0.1   worker1   <none>           <none>            app.kubernetes.io/instance=ssd,app.kubernetes.io/name=cpu,pod-template-hash=5dfc6bd7d7
pod/ssd1-cpu-7666c44784-cms6z   1/1     Running   0          3m43s   10.44.0.3   worker1   <none>           <none>            app.kubernetes.io/instance=ssd1,app.kubernetes.io/name=cpu,pod-template-hash=7666c44784
pod/ssd1-cpu-7666c44784-r6xp6   1/1     Running   0          3m43s   10.44.0.2   worker1   <none>           <none>            app.kubernetes.io/instance=ssd1,app.kubernetes.io/name=cpu,pod-template-hash=7666c44784

NAME                       READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS   IMAGES         SELECTOR                                                     LABELS
deployment.apps/ssd-cpu    1/1     1            1           12m     cpu          nginx:1.16.0   app.kubernetes.io/instance=ssd,app.kubernetes.io/name=cpu    app.kubernetes.io/instance=ssd,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=cpu,app.kubernetes.io/version=1.16.0,helm.sh/chart=cpu-0.1.0
deployment.apps/ssd1-cpu   2/2     2            2           3m43s   cpu          nginx:1.16.0   app.kubernetes.io/instance=ssd1,app.kubernetes.io/name=cpu   app.kubernetes.io/instance=ssd1,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=cpu,app.kubernetes.io/version=1.16.0,helm.sh/chart=cpu-0.1.0
"""
anji@master:~$ helm install  --set replicaCount=1   --set replicaCount=3  ap1  india/ 
NAME: ap1
LAST DEPLOYED: Thu Jan  5 12:46:48 2023
NAMESPACE: default
STATUS: deployed
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=india,app.kubernetes.io/instance=ap1" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT "
====\\\//\//\/\//
anji@master:~$ kubectl get pod,deployment  -o wide  --show-labels
NAME                             READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/ap-india-f7dcc47b-tfj49      1/1     Running   0          47m   10.44.0.1   worker1   <none>           <none>            app.kubernetes.io/instance=ap,app.kubernetes.io/name=india,pod-template-hash=f7dcc47b
pod/ap1-india-7b69dffb47-jz98f   1/1     Running   0          6s    10.44.0.3   worker1   <none>           <none>            app.kubernetes.io/instance=ap1,app.kubernetes.io/name=india,pod-template-hash=7b69dffb47
pod/ap1-india-7b69dffb47-mbp4q   1/1     Running   0          6s    10.44.0.2   worker1   <none>           <none>            app.kubernetes.io/instance=ap1,app.kubernetes.io/name=india,pod-template-hash=7b69dffb47
pod/ap1-india-7b69dffb47-w7h9w   1/1     Running   0          6s    10.44.0.4   worker1   <none>           <none>            app.kubernetes.io/instance=ap1,app.kubernetes.io/name=india,pod-template-hash=7b69dffb47

NAME                        READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES         SELECTOR                                                      LABELS
deployment.apps/ap-india    1/1     1            1           47m   india        nginx:1.16.0   app.kubernetes.io/instance=ap,app.kubernetes.io/name=india    app.kubernetes.io/instance=ap,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=india,app.kubernetes.io/version=1.16.0,helm.sh/chart=india-0.1.0
deployment.apps/ap1-india   3/3     3            3           6s    india        nginx:1.16.0   app.kubernetes.io/instance=ap1,app.kubernetes.io/name=india   app.kubernetes.io/instance=ap1,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=india,app.kubernetes.io/version=1.16.0,helm.sh/chart=india-0.1.0

============##########+++++++++++++++++++++++++++=="
nano myvalues.yaml 
  replicaCount: 3
anji@master:~$ ls
Desktop    Downloads                        laptop       Music          Pictures  Public     Videos
Documents  helm-v3.10.3-linux-amd64.tar.gz  linux-amd64  "myvalues.yaml " project   Templates

anji@master:~$ helm install -f myvalues.yaml  release1-1 project/
NAME: release1-1
LAST DEPLOYED: Thu Jan  5 13:07:53 2023
NAMESPACE: default
STATUS: "deployed"
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=project,app.kubernetes.io/instance=release1-1" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT

nji@master:~$ kubectl get pod,deployment  -o wide  --show-labels
NAME                                     READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/release1-1-project-584d48d5f-5twgp   1/1     Running   0          8s    10.44.0.2   worker1   <none>           <none>            app.kubernetes.io/instance=release1-1,app.kubernetes.io/name=project,pod-template-hash=584d48d5f
pod/release1-1-project-584d48d5f-j7rvd   1/1     Running   0          8s    10.44.0.1   worker1   <none>           <none>            app.kubernetes.io/instance=release1-1,app.kubernetes.io/name=project,pod-template-hash=584d48d5f
pod/release1-1-project-584d48d5f-zkmbv   1/1     Running   0          8s    10.44.0.3   worker1   <none>           <none>            app.kubernetes.io/instance=release1-1,app.kubernetes.io/name=project,pod-template-hash=584d48d5f

NAME                                 READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES         SELECTOR                                                               LABELS
deployment.apps/release1-1-project   "3/3 "    3            3           8s    project      nginx:1.16.0   app.kubernetes.io/instance=release1-1,app.kubernetes.io/name=project   app.kubernetes.io/instance=release1-1,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=project,app.kubernetes.io/version=1.16.0,helm.sh/chart=project-0.1.0
===+++++++++++""
anji@master:~$ helm create gold 
anji@master:~$ ls
Desktop    Downloads  helm-v3.10.3-linux-amd64.tar.gz  Music          Pictures  Public     Videos
Documents  "gold "      linux-amd64                     project   Templates

anji@master:~$ cat env-values.yaml 
examplemap:
 - name: "USERNAME"
   value: "test1"
 - name: "PASSWORD"
   value: "test2"
++\\\/// ///// BEFORE   BEFORE  ""
"anji@master:~$ ls
Desktop    Downloads        gold                             linux-amd64  Pictures  Public     Videos
Documents  env-values.yaml  helm-v3.10.3-linux-amd64.tar.gz  Music        project   Templates

anji@master:~$ cat gold/
charts/      Chart.yaml   .helmignore  templates/   values.yaml  

anji@master:~$ cat gold/templates/
deployment.yaml      hpa.yaml             NOTES.txt            service.yaml         
_helpers.tpl         ingress.yaml         serviceaccount.yaml  tests/          "

anji@master:~$ cat gold/templates/deployment.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "gold.fullname" . }}
  labels:
    {{- include "gold.labels" . | nindent 4 }}
spec:
  {{- if not .Values.autoscaling.enabled }}
  replicas: {{ .Values.replicaCount }}
  {{- end }}
  selector:
    matchLabels:
      {{- include "gold.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      {{- with .Values.podAnnotations }}
      annotations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      labels:
        {{- include "gold.selectorLabels" . | nindent 8 }}
    spec:
      {{- with .Values.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      serviceAccountName: {{ include "gold.serviceAccountName" . }}
      securityContext:
        {{- toYaml .Values.podSecurityContext | nindent 8 }}
      containers:
        - name: {{ .Chart.Name }}
          securityContext:
            {{- toYaml .Values.securityContext | nindent 12 }}
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}     ##  see look   modify 
          ports:
            - name: http
              containerPort: {{ .Values.service.port }}
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: http
          readinessProbe:
            httpGet:
              path: /
              port: http
          resources:
            {{- toYaml .Values.resources | nindent 12 }}
      {{- with .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
###################+++
anji@master:~$ cat gold/templates/deployment.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
           image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }} 
          env:                          ###    =-   see look  
            {{- range .Values.examplemap }}
            - name: {{ .name }}
              value: {{ .value }}
            {{- end }} 
          ports: "
---======
anji@master:~$ helm template -f env-values.yaml  gold/
---
# Source: gold/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:

---
# Source: gold/templates/service.yaml
apiVersion: v1
kind: Service
metadata:

---
# Source: gold/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment

        {}
      containers:
        - name: gold
          securityContext:
            {}
          image: "nginx:1.16.0"
          imagePullPolicy: IfNotPresent 
          env:
            - name: USERNAME     ## "see look "
              value: test1
            - name: PASSWORD
              value: test2 

            {}
---
# Source: gold/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "release-name-gold-test-connection"

    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args: ['release-name-gold:80']
  restartPolicy: Never
=====++++"
anji@master:~$ helm install -f env-values.yaml release1 gold/
NAME: release1
LAST DEPLOYED: Thu Jan  5 14:10:33 2023
NAMESPACE: default
STATUS: "deployed"
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=gold,app.kubernetes.io/instance=release1" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT "

anji@master:~$ kubectl get pod,deployment  -o wide  --show-labels
NAME                                 READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/release1-gold-7f4d4cc84b-hk8mb   1/1     Running   0          3m24s   10.44.0.1   worker1   <none>           <none>            app.kubernetes.io/instance=release1,app.kubernetes.io/name=gold,pod-template-hash=7f4d4cc84b

NAME                            READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS   IMAGES         SELECTOR                                                          LABELS
deployment.apps/release1-gold   1/1     1            1           3m24s   gold         nginx:1.16.0   app.kubernetes.io/instance=release1,app.kubernetes.io/name=gold   app.kubernetes.io/instance=release1,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=gold,app.kubernetes.io/version=1.16.0,helm.sh/chart=gold-0.1.0

anji@master:~$ kubectl describe pod release1-gold-7f4d4cc84b-hk8mb 
Name:             release1-gold-7f4d4cc84b-hk8mb
Labels:           app.kubernetes.io/instance=release1
                  app.kubernetes.io/name=gold
                  pod-template-hash=7f4d4cc84b
IPs:
  IP:           10.44.0.1
Controlled By:  ReplicaSet/release1-gold-7f4d4cc84b
Containers:
  gold:
    Image:          nginx:1.16.0
    Liveness:       http-get http://:http/ delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:      http-get http://:http/ delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
   "   USERNAME:  test1
      PASSWORD:  test2"
####################################################################################################################
Helm Upgrade Command | How to use Helm Upgrade Command - Part 12
https://www.youtube.com/watch?v=6ua2LBzPfxo&list=PL7iMyoQPMtANm_35XWjkNzDCcsw9vy01b&index=13
https://jhooq.com/building-first-helm-chart-with-spring-boot/#8-upgrade-helm-release
8. Upgrade helm release "
anji@master:~$ helm create jio 
Creating jio

anji@master:~$ ls
Desktop  Documents  Downloads  gold  helm-v3.10.3-linux-amd64.tar.gz  jio  linux-amd64  Music  Pictures  project  Public  Templates  Videos

anji@master:~$ tree jio 
jio
â”œâ”€â”€ charts
â”œâ”€â”€ Chart.yaml
â”œâ”€â”€ templates
â”‚Â Â  â”œâ”€â”€ deployment.yaml
â”‚Â Â  â”œâ”€â”€ _helpers.tpl
â”‚Â Â  â”œâ”€â”€ hpa.yaml
â”‚Â Â  â”œâ”€â”€ ingress.yaml
â”‚Â Â  â”œâ”€â”€ NOTES.txt
â”‚Â Â  â”œâ”€â”€ serviceaccount.yaml
â”‚Â Â  â”œâ”€â”€ service.yaml
â”‚Â Â  â””â”€â”€ tests
â”‚Â Â      â””â”€â”€ test-connection.yaml
â””â”€â”€ values.yaml
3 directories, 10 files
====|||||||||||||||||//////////\\\\\ 
"anji@master:~$ helm install aoc jio/
NAME: aoc
LAST DEPLOYED: Thu Jan  5 14:34:25 2023
NAMESPACE: default
STATUS: deployed
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=jio,app.kubernetes.io/instance=aoc" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT

"anji@master:~$ kubectl get pod,deployment  -o wide  --show-labels
NAME                          READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/aoc-jio-9d9cbcc78-cw7vn   1/1     Running   0          98s   10.44.0.1   worker1   <none>           <none>            app.kubernetes.io/instance=aoc,app.kubernetes.io/name=jio,pod-template-hash=9d9cbcc78

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES         SELECTOR                                                    LABELS
deployment.apps/aoc-jio   1/1     1            1           98s   jio          nginx:1.16.0   app.kubernetes.io/instance=aoc,app.kubernetes.io/name=jio   app.kubernetes.io/instance=aoc,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=jio,app.kubernetes.io/version=1.16.0,helm.sh/chart=jio-0.1.0
" anji@master:~$ cd jio/
anji@master:~/jio$ ls
charts  Chart.yaml  templates  values.yaml

anji@master:~/jio$ nano values.yaml 
# Default values for jio.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
replicaCount: 2
image:
===-----"
anji@master:~$ helm upgrade aoc  jio/
Release "aoc" has been upgraded. Happy Helming!
NAME: "aoc"
LAST DEPLOYED: Thu Jan  5 15:12:46 2023
NAMESPACE: default
STATUS: deployed
REVISION: "2"
NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=jio,app.kubernetes.io/instance=aoc" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT "

anji@master:~$ helm list 
NAME	NAMESPACE	REVISION	UPDATED                                	STATUS  	CHART    	APP VERSION
aoc 	default  	2       	2023-01-05 15:12:46.709636937 +0530 IST	deployed	jio-0.1.0	1.16.0     
++=="
anji@master:~$ kubectl get pod,deployment  -o wide  --show-labels
NAME                          READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/aoc-jio-9d9cbcc78-cw7vn   1/1     Running   0          38m   10.44.0.1   worker1   <none>           <none>            app.kubernetes.io/instance=aoc,app.kubernetes.io/name=jio,pod-template-hash=9d9cbcc78
pod/aoc-jio-9d9cbcc78-xj6v4   1/1     Running   0          31s   10.44.0.2   worker1   <none>           <none>            app.kubernetes.io/instance=aoc,app.kubernetes.io/name=jio,pod-template-hash=9d9cbcc78

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES         SELECTOR                                                    LABELS
deployment.apps/aoc-jio   2/2     2            2           38m   jio          nginx:1.16.0   app.kubernetes.io/instance=aoc,app.kubernetes.io/name=jio   app.kubernetes.io/instance=aoc,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=jio,app.kubernetes.io/version=1.16.0,helm.sh/chart=jio-0.1.0
#################################################################################################################################
"9. Rollback Helm release
Helm Rollback Command | How to use Helm Rollback Command - Part 13
https://jhooq.com/building-first-helm-chart-with-spring-boot/#9-rollback-helm-release
anji@master:~/jio$ cd
anji@master:~$ helm create book 
Creating book

anji@master:~$ ls
book  Desktop  Documents  Downloads  linux-amd64  Music  Pictures  project  Public  Templates  Videos

anji@master:~$ helm install version1 book/
NAME: version1
LAST DEPLOYED: Thu Jan  5 16:10:54 2023
NAMESPACE: default
STATUS: deployed
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=book,app.kubernetes.io/instance=version1" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT

anji@master:~$ helm list -a 
NAME    	NAMESPACE	REVISION	UPDATED                                	STATUS  	CHART     	APP VERSION
version1	default  	"1"       	2023-01-05 16:10:54.409921948 +0530 IST	deployed	book-0.1.0	1.16.0     

anji@master:~$ kubectl get pod,deployment  -o wide  --show-labels
NAME                                 READY   STATUS    RESTARTS   AGE     IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/version1-book-76d96ffb74-dn7gp   1/1     Running   0          8m54s   10.44.0.1   worker1   <none>           <none>            app.kubernetes.io/instance=version1,app.kubernetes.io/name=book,pod-template-hash=76d96ffb74

NAME                            READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS   IMAGES         SELECTOR                                                          LABELS
deployment.apps/version1-book   1/1     1            1           8m54s   book         nginx:1.16.0   app.kubernetes.io/instance=version1,app.kubernetes.io/name=book   app.kubernetes.io/instance=version1,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=book,app.kubernetes.io/version=1.16.0,helm.sh/chart=book-0.1.0
=======++++++++--------"
anji@master:~$ nano book/values.yaml 
anji@master:~$ cat book/values.yaml 
# Default values for book.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
replicaCount: 5 "
---
anji@master:~$ helm upgrade version1 book/
Release "version1" has been upgraded. Happy Helming!
NAME: version1
LAST DEPLOYED: Thu Jan  5 16:24:05 2023
NAMESPACE: default
STATUS: deployed
REVISIO"N: 2"
NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=book,app.kubernetes.io/instance=version1" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT

anji@master:~$ helm list 
NAME    	NAMESPACE	REVISION	UPDATED                                	STATUS  	CHART     	APP VERSION
version1	default  	"2 "      	2023-01-05 16:24:05.600255414 +0530 IST	deployed	book-0.1.0	1.16.0     

anji@master:~$ kubectl get pod,deployment  -o wide  --show-labels
NAME                                 READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/version1-book-76d96ffb74-9mgvk   1/1     Running   0          14s   10.44.0.5   worker1   <none>           <none>            app.kubernetes.io/instance=version1,app.kubernetes.io/name=book,pod-template-hash=76d96ffb74
pod/version1-book-76d96ffb74-dn7gp   1/1     Running   0          13m   10.44.0.1   worker1   <none>           <none>            app.kubernetes.io/instance=version1,app.kubernetes.io/name=book,pod-template-hash=76d96ffb74
pod/version1-book-76d96ffb74-hllvz   1/1     Running   0          14s   10.44.0.4   worker1   <none>           <none>            app.kubernetes.io/instance=version1,app.kubernetes.io/name=book,pod-template-hash=76d96ffb74
pod/version1-book-76d96ffb74-kncbn   1/1     Running   0          14s   10.44.0.2   worker1   <none>           <none>            app.kubernetes.io/instance=version1,app.kubernetes.io/name=book,pod-template-hash=76d96ffb74
pod/version1-book-76d96ffb74-m8bfp   1/1     Running   0          14s   10.44.0.3   worker1   <none>           <none>            app.kubernetes.io/instance=version1,app.kubernetes.io/name=book,pod-template-hash=76d96ffb74

NAME                            READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES         SELECTOR                                                          LABELS
deployment.apps/version1-book  " 5/5    " 5            5           13m   book         nginx:1.16.0   app.kubernetes.io/instance=version1,app.kubernetes.io/name=book   app.kubernetes.io/instance=version1,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=book,app.kubernetes.io/version=1.16.0,helm.sh/chart=book-0.1.0
"anji@master:~$ helm list 
NAME    	NAMESPACE	REVISION	UPDATED                                	STATUS  	CHART     	APP VERSION
version1	default  	"2   "    	2023-01-05 16:24:05.600255414 +0530 IST	deployed	book-0.1.0	1.16.0     

anji@master:~$ helm rollback version1 1
Rollback was a success! Happy Helming!

anji@master:~$ helm list 
NAME    	NAMESPACE	REVISION	UPDATED                                	STATUS  	CHART     	APP VERSION
version1	default  "	3 "      	2023-01-05 16:29:09.061914373 +0530 IST	deployed	book-0.1.0	1.16.0     

anji@master:~$ kubectl get pod,deployment  -o wide  --show-labels
NAME                                 READY   STATUS    RESTARTS   AGE   IP          NODE      NOMINATED NODE   READINESS GATES   LABELS
pod/version1-book-76d96ffb74-dn7gp   1/1     Running   0          18m   10.44.0.1   worker1   <none>           <none>            app.kubernetes.io/instance=version1,app.kubernetes.io/name=book,pod-template-hash=76d96ffb74

NAME                            READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES         SELECTOR                                                          LABELS
deployment.apps/version1-book   "1/1  "   1            1           18m   book         nginx:1.16.0   app.kubernetes.io/instance=version1,app.kubernetes.io/name=book   app.kubernetes.io/instance=version1,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=book,app.kubernetes.io/version=1.16.0,helm.sh/chart=book-0.1.0
"
#####################################################################################################################################
https://linuxhostsupport.com/blog/how-to-install-grafana-on-ubuntu-20-04/
How to Install Grafana on Ubuntu 20.04

root@grafana:~# apt-get install wget curl gnupg2 apt-transport-https software-properties-common -y

root@grafana:~# wget -q -O - https://packages.grafana.com/gpg.key | apt-key add -

root@grafana:~# echo "deb https://packages.grafana.com/oss/deb stable main" | tee -a /etc/apt/sources.list.d/grafana.list

apt-get update -y

apt-get install grafana -y


 root@grafana:~# grafana-server -v
   Version 9.3.2 (commit: 21c1d14e91, branch: HEAD)

systemctl start grafana-server
systemctl enable grafana-server

systemctl status grafana-server

root@grafana:~# ss -antpl | grep 3000
LISTEN    0         4096                     *:3000                   *:*        users:(("grafana-server",pid=4279,fd=9)) 

http://localhost:3000/login

################   or  or  or  second method {}{}
" https://grafana.com/grafana/download?pg=get&plcmt=selfmanaged-box1-cta1

root@anji:~# wget https://dl.grafana.com/enterprise/release/grafana-enterprise-9.3.2.linux-amd64.tar.gz
root@anji:~# tar -zxvf grafana-enterprise-9.3.2.linux-amd64.tar.gz
root@anji:~# tar -zxvf grafana-enterprise-9.3.2.linux-amd64.tar.gz
root@anji:~# cd grafana-9.3.2/
root@anji:~/grafana-9.3.2# ls
bin  conf  LICENSE  NOTICE.md  plugins-bundled  public  README.md  scripts  VERSION
root@anji:~/grafana-9.3.2# cd bin/
root@anji:~/grafana-9.3.2/bin# la
grafana-cli  grafana-cli.md5  grafana-server  grafana-server.md5
root@anji:~/grafana-9.3.2/bin# ./grafana-server 


++++++++++=============+++++++==================================//////\\\
Configure Nginx as a Reverse Proxy for Grafana "
Next, you will need to install the Nginx as a reverse proxy for Grafana. First, install the Nginx package using the following command:
apt-get install nginx -y
nano /etc/nginx/conf.d/grafana.conf

root@grafana:~# cat  /etc/nginx/conf.d/grafana.conf
Server {
        server_name grafana.example.com;
        listen 80 ;
        access_log /var/log/nginx/grafana.log;

    location / {
                proxy_pass http://localhost:3000;
        proxy_set_header Host $http_host;
                proxy_set_header X-Forwarded-Host $host:$server_port;
                proxy_set_header X-Forwarded-Server $host;
                proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        }
}
-----------------------
nginx -t
systemctl restart nginx
"
########################################################################################
How to Install Prometheus on Ubuntu 20.04
https://linuxopsys.com/topics/install-prometheus-on-ubuntu

sudo mkdir -p /etc/prometheus
sudo mkdir -p /var/lib/prometheus

https://prometheus.io/download/

https://github.com/prometheus/prometheus/releases/download/v2.41.0/prometheus-2.41.0.linux-amd64.tar.gz

root@grafana:~# tar -xvzf prometheus-2.41.0.linux-amd64.tar.gz 

root@grafana:~# cd prometheus-2.41.0.linux-amd64/

root@grafana:~/prometheus-2.41.0.linux-amd64# ls
console_libraries  consoles  LICENSE  NOTICE  prometheus  prometheus.yml  promtool
root@anji:~/prometheus-2.41.0.linux-amd64# ./prometheus 
     ##\\ || second model == under --------=====================-------------------------//////////

root@grafana:~/prometheus-2.41.0.linux-amd64# sudo mv prometheus promtool /usr/local/bin/

root@grafana:~/prometheus-2.41.0.linux-amd64# mkdir -p  /etc/prometheus/
root@grafana:~/prometheus-2.41.0.linux-amd64# sudo mv consoles/ console_libraries/ /etc/prometheus/

root@grafana:~/prometheus-2.41.0.linux-amd64# sudo mv prometheus.yml /etc/prometheus/prometheus.yml

root@grafana:~# prometheus --version
prometheus, version 2.41.0 (branch: HEAD, revision: c0d8a56c69014279464c0e15d8bfb0e153af0dab)
  build user:       root@d20a03e77067
  build date:       20221220-10:40:45
  go version:       go1.19.4
  platform:         linux/amd64
-----
root@grafana:~# promtool --version
promtool, version 2.41.0 (branch: HEAD, revision: c0d8a56c69014279464c0e15d8bfb0e153af0dab)
  build user:       root@d20a03e77067
  build date:       20221220-10:40:45
  go version:       go1.19.4
  platform:         linux/amd64
-----
root@grafana:~# sudo groupadd --system prometheus

root@grafana:~# sudo useradd -s /sbin/nologin --system -g prometheus prometheus

root@grafana:~# sudo chown -R prometheus:prometheus /etc/prometheus/ /var/lib/prometheus/

root@grafana:~# sudo chmod -R 775 /etc/prometheus/ /var/lib/prometheus/

root@grafana:~# sudo nano /etc/systemd/system/prometheus.service
root@grafana:~# cat  nano /etc/systemd/system/prometheus.service
cat: nano: No such file or directory
[Unit]
Description=Prometheus
Wants=network-online.target
After=network-online.target

[Service]
User=prometheus
Group=prometheus
Restart=always
Type=simple
ExecStart=/usr/local/bin/prometheus \
    --config.file=/etc/prometheus/prometheus.yml \
    --storage.tsdb.path=/var/lib/prometheus/ \
    --web.console.templates=/etc/prometheus/consoles \
    --web.console.libraries=/etc/prometheus/console_libraries \
    --web.listen-address=0.0.0.0:9090

[Install]
WantedBy=multi-user.target
---------
sudo systemctl start prometheus
sudo systemctl enable prometheus
sudo systemctl status prometheus

root@grafana:~# sudo ufw allow 9090/tcp
Rules updated
Rules updated (v6)

root@grafana:~# systemctl start firewalld
root@grafana:~# sudo ufw allow 9090/tcp
Skipping adding existing rule
Skipping adding existing rule (v6)
http://localhost:9090/                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    
################################################################################3
https://jhooq.com/prometheous-grafan-setup/

http://localhost:9090/metrics

2.1 How to Install Node exporter
After installing the Prometheus in the previous step the next package we are going to install is **Node Exporter **. Node exported is used for collecting various hardware and kernel-level metrics of your machine.

https://prometheus.io/download/#node_exporter
root@anji:~# wget https://github.com/prometheus/node_exporter/releases/download/v1.5.0/node_exporter-1.5.0.linux-amd64.tar.gz

root@anji:~# tar -xvzf node_exporter-1.5.0.linux-amd64.tar.gz 
node_exporter-1.5.0.linux-amd64/
node_exporter-1.5.0.linux-amd64/LICENSE
node_exporter-1.5.0.linux-amd64/NOTICE
node_exporter-1.5.0.linux-amd64/node_exporter

root@anji:~# cd node_exporter-1.5.0.linux-amd64

root@anji:~/node_exporter-1.5.0.linux-amd64# ls
LICENSE  node_exporter  NOTICE

root@anji:~/node_exporter-1.5.0.linux-amd64# ./node_exporter 
ts=2023-01-05T13:38:16.088Z caller=node_exporter.go:180 level=info msg="Starting node_exporter" version="(version=1.5.0, branch=HEAD, revision=1b48970ffcf5630534fb00bb0687d73c66d1c959)"
ts=2023-01-05T13:38:16.088Z caller=node_exporter.go:181 level=info msg="Build context" build_context="(go=go1.19.3, user=root@6e7732a7b81b, date=20221129-18:59:09)"
ts=2023-01-05T13:38:16.095Z caller=node_exporter.go:117 level=info collector=zfs
ts=2023-01-05T13:38:16.096Z caller=tls_config.go:232 level=info msg="Listening on" "address=[::]:9100"
ts=2023-01-05T13:38:16.096Z caller=tls_config.go:235 level=info msg="TLS is disabled." http2=false address=[::]:9100

http://192.168.122.205:9100/
http://192.168.122.205:9100/metrics

root@worker1:/etc/kubernetes# export PS1='$'
$
$

https://prometheus.io/docs/guides/node-exporter/

global:
  scrape_interval: 15s

scrape_configs:
- job_name: node
  static_configs:
  - targets: ['localhost:9100']
root@anji:~# nano exporter-config.yaml 
root@anji:~# cat exporter-config.yaml 
global:
  scrape_interval: 15s

scrape_configs:
- job_name: node
  static_configs:
  - targets: ['localhost:9100']
---====
 root@anji:~# ls
exporter-config.yaml             node_exporter-1.5.0.linux-amd64.tar.gz  snap
git                              prometheus-2.41.0.linux-amd64
node_exporter-1.5.0.linux-amd64  prometheus-2.41.0.linux-amd64.tar.gz

root@anji:~# cd prometheus-2.41.0.linux-amd64/
root@anji:~/prometheus-2.41.0.linux-amd64# ls
console_libraries  consoles  LICENSE  NOTICE  prometheus  prometheus.yml  promtool

root@anji:~/prometheus-2.41.0.linux-amd64# cp /root/
.bash_history                           node_exporter-1.5.0.linux-amd64/
.bashrc                                 node_exporter-1.5.0.linux-amd64.tar.gz
.cache/                                 .profile
exporter-config.yaml                    prometheus-2.41.0.linux-amd64/
git/                                    prometheus-2.41.0.linux-amd64.tar.gz
.gitconfig                              snap/
.local/                                 .wget-hsts
root@anji:~/prometheus-2.41.0.linux-amd64# cp /root/exporter-config.yaml  .

root@anji:~/prometheus-2.41.0.linux-amd64# ls
console_libraries  exporter-config.yaml  NOTICE      prometheus.yml
consoles           LICENSE               prometheus  promtool

root@anji:~/prometheus-2.41.0.linux-amd64# ./prometheus --config.file=exporter-config.yaml       \\\\\\\\  importent 
ts=2023-01-05T14:09:58.534Z caller=main.go:512 level=info msg="No time or size retention was set so using the default time retention" duration=15d
ts=2023-01-05T14:09:58.534Z caller=main.go:556 level=info msg="Starting Prometheus Server" mode=server version="(version=2.41.0, branch=HEAD, revision=c0d8a56c69014279464c0e15d8bfb0e153af0dab)"
ts=2023-01-05T14:09:58.534Z caller=main.go:561 level=info build_context="(go=go1.19.4, platform=linux/amd64, user=root@d20a03e77067, date=20221220-10:40:45)"
ts=2023-01-05T14:09:58.534Z caller=main.go:562 level=info host_details="(Linux 5.15.0-56-generic #62~20.04.1-Ubuntu SMP Tue Nov 22 21:24:20 UTC 2022 x86_64 anji (none))"
ts=2023-01-05T14:09:58.535Z caller=main.go:563 level=info fd_limits="(soft=1048576, hard=1048576)"
ts=2023-01-05T14:09:58.535Z caller=main.go:564 level=info vm_limits="(soft=unlimited, hard=unlimited)"
ts=2023-01-05T14:09:58.537Z caller=web.go:559 level=info component=web msg="Start listening for connections" "address=0.0.0.0:9090"
ts=2023-01-05T14:13:06.563Z caller=main.go:1234 level=info msg="Completed loading of configuration file"" filename=exporter-config.yaml "totalDuration=407.773Âµs db_storage=1.04Âµs remote_storage=1.623Âµs web_handler=334ns query_engine=671ns scrape=186.24Âµs scrape_sd=16.487Âµs notify=780ns notify_sd=1.527Âµs rules=1.129Âµs tracing=10.52Âµs
ts=2023-01-05T14:13:06.563Z caller=main.go:978 level=info msg="Server is ready to receive web requests."
ts=2023-01-05T14:13:06.563Z caller=manager.go:953 level=info component="rule manager" msg="Starting rule manager..."

{{}{{}{}}}    OR  OR  OR  
root@anji:~/prometheus-2.41.0.linux-amd64# ls
console_libraries  data                  LICENSE  prometheus      promtool
consoles           exporter-config.yaml  NOTICE   prometheus.yml
root@anji:~/prometheus-2.41.0.linux-amd64# cat prom
prometheus      prometheus.yml  promtool        
root@anji:~/prometheus-2.41.0.linux-amd64# cat prometheus.yml 
# my global config
global:
  scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.
  # scrape_timeout is set to the global default (10s).

# Alertmanager configuration
alerting:
  alertmanagers:
    - static_configs:
        - targets:
          # - alertmanager:9093

# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.
rule_files:
  # - "first_rules.yml"
  # - "second_rules.yml"

# A scrape configuration containing exactly one endpoint to scrape:
# Here it's Prometheus itself.
scrape_configs:
  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.
  - job_name: "prometheus"

    # metrics_path defaults to '/metrics'
    # scheme defaults to 'http'.

    static_configs:
      - targets: ["localhost:9090","192.168.29.152:9100","192.168.122.232:9100"]      "best see look  follow "

=======================+++++++++++
add the  node1  to  prometheus 
 https://prometheus.io/download/#node_exporter
 anji@worker1:~$ mkdir nodeexporter
anji@worker1:~$ cd nodeexporter/

anji@worker1:~/nodeexporter$ wget https://github.com/prometheus/node_exporter/releases/download/v1.5.0/node_exporter-1.5.0.linux-amd64.tar.gz
anji@worker1:~/nodeexporter$ ls
node_exporter-1.5.0.linux-amd64.tar.gz

anji@worker1:~/nodeexporter$ tar -xvzf node_exporter-1.5.0.linux-amd64.tar.gz 
node_exporter-1.5.0.linux-amd64/
node_exporter-1.5.0.linux-amd64/LICENSE
node_exporter-1.5.0.linux-amd64/NOTICE
node_exporter-1.5.0.linux-amd64/node_exporter

anji@worker1:~/nodeexporter$ ls
node_exporter-1.5.0.linux-amd64  node_exporter-1.5.0.linux-amd64.tar.gz

anji@worker1:~/nodeexporter$ cd node_exporter-1.5.0.linux-amd64/

anji@worker1:~/nodeexporter/node_exporter-1.5.0.linux-amd64$ ls
LICENSE  node_exporter  NOTICE

anji@worker1:~/nodeexporter/node_exporter-1.5.0.linux-amd64$ ./node_exporter 
ts=2023-01-05T14:33:29.309Z caller=node_exporter.go:117 level=info collector=uname
ts=2023-01-05T14:33:29.309Z caller=node_exporter.go:117 level=info collector=vmstat
ts=2023-01-05T14:33:29.309Z caller=node_exporter.go:117 level=info collector=xfs
ts=2023-01-05T14:33:29.309Z caller=node_exporter.go:117 level=info collector=zfs
ts=2023-01-05T14:33:29.314Z caller=tls_config.go:232 level=info msg="Listening on" "address=[::]:9100"
ts=2023-01-05T14:33:29.314Z caller=tls_config.go:235 level=info msg="TLS is disabled." http2=false address=[::]:9100

http://192.168.122.170:9100/   or  or  http://worker1:9100/

------------------------
root@anji:~/prometheus-2.41.0.linux-amd64# cat exporter-config.yaml 
global:
  scrape_interval: 15s

scrape_configs:
- job_name: node
  static_configs:
  - targets: ['localhost:9100','192.168.122.170:9100']
root@anji:~/prometheus-2.41.0.linux-amd64# 

{}{{{{{{{}{{{{{{{{{}}}}}}}}}}{{{{{{{{}}}}}}}}}}}}}  error "error "
msg="Unable to start web listener" err="listen tcp 0.0.0.0:9090: bind: address already in use"
sudo lsof -i -P -n | grep 9090

https://stackoverflow.com/questions/47552888/prometheus-error-starting-web-server-address-already-in-use
root@anji:~# lsof -i :9090
COMMAND    PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME
prometheu 3521 root    7u  IPv6 102416      0t0  TCP *:9090 (LISTEN)
prometheu 3521 root   12u  IPv4 102643      0t0  TCP localhost:60932->localhost:9090 (ESTABLISHED)
prometheu 3521 root   13u  IPv6 101242      0t0  TCP localhost:9090->localhost:60932 (ESTABLISHED)

root@anji:~# kill -9 3521
root@anji:~# lsof -i :9090

{}{{{{{{{}{{{{{{{}{}}}}}}}}}}}}}  ERROR 
Get "http://192.168.122.232:9100/metrics": dial tcp 192.168.122.232:9100: connect: no route to host

https://www.henryxieblogs.com/2018/11/how-to-fix-no-route-to-host-in.html

wget -O- localhost:9100/metrics
firewall-cmd --add-port=9100/tcp --permanent
systemctl restart firewalld

########################
"  GRAFANA DASH BOARD ADD 
https://grafana.com/grafana/dashboards/
https://grafana.com/grafana/dashboards/2747-linux-memory/
ID: 2747      "
{}{}{}{[]]{}{{}{}}{][[[{{{]][[{}{}{[][[[][{}{}{{{{{{{{{{}{}}}}}}}}}}]]]}]]}}}  error  ERROR  
N: Skipping acquire of configured file 'main/binary-i386/Packages' as repository 
 'https://packages.grafana.com/oss/deb stable InRelease' doesn't support architecture 'i386'

https://askubuntu.com/questions/741410/skipping-acquire-of-configured-file-main-binary-i386-packages-as-repository-x

dpkg --print-foreign-architectures
dpkg --print-architecture 
sudo dpkg --remove-architecture i386
=========++++++++  {{{}}{}{}[][[]{}{}{}[[]]][[[[[]]{}{}{{}}]]]]]}    error error 
add prometheus data source to grafana error message Error reading Prometheus: An error occurred within the plugin

http://192.168.122.48:9090/metrics  check once 

https://community.grafana.com/t/getting-403-forbidden-error-when-adding-data-source-by-ip/78400
root@anji:~# cat /var/log/grafana/grafana.log
logger=settings t=2023-01-06T09:36:58.922634288+05:30 level=info msg="Starting Grafana" version=9.3.2 commit=21c1d14e91 branch=HEAD compiled=2022-12-14T16:10:18+05:30
logger=settings t=2023-01-06T09:36:58.923330109+05:30 level=inf
-----
https://www.youtube.com/watch?v=4WWW2ZLEg74&t=1s    nice videos 

root@anji:~# kill -s HUP 
kill: usage: kill [-s sigspec | -n signum | -sigspec] pid | jobspec ... or kill -l [sigspec]
root@anji:~# kill -s HUP   pid no 

dash boards 
https://grafana.com/grafana/dashboards/?search=pod
https://grafana.com/orgs/grafana/plugins
https://grafana.com/orgs/grafana/dashboards

https://grafana.com/grafana/dashboards/
https://grafana.com/grafana/dashboards/7743-node-exporter-server-metrics-template/
https://grafana.com/grafana/dashboards/704-node-exporter-single-server/
https://grafana.com/grafana/dashboards/11757-hpc-node-exporter-server-simple-metrics-v2/
https://grafana.com/grafana/dashboards/9894-node-exporter-0-16-for-prometheus-monitoring-display-board/    super nice 

https://grafana.com/grafana/dashboards/10242-node-exporter-full/   best super exlent
https://grafana.com/grafana/dashboards/11206-traffic-analysis/
https://grafana.com/grafana/dashboards/12619-network-traffic/
https://grafana.com/grafana/dashboards/17229-hardware-sentry-site/
https://grafana.com/grafana/dashboards/9969-zookeeper-dashboard/
https://grafana.com/grafana/dashboards/6293-traefik/
https://grafana.com/grafana/dashboards/4475-traefik/
https://grafana.com/grafana/dashboards/8396-libvirt-kvm/
https://grafana.com/grafana/dashboards/15682-libvirt/
=={}{{}{}{}  }  ONLY  LINUX 
https://grafana.com/grafana/dashboards/13052-node-stats-and-alerts/
https://grafana.com/grafana/dashboards/12542-power-information/
https://grafana.com/grafana/dashboards/14513-linux-exporter-node/       === BEST OK 
https://grafana.com/grafana/dashboards/14731-1-linux-stats-with-node-exporter/      ==== BEST SUPER 
https://grafana.com/grafana/dashboards/10301-grafana/
https://grafana.com/grafana/dashboards/13021-system-metrics-for-the-linux-hosts/
https://grafana.com/grafana/dashboards/2747-linux-memory/                           == BEST
https://grafana.com/grafana/dashboards/6490-linux-node-exporter/
https://grafana.com/grafana/dashboards/8378-system-processes-metrics/
https://grafana.com/grafana/dashboards/5984-alerts-linux-nodes/

=============================================================#####################################3
grafana in kubenrets  
https://www.youtube.com/watch?v=HgjTUiU0Ihk      --  aws

The Istio service mesh
https://istio.io/latest/docs/setup/getting-started/


apiVersion: apps/v1
kind: Deployment
metadata: 
  name: kubectl-istio-deployment
spec:
  replicas: 1
  selector:
     matchLabels: 
       chapter: istio
       topic: istio-installation
  template:
     metadata:
         name: kubectl-istio-pod
         labels: 
           chapeter: istio
           topic: istio-installation
     spec: 
       containers: 
       - name: kubectl-container
         image: deepcloud2208/kubectlcontainer            
"---------=======================================================================
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: app-deployment
spec: 
  replicas: 1
  selector:
    matchLabels: 
      chapter: istio
      topic: traffic-management
      app: app
      version: v2
  template:
    metadata: 
      name: app-pod
      labels:
        chapter: istio
        topic: traffic-management
        app: app
        version: v2
    spec: 
       containers: 
       - name: app-container
         image: deepcloud2208/app:v2
         ports:
         - containerPort: 80
---
apiVersion: v1
kind: Service
metadata: 
  name: app-svc
  labels: 
     chapter: istio
     topic: trafic-management
     app: app
     version: v2
spec: 
   type: ClusterIp
   ports: 
   - targetPort: 80
     port: 8080
   selector: 
      chapter: istio
      topic: traffic-management
      app: app
      version: v2
---
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: app2-deployment
spec:
  replicas: 1
  selector: 
     matchLabels:
        chapter: istio
        topic: traffic-management
        app: app2
        version: v2
  template:
    metadata:
       name: app2-pod
       labels:
          chapter: istio
          topic: traffic-management
          app: app2
          version: v2
    spec: 
      containers: 
      - name: app2-container
        image:   deepcloud2208/app:v2
        ports: 
        - containerPort: 80
---
apiVersion: 
kind: Service
metadata:    
  name: app2-svc
  labels: 
    chapter: istio
    topic: traffic-management
    app: app2
    version: v2
spec:
  type: ClusterIp
  ports: 
  - targetPort: 80
    port: 8080
    selector:
       chapter: istio
       topic: traffic-management
       app: app2
       version: v2
========+++++++++++++++++++++++++++================================================="
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata: 
  name: istio-virtual-svc
spec: 
  hosts: 
  - app-svc.prod.svc.cluster.local
  http: 
  - name: virtual-service-routes
    route:
    - destination: 
         host: app-svc.prod.svc.cluster.local
         subset: app-service-subset
      weight: 80
    - destination:
         host: app2-svc.prod.svc.cluster.local
         subset: app2-service-subset
      weight: 20
---
apiVersion: networking.istio.io/v1beta1        
kind: DestinationRule
metadata: 
  name: istio-destination-rule1
spec: 
  host: app-svc.prod.svc.cluster.local
  subsets:
  - name: app-service-subset
    labels: 
      app: app
      version: v2
      chapter: istio
      topic: traffic-management
    trafficPolicy:
      loadBalancer:
        simple: ROUND_ROBIN
---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata: 
  name: istio-destination-rule1
    - name: app2-service-subset
      labels: 
         app: app2
         version: v2
         chapter: istio
         topic: traffic-management
      trafficPolicy:
        loadBalancer
          simple: ROUND_ROBIN
+++++++++++++++++++=+++++++++++++++++======================++++++++++++++"
while true
do
curl http:app:svc.prod.local:8080
sleep 1 
done
++++++++++===================================================
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: books-depl
spec: 
  replicas: 1
  selector: 
     matchLabels: 
        chapter: istio
        topic: traffic-management
        app: books  
        version: v1
  template: 
    metadata: 
       name: books-pod
       labels: 
          chapter: istio
          topic: traffic-management
          app: books
          version: v1
    spec: 
      containers:
      - name: books-cont
        image: deepcloud2208/books:v1
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata: 
  name: books-svc
  labels:
    chapter: istio
    topic: traffic-management
    app: books
    version: v1
spec: 
  type: ClusterIp
  ports: 
  - targetPort: 80
    port: 8000
  selector: 
    chapter: istio
    topic: traffic-management
    app: books
    version: v1

---
apiVersion: apps/v1
kind: Deployment
metadata: 
   name: electronics-depl
spec:
  replicas: 1
  selector:
    matchLabels:
      chapter: istio
      topic: traffic-management
      app: electronics
      version: v1
  template: 
    metadata:
      name: electronics-pod
      labels:
        chapter: istio
        topic: trafficmanagement
        app: electronics
        version: v1
    spec: 
      containers:
      - name: electronics-cont
        image: deeplcoud2208/electronics:v1
        ports:
          - containerPort: 80
---
apiVersion: v1
kind: Service
metadata: 
   name: electronics-svc
   labels: 
      chapter: istio
      toopic: traffic-management
      app: electronics
      version: v1
spec: 
  type: ClusterIP
  ports: 
  - targetPort: 80
    port: 8000
  selector: 
    chapter: istio
    topic: traffic-management
    app: electronics
    version: v1
---
apiVersion: apps/v1
kind: Deployment
metadata: 
   name: retail-depl
spec: 
   replicas: 1
   selector: 
     matchLabels: 
       chapter: istio
       topic: traffic-management
       app: retail
       version: v1
   template: 
      metadata: 
         name: retail-pod
         labels: 
            chapter: istio
            topic: traffic-management
            app: retail
            version: v1
      spec: 
        containers: 
        - name: retail-cont
          image: deepcloud2208/retail:v1
          ports: 
            - containerPort: 80
---
apiVersion: v1
kind: Service
metadata: 
  name: retail-svc
  labels: 
     chapter: istio
     topic: traffic-management
     app: retail
     version: v1
spec: 
  type: ClusterIP
  ports: 
  - targetPort: 80
    port: 8000
  selector: 
     chapter: istio
     topic: traffic-management
     app: retail
     version: v1
###############################3++++++++++++++++++++++++==================" 
apiVersion: networking.istio.io/v1beta1
kind: VirtuvalService
metadata: 
  name: retail-portal-vs
spec: 
  hosts: 
  - retail-svc.prod.svc.cluster.local
  http: 
  - name: books-routes
    match:
    - uri:
       prefix: "/books"
    rewrite:
       uri: "/"           ## optional  
    route: 
    - destination:
        host: books-svc.prod.svc.cluster.local
        subset: books-service-subset

  - name: electronics-routes
    match: 
    - uri: 
       prefix: "/electronics"
    rewrite:
       uri: "/"           ## optional  
    route:
    - destination:
         host: electronics-svc.prod.svc.cluster.local
         subset: electronics-service-subset
  - name: retail-routes
    route: 
    - destination: 
         host: retail-svc.prod.svc.cluster.local
         subset: retail-service-subset
  - name: retail-routes
    route:
    - destination:
          host: retail-svc.prod.svc.cluster.local
          subset: retails-service-subset
---
apiVersion: networking.istio.io/v1beta1  
kind: DestinationRule
metadata: 
   name: books-destination-rule
spec: 
  host: books-svc.prod.svc.cluster.local
  subsets:
  - name: books-service-subset
    labels: 
      app: books
      version: v1
      chapter: istio
      topic: traffic-management
    trafficPolicy:
       loadBalancer: 
          simple: ROUND_ROBIN
---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata: 
  name: electronics-destination-rule
spec: 
  host: electronics-svc.prod.svc.cluster.local
  subsets:
  -  name: electronics-service-subset
     labels: 
       app: electronics
       version: v1
       chapter: istio
       topic: traffic-management
     trafficPolicy:
       loadBlancer:
         simple: ROUND_ROBIN
---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: retail-destination-rule
spec: 
  host: retail-svc.prod.svc.cluster.local
  subsets:
  - name: retail-service-subset
    labels:
      app: retail
      version: v1
      chapter: istio
      topic: traffic-management
    trafficPolicy
       loadBalancer:
         simple: ROUND_ROBIN
#################################################++++++++++++++++++++++++
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: ab-depl1              
spec: 
  replicas: 1
  selector: 
      matchLabels:
         chapter: istio
         topic: traffic-management
         app: ab
         version: v1
  template:
    metadata: 
       name: ab-pod
       labels:
         chapter: istio
         topic: traffic-management
         app: ab
         version: v1
    spec: 
      containers:
      - name: ab-cont
        image: deepcloud2208/ab:v1
        ports: 
        - containerPort: 80
---
apiVersion: apps/v1
kind: Deployment
metadata: 
   name: ab-depl2
spec: 
  replicas: 1
  selector: 
      matchLabels:
         chapter: istio
         topic: traffic-management
         app: ab
         version: v2
  template:
    metadata: 
       name: ab-pod
       labels:
          chapter: istio
          topic: traffic-management
          app: ab
          verasion: v2
    spec: 
      containers: 
      - name: ab-cont
        image: deepcloud2208/ab:v2
        ports: 
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata: 
   name: ab-svc
   labels: 
      chapter: istio
      topic: traffic-management
      app: ab
spec: 
  type: ClusterIP
  ports: 
  - targetPort: 80
    port: 8000
  selector:
     chapter: istio
     topic: traffic-management
     app: ab
##########################################+++++++++++++++++++++++++++
apiVersion: networking.istio.io/v1beta1
kind: Gateway
metadata: 
  name: istio-ingress-gateway
spec: 
  selector: 
    app: istio-ingressgateway
    istio: ingressgateway


  servers: 
  - port:
      number: 80
      name: app
      ptotocol: HTTP
    hosts:
    - "anji.com"
    - "app-svc.prod.svc.cluster.local"

=======================================================
https://istio.io/latest/docs/reference/config/networking/service-entry/
Service Entry
apiVersion: networking.istio.io/v1beta1
kind: ServiceEntry
metadata:
  name: external-svc-https
spec:
  hosts:
  - api.dropboxapi.com
  - www.googleapis.com
  - api.facebook.com
  location: MESH_EXTERNAL
  ports:
  - number: 443
    name: https
    protocol: TLS
  resolution: DNS
https://github.com/kubernetes/kubectl/issues/837
https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/
kubectl get configmap istio -n anjireddy -o yaml | sed 's/mode: ALLOW_ANY/mode: REGISTRY_ONLY/g' | kubectl replace -n anjireddy -f -


apiVersion: networking.istio.io/v1beta1
kind: ServiceEntry
metadata: 
  name: webpage-on-ec2
spec:
  hosts: 
  - ec2-54.87.21.45.compute-1.amazonaws.com
  ports: 
  - number: 80
    name: http
    protocol: HTTP
  - number: 443
    name: https
    protocol: HTTPS
  resolution: DNS
  location: MESH_EXTERNAL
---------------------------===================
kubectl get se -n prod

apiVersion: networking.istio.io/v1beta1
kind: Gateway
metadata: 
  name: kiali-ingress-gateway
  namespace: istio-system
spec: 
  selector: 
    app: istio-ingressgateway
    istio: ingressgateway
  servers:
  - port:
      number: 15029
      name: http-kiali
      protocol: HTTP
    hosts:
    - "*"      
===============+++++++++++++++++++++
apiVersion: networking.istio.io/v1beta1
kind: VirtuvalService
metadata: 
  name: kiali-vurtual-service
  namespace: istio-system
spec: 
  hosts:
  - "*"
  gateways:
  - kiali-ingress-gateway    
  http:
  - match:
    - port: 15029
    route:
    - destination:
      host: kiali.istio-system.svc.cluster.local
      port:
        number: 20001
---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata: 
  name: kiali-destination-rule
  namespace: istio-system
spec:
  host: kiali.istio-system.svc.cluster.local
  trafficPolicy:
    tls:
      mode: DISABLE
==================================++++++++++++++++++###########3
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata: 
  name: prometheus-gateway
  namespace: istio-system
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 15030
      name: http-prom
      protocol: HTTP
    hosts:
    - "*"
---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata: 
  name: prometheous-vs
  namespace: istio-system
  spec: 
    hosts: 
    - "*"
    gateway:
    - prometheus-gateway
    http:
    - match:
      - port: 15030
      route:
      - destination:
          host: prometheus
          port:
            number: 9090
---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata: 
  name: prometheus
  namespace: istio-system
spec: 
 host: prometheus
 trafficPolicy:
   tls: 
     mode: DISABLE
###############################################################################3
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata: 
   name: grafana-gateway
   namespace: istio-system
spec: 
  selector: 
    istio: ingressgateway
  servers: 
  - port: 
      number: 15031
      name: http-grafana
      protocol: HTTP
    hosts:
    - "*"                
---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata: 
  name: grafana-vs
  namespace: istio-system
spec: 
  hosts: 
  - "*"
  gateways:
  - grafana-gateway
  http:
  - match:
    - port: 15031
    route: 
    - destination: 
         host: grafana
         port: 
           number: 3000
---
apiVersion: networking.istio.io/v1alpha3
kind: Destination
metadata: 
  name: grafana
  namespace: istio-system
spec: 
  host: grafana
  trafficPolicy:
     tls:
       mode: DISABLE
############################################################################33
kubectl get gw -n  anjireddy   11.15 
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: app1
spec: 
  replicas: 1
  selector: 
    matchLabels: 
       chapter: istio
       topic: kiali
       app: app1
       version: v1
  template: 
     metadata: 
       name: app1-pod
       labels: 
          chapter: istio
          topic: kiali
          app: app1
          version: v1
     spec: 
       containers: 
       - name: app1-container
         image: deepcloud2208/app1:v1
         ports: 
         - containerPort: 80
---
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: app2
spec: 
  replicas: 1
  selector: 
    matchLabels: 
       chapter: istio
       topic: kiali
       app: app2
       version: v1
  template: 
     metadata: 
       name: app1-pod
       labels: 
          chapter: istio
          topic: kiali
          app: app2
          version: v1
     spec: 
       containers: 
       - name: app1-container
         image: deepcloud2208/app2:v1
         ports: 
         - containerPort: 80         
---
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: app3
spec: 
  replicas: 1
  selector: 
    matchLabels: 
       chapter: istio
       topic: kiali
       app: app3
       version: v1
  template: 
     metadata: 
       name: app1-pod
       labels: 
          chapter: istio
          topic: kiali
          app: app3
          version: v1
     spec: 
       containers: 
       - name: app1-container
         image: deepcloud2208/app1:v1
         ports: 
         - containerPort: 80                 
---
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: app4
spec: 
  replicas: 1
  selector: 
    matchLabels: 
       chapter: istio
       topic: kiali
       app: app4
       version: v1
  template: 
     metadata: 
       name: app1-pod
       labels: 
          chapter: istio
          topic: kiali
          app: app4
          version: v1
     spec: 
       containers: 
       - name: app1-container
         image: deepcloud2208/app1:v1
         ports: 
         - containerPort: 80
---
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: app5
spec: 
  replicas: 1
  selector: 
    matchLabels: 
       chapter: istio
       topic: kiali
       app: app5
       version: v1
  template: 
     metadata: 
       name: app1-pod
       labels: 
          chapter: istio
          topic: kiali
          app: app5
          version: v1
     spec: 
       containers: 
       - name: app1-container
         image: deepcloud2208/app1:v1
         ports: 
         - containerPort: 80
---


apiVersion: v1
kind: Service
metadata: 
  name: app5
  labels: 
     chapter: istio
     topic: kiali
     app: app5
     version: v1
spec: 
  type: ClusterIP
  ports: 
  - targetPort: 80
    port: 8080
  selector: 
     chapter: istio
     topic: kiali
     app: app5
     version: v1
---
apiVersion: v1
kind: Service
metadata: 
  name: app4
  labels: 
     chapter: istio
     topic: kiali
     app: app4
     version: v1
spec: 
  type: ClusterIP
  ports: 
  - targetPort: 80
    port: 8080
  selector: 
     chapter: istio
     topic: kiali
     app: app4
     version: v1
---
apiVersion: v1
kind: Service
metadata: 
  name: app3
  labels: 
     chapter: istio
     topic: kiali
     app: app3
     version: v1
spec: 
  type: ClusterIP
  ports: 
  - targetPort: 80
    port: 8080
  selector: 
     chapter: istio
     topic: kiali
     app: app3
     version: v1
---
apiVersion: v1
kind: Service
metadata: 
  name: app2
  labels: 
     chapter: istio
     topic: kiali
     app: app2
     version: v1
spec: 
  type: ClusterIP
  ports: 
  - targetPort: 80
    port: 8080
  selector: 
     chapter: istio
     topic: kiali
     app: app2
     version: v1
---
apiVersion: v1
kind: Service
metadata: 
  name: app1
  labels: 
     chapter: istio
     topic: kiali
     app: app1
     version: v1
spec: 
  type: ClusterIP
  ports: 
  - targetPort: 80
    port: 8080
  selector:       ###  11.15  videos 
     chapter: istio
     topic: kiali
     app: app1
     version: v1
####################################################################################################333
apiVersion: networking.istio.io/v1beta1      ## 11.16 video VSDR.YAML
kind: VirtuvalService
metadata: 
  name: app1
spec:                                                   
  hosts: 
  - app1.prod.svc.cluster.local
  http: 
  - name: app1-service-routes
    route:
    - destination:
        host: app1.prod.svc.cluster.local
        subset: app1-service-subset
      weight: 90
    - destination: 
        host: app2.prod.svc.cluster.local    
        subset: app2-service-subset
      weight: 10
---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata: 
   name: app1-destination-rule
spec: 
  host: app1.prod.svc.cluster.local
  subsets:
  - name: app1-service-subset
    labels:
       app: app1
       version: v1
       chapter: istio
       topic: kiali
    trafficPolicy:
       loadBalancer: 
         simple: ROUND_ROBIN
---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata: 
   name: app2-destination-rule
spec: 
  host: app2.prod.svc.cluster.local
  subsets:
  - name: app2-service-subset
    labels:
       app: app1
       version: v1
       chapter: istio
       topic: kiali
    trafficPolicy:
       loadBalancer: 
         simple: ROUND_ROBIN                  
---
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata: 
  name: app3
spec: 
  hosts: 
  - app3.prod.svc.cluster.local
  http: 

  - name: app3-routes
    match: 
    - uri: 
       prefix: "/app3"
    rewrite:
      uri: "/"
    route:
    - destination: 
         host: app3.prod.svc.cluster.local
         subset: app3-service-subset
  - name: app4-routes
    match: 
    - uri: 
       prefix: "/app4"
    rewrite: 
      uri: "/"
    route:
    - destination:
         host: app4.prod.svc.cluster.local
         subset: app4-service-subset
    - name: app5-routes
      match: 
      - uri:
         prefix: "/app5"
      rewrite: 
      - destination: 
           host: app5.prod.svc.cluster.local
           subset: app5-service-subset
---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata: 
   name: app3-destination-rule
spec: 
  host: app3.prod.svc.cluster.local
  subsets: 
  - name: app3-service-subset
    labels: 
      app: app3
      version: v1
      chapter: istio
      topic: kiali
    trafficPolicy:
      loadBalancer:
         simple: ROUND_ROBIN
---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata: 
   name: app4-destination-rule
spec: 
   host: app4.prod.svc.cluster.local
   subsets:
   - name: app4-service-subset
     labels: 
       app: app4
       version: v1
       chapter: istio
       topic: kiali
     trafficPolicy: 
       loadBalancer: 
         simple: ROUND_ROBIN
---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata: 
   name: app5-destination-rule
spec: 
   host: app5.prod.svc.cluster.local
   subsets: 
   - labels: 
       app: app5
       version: v1
       chapter: istio
       topic: kiali
     trafficPolicy: 
       loadBalancer: 
          simple: ROUND_ROBIN
#######################################################################################################3
11.17 appgateway
apiVersion: networking.istio.io/v1beta1
kind: Gateway
metadata: 
   name: allapp-ingress-gateway
spec: 
  selector: 
    app: istio-ingressgateway
    istio:  ingressgateway
  servers: 
  - port: 
      number: 80
      name: app
      protocol: HTTP
    hosts: 
    - "deep.com"
    - "app.com"    
===============================================================
gateway virtualService
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata: 
   name: allapp-virtual-svc-for-ingress-gtw
spec: 
  hosts: 
  - "deep.com"
  - "app.com"
  gateway: 
  - allapp-ingress-gateway
  http:
  - name: allapp-service-routes
    route: 
    - destination: 
         host: app1.prod.svc.cluster.local
         subset: app-service-subset
      weight: 0
    - destination: 
        host: app2.prod.svc.cluster.local
        subset: app2-service-subset
      weight: 100
---
apiVersion: network.istio.io/vbeta1
kind: DestinationRule
metadata: 
   name: ingress-gtw-destination-rule1
spec: 
  host: app1.prod.svc.cluster.local
  subsets:
  - name: app1-service-subset
    labels: 
       app: app1
       version: v1
       chapter: istio
       topic: kiali
    trafficPolicy: 
       loadBalancer: 
         simple: ROUND_ROBIN
---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata: 
   name: ingress-gtw-destination-rule2
spec: 
   host: app2.prod.svc.cluster.local
   subsets: 
   - name: app2-service-subset
     labels: 
       app: app2
       version: v1
       chapter: istio
       topic: kiali
     trafficPolicy:
        loadBalancer: 
           simple: ROUND_ROBIN
######################################################################################################3
11.6 - video,    11.19 SERVICEENTRY
apiVersion: networking.istio.io/v1beta1
kind: ServiceEntry
metadata: 
   name: 
spec: 
  hosts:
  - "*.compute-1.amazoneaws.com"
  - www.amazon.com
  - www.anji.com
  ports: 
  - number: 80
    name: http
    protocol: HTTP
  - number: 443
    name: https
    protocol: HTTPS
  resolution: NONE
  location: MESH_EXTERNAL
#########################################################################################################################3

centos  9 
sudo dnf install java-11-openjdk-devel

https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-centos-7
curl -fsSL https://get.docker.com/ | sh
sudo systemctl start docker
sudo systemctl enable docker
[root@localhost ~]# docker --version

==========================================================================
kubeadm install in centos = 9
https://upcloud.com/resources/tutorials/install-kubernetes-cluster-centos-8
https://tayeh.me/posts/install-kubernetes-cluster-on-centos-8-with-kubeadm-crio/

[root@localhost ~]# nano /etc/fstab 

[root@localhost ~]# setenforce 0
[root@localhost ~]# nano /etc/selinux/config    ### SELINUX=disabled     or  
[root@localhost ~]# sed -i --follow-symlinks 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux


[root@localhost ~]# modprobe overlay
[root@localhost ~]# modprobe br_netfilter
tee /etc/sysctl.d/k8s.conf<<EOF
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward = 1
EOF

[root@localhost ~]# sysctl --system



[root@localhost ~]# firewall-cmd --reload

curl -fsSL https://get.docker.com/ | sh
sudo systemctl start docker
sudo systemctl enable docker
systemctl restart docker
[root@localhost ~]# docker --version


[root@localhost ~]# echo '{
  "exec-opts": ["native.cgroupdriver=systemd"]
}' > /etc/docker/daemon.json

tee /etc/yum.repos.d/kubernetes.repo<<EOF
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF


[root@localhost ~]# yum -y install kubelet kubeadm kubectl --disableexcludes=kubernetes epel-release
[root@localhost ~]# dnf  -y install kubelet kubeadm kubectl --disableexcludes=kubernetes epel-release     latest
sudo yum-mark hold kubeadm kubelet kubectl

[root@localhost ~]# kubectl version --client
[root@localhost ~]# systemctl enable kubelet
[root@localhost ~]# systemctl start kubelet

[root@localhost ~]# firewall-cmd --zone=public --permanent --add-port={6443,2379,2380,10250,10251,10252}/tcp

firewall-cmd --zone=public --permanent --add-rich-rule 'rule family=ipv4 source address=worker-IP-address/32 accept' 
firewall-cmd --zone=public --permanent --add-rich-rule 'rule family=ipv4 source address=172.17.0.0/16 accept'
[root@localhost ~]# firewall-cmd --reload
[root@master kubernetes]# systemctl restart containerd 

[aws@master ~]$ sudo kubeadm init --pod-network-cidr 10.244.0.0/16
[init] Using Kubernetes version: v1.26.0
[preflight] Running pre-flight checks
	[WARNING Firewalld]: firewalld is active, please ensure ports [6443 10250] are open or your cluster may not function correctly
	[WARNING Hostname]: hostname "master" could not be reached
	[WARNING Hostname]: hostname "master": lookup master on 192.168.122.1:53: server misbehaving
[preflight] Pulling images required for setting up a Kubernetes cluster
     ###  OR  OR  OR 
 kubeadm init --ignore-preflight-errors all

###  OR   OR OR 
 
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.122.5:6443 --token fsn8g9.045q38upvmkriqem \
	--discovery-token-ca-cert-hash sha256:57d3ba8ea0d375e7b7957203c239c041901ce75fa51ef15a3ac34349c75672ea 
[root@master ~]# 




 kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml
  kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml


================================================================================================

How to install helm?
https://helm.sh/docs/intro/install/
https://get.helm.sh/helm-v3.10.3-linux-amd64.tar.gz

Download your desired version
Unpack it (tar -zxvf helm-v3.0.0-linux-amd64.tar.gz)
(mv linux-amd64/helm /usr/local/bin/helm)

===============================================================
rancher install in DOCKER 

anji@master:~$ sudo docker run --privileged -d --restart=unless-stopped -p 800:80 -p 443:443 rancher/rancher
723ab7b978600b308299c4acfb8c90d5380faab56278bf7bea26acc03edc1aae

anji@master:~$ sudo docker images
REPOSITORY        TAG       IMAGE ID       CREATED       SIZE
rancher/rancher   latest    b8d45566fae9   7 weeks ago   1.56GB

anji@master:~$ sudo docker ps
CONTAINER ID   IMAGE             COMMAND           CREATED          STATUS          PORTS                                                                        NAMES
723ab7b97860   rancher/rancher   "entrypoint.sh"   28 seconds ago   Up 26 seconds   0.0.0.0:443->443/tcp, :::443->443/tcp, 0.0.0.0:800->80/tcp, :::800->80/tcp   wizardly_northcutt

docker logs   723ab7b97860  2>&1 | grep "Bootstrap Password:"

root@master:~# docker logs   723ab7b97860  2>&1 | grep "Bootstrap Password:"
2023/01/10 11:13:47 [INFO] Bootstrap Password: 95vwrsldjpvb84d464ncpp9nd59kk2ss254fpdmw58b2qkjtms9rgl

https://192.168.122.244/dashboard/

+++=========////////\\\\\\\\\\\\\\\\\

rancher installed in ubuntu  kubenretes cluster

https://www.devopsschool.com/blog/rancher-installation-deployment-in-kubernetes-clustor/
https://ranchermanager.docs.rancher.com/v2.5/pages-for-subheaders/install-upgrade-on-a-kubernetes-cluster

Add the Helm chart repository
Create a namespace for Rancher
Choose your SSL configuration
Install cert-manager (unless you are bringing your own certificates, or TLS will be terminated on a load balancer)
Install Rancher with Helm and your chosen certificate option
Verify that the Rancher server is successfully deployed
Save your options

anji@master:~$ helm repo add rancher-latest https://releases.rancher.com/server-charts/latest

anji@master:~$ helm repo add rancher-stable https://releases.rancher.com/server-charts/stable

anji@master:~$ helm repo add rancher-stable https://releases.rancher.com/server-charts/stable

anji@master:~$ kubectl create namespace rancher
namespace/rancher created
anji@master:~$ kubectl get ns
NAME              STATUS   AGE
default           Active   3d
kube-node-lease   Active   3d
kube-public       Active   3d
kube-system       Active   3d
"rancher   "        Active   9s

anji@master:~$ kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.5.1/cert-manager.crds.yaml

anji@master:~$ helm repo add jetstack https://charts.jetstack.io

anji@master:~$ helm repo update

anji@master:~$ helm install cert-manager jetstack/cert-manager \
  --namespace cert-manager \
  --create-namespace \
  --version v1.5.1


NAME:" cert-manager"
LAST DEPLOYED: Tue Jan 10 12:13:16 2023
NAMESPACE: cert-manager
STATUS: "deployed"
REVISION: 1
TEST SUITE: None
NOTES:
cert-manager v1.5.1 has been deployed successfully!
In order to begin issuing certificates, you will need to set up a ClusterIssuer
or Issuer resource (for example, by creating a 'letsencrypt-staging' issuer).
More information on the different types of issuers and how to configure them
can be found in our documentation:
https://cert-manager.io/docs/configuration/
For information on how to configure cert-manager to automatically provision
Certificates for Ingress resources, take a look at the `ingress-shim`
documentation:
https://cert-manager.io/docs/usage/ingress/

anji@master:~$  kubectl get pods --namespace cert-manager
NAME                                       READY   STATUS    RESTARTS   AGE
cert-manager-5879b6cc6b-mwc8f              1/1     Running   0          62s
cert-manager-cainjector-6f875446dc-sd2km   1/1     Running   0          62s
cert-manager-webhook-65745fbb58-jbn4m      1/1     Running   0          62s

anji@master:~$ kubectl get pods --all-namespaces  --show-labels  -o wide
NAMESPACE      NAME                                       READY   STATUS    RESTARTS      AGE     IP                NODE      NOMINATED NODE   READINESS GATES   LABELS
"cert-manager   cert-manager-5879b6cc6b-mwc8f              1/1     Running   0             6m42s   10.44.0.2         worker1   <none>           <none>            app.kubernetes.io/component=controller,app.kubernetes.io/instance=cert-manager,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=cert-manager,app.kubernetes.io/version=v1.5.1,app=cert-manager,helm.sh/chart=cert-manager-v1.5.1,pod-template-hash=5879b6cc6b
cert-manager   cert-manager-cainjector-6f875446dc-sd2km   1/1     Running   0             6m42s   10.44.0.1         worker1   <none>           <none>            app.kubernetes.io/component=cainjector,app.kubernetes.io/instance=cert-manager,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=cainjector,app.kubernetes.io/version=v1.5.1,app=cainjector,helm.sh/chart=cert-manager-v1.5.1,pod-template-hash=6f875446dc
cert-manager   cert-manager-webhook-65745fbb58-jbn4m"      1/1     Running   0             6m42s   10.44.0.3         worker1   <none>           <none>            app.kubernetes.io/component=webhook,app.kubernetes.io/instance=cert-manager,app.kubernetes.io/managed-by=Helm,app.kubernetes.io/name=webhook,app.kubernetes.io/version=v1.5.1,app=webhook,helm.sh/chart=cert-manager-v1.5.1,pod-template-hash=65745fbb58
kube-system    coredns-565d847f94-fsdjv                   1/1     Running   0             20m     10.32.0.2         master    <none>           <none>            k8s-app=kube-dns,pod-template-hash=565d847f94
kube-system    coredns-565d847f94-nczt7                   1/1     Running   0             20m     10.32.0.3         master    <none>           <none>            k8s-app=kube-dns,pod-template-hash=565d847f94
kube-system    etcd-master                                1/1     Running   0             21m     192.168.122.151   master    <none>           <none>            component=etcd,tier=control-plane
kube-system    kube-apiserver-master                      1/1     Running   0             21m     192.168.122.151   master    <none>           <none>            component=kube-apiserver,tier=control-plane
kube-system    kube-controller-manager-master             1/1     Running   0             21m     192.168.122.151   master    <none>           <none>            component=kube-controller-manager,tier=control-plane
kube-system    kube-proxy-gs8d4                           1/1     Running   0             20m     192.168.122.151   master    <none>           <none>            controller-revision-hash=cc5dd58d4,k8s-app=kube-proxy,pod-template-generation=1
kube-system    kube-proxy-wkm2r                           1/1     Running   0             15m     192.168.122.212   worker1   <none>           <none>            controller-revision-hash=cc5dd58d4,k8s-app=kube-proxy,pod-template-generation=1
kube-system    kube-scheduler-master                      1/1     Running   0             21m     192.168.122.151   master    <none>           <none>            component=kube-scheduler,tier=control-plane
kube-system    weave-net-57s4z                            2/2     Running   1 (14m ago)   15m     192.168.122.212   worker1   <none>           <none>            controller-revision-hash=d6dc745f,name=weave-net,pod-template-generation=1
kube-system    weave-net-sc862                            2/2     Running   1 (14m ago)   15m     192.168.122.151   master    <none>           <none>            controller-revision-hash=d6dc745f,name=weave-net,pod-template-generation=1
-------=======

helm install rancher1 rancher \
  --namespace cattle-system \
  --set hostname=rancher.my.org \
  --set replicas=3
NAME: rancher1
LAST DEPLOYED: Tue Jan 10 13:10:00 2023
NAMESPACE: cattle-system
STATUS: deployed
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace cattle-system -l "app.kubernetes.io/name=rancher,app.kubernetes.io/instance=rancher1" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace cattle-system $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace cattle-system port-forward $POD_NAME 8080:$CONTAINER_PORT

anji@master:~$ helm list -a  -n rancher
NAME    	NAMESPACE	REVISION	UPDATED                                	STATUS  	CHART        	APP VERSION
rancher1	rancher  	1       	2023-01-10 12:33:30.367563213 +0530 IST	deployed	rancher-0.1.0	1.16.0     

anji@master:~$ kubectl get  all  --all-namespaces
NAMESPACE       NAME                                           READY   STATUS    RESTARTS      AGE
cattle-system   pod/rancher1-b5bf85f65-6cxq8                   1/1     Running   0             83s
cert-manager    pod/cert-manager-5879b6cc6b-6g2xr              1/1     Running   0             4m23s
cert-manager    pod/cert-manager-cainjector-6f875446dc-ztt4b   1/1     Running   0             4m23s
cert-manager    pod/cert-manager-webhook-65745fbb58-q5hdt      1/1     Running   0             4m23s
kube-system     pod/coredns-565d847f94-fsdjv                   1/1     Running   0             72m
kube-system     pod/coredns-565d847f94-nczt7                   1/1     Running   0             72m
kube-system     pod/etcd-master                                1/1     Running   0             72m
kube-system     pod/kube-apiserver-master                      1/1     Running   0             72m
kube-system     pod/kube-controller-manager-master             1/1     Running   0             72m
kube-system     pod/kube-proxy-gs8d4                           1/1     Running   0             72m
kube-system     pod/kube-proxy-wkm2r                           1/1     Running   0             66m
kube-system     pod/kube-scheduler-master                      1/1     Running   0             72m
kube-system     pod/weave-net-57s4z                            2/2     Running   1 (66m ago)   66m
kube-system     pod/weave-net-sc862                            2/2     Running   1 (66m ago)   66m

NAMESPACE       NAME                           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                  AGE
cattle-system   service/rancher1               ClusterIP   10.101.146.39   <none>        80/TCP                   83s
cert-manager    service/cert-manager           ClusterIP   10.99.246.26    <none>        9402/TCP                 4m23s
cert-manager    service/cert-manager-webhook   ClusterIP   10.100.41.108   <none>        443/TCP                  4m23s
default         service/kubernetes             ClusterIP   10.96.0.1       <none>        443/TCP                  72m
kube-system     service/kube-dns               ClusterIP   10.96.0.10      <none>        53/UDP,53/TCP,9153/TCP   72m

NAMESPACE     NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
kube-system   daemonset.apps/kube-proxy   2         2         2       2            2           kubernetes.io/os=linux   72m
kube-system   daemonset.apps/weave-net    2         2         2       2            2           <none>                   66m

NAMESPACE       NAME                                      READY   UP-TO-DATE   AVAILABLE   AGE
cattle-system   deployment.apps/rancher1                  1/1     1            1           83s
cert-manager    deployment.apps/cert-manager              1/1     1            1           4m23s
cert-manager    deployment.apps/cert-manager-cainjector   1/1     1            1           4m23s
cert-manager    deployment.apps/cert-manager-webhook      1/1     1            1           4m23s
kube-system     deployment.apps/coredns                   2/2     2            2           72m

NAMESPACE       NAME                                                 DESIRED   CURRENT   READY   AGE
cattle-system   replicaset.apps/rancher1-b5bf85f65                   1         1         1       83s
cert-manager    replicaset.apps/cert-manager-5879b6cc6b              1         1         1       4m23s
cert-manager    replicaset.apps/cert-manager-cainjector-6f875446dc   1         1         1       4m23s
cert-manager    replicaset.apps/cert-manager-webhook-65745fbb58      1         1         1       4m23s
kube-system     replicaset.apps/coredns-565d847f94                   2         2         2       72m "

====
anji@master:~$ kubectl get pod,deployments  -n cert-manager
NAME                                           READY   STATUS    RESTARTS   AGE
pod/cert-manager-5879b6cc6b-6g2xr              1/1     Running   0          9m25s
pod/cert-manager-cainjector-6f875446dc-ztt4b   1/1     Running   0          9m25s
pod/cert-manager-webhook-65745fbb58-q5hdt      1/1     Running   0          9m25s

NAME                                      READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/cert-manager              1/1     1            1           9m25s
deployment.apps/cert-manager-cainjector   1/1     1            1           9m25s
deployment.apps/cert-manager-webhook      1/1     1            1           9m25s



anji@master:~$ kubectl -n cattle-system rollout status deploy/rancher1
deployment "rancher1" successfully rolled out

nji@master:~$ kubectl -n cattle-system rollout status deploy/rancher1
deployment "rancher1" successfully rolled out

anji@master:~$ kubectl -n cattle-system get deploy rancher1
NAME       READY   UP-TO-DATE   AVAILABLE   AGE
rancher1   1/1     1            1           14m "
======++++++++++++++++++
anji@master:~$ kubectl get svc -n cattle-system
NAME       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
rancher1   "ClusterIP "  10.101.146.39   <none>        80/TCP    45m

anji@master:~$ kubectl edit svc rancher1  -n cattle-system
# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: "2023-01-10T07:40:01Z"
  labels:
spec:
  clusterIP: 10.101.146.39
  clusterIPs:
  - 10.101.146.39
======OR   OR  

anji@master:~$ kubectl patch svc rancher1 -p  '{"spec":  {"type": "NodePort"}}' -n cattle-system
service/rancher1 patched

anji@master:~$ kubectl get svc -n cattle-system
NAME       TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
rancher1   "NodePort "  10.101.146.39   <none>        80:"32642/"TCP   59m  "
--=====
                 ip                          192.168.122.151:32642

anji@master:~$ curl http://192.168.122.151:32642/
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
anji@master:~$ 
==================
https://www.youtube.com/watch?v=LK6KbAlQRIg
https://gist.github.com/rritsoft/a60f3976799aa7170b33758818bf097e

helm repo upgrade

helm upgrade --install \
    rancher rancher/rancher \
    --namespace cattle-system \
    --create-namespace \
    --set hostname=$RANCHER_ADDR \
    --set ingress.tls.source=letsEncrypt \
    --set letsEncrypt.email=$EMAIL \
    --wait
anji@master:~$ helm upgrade --install \
>     rancher rancher/rancher \
>     --namespace cattle-system \
>     --create-namespace \
>     --set hostname=$RANCHER_ADDR \
>     --set ingress.tls.source=letsEncrypt \
>     --set letsEncrypt.email=$EMAIL \
>     --wait
Release "rancher" does not exist. Installing it now.
coalesce.go:223: warning: destination for rancher.ingress.tls is a table. Ignoring non-table value ([])
NAME: rancher
LAST DEPLOYED: Tue Jan 10 16:17:00 2023
NAMESPACE: cattle-system
STATUS: deployed
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace cattle-system -l "app.kubernetes.io/name=rancher,app.kubernetes.io/instance=rancher" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace cattle-system $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace cattle-system port-forward $POD_NAME 8080:$CONTAINER_PORT
================================================================================
How To Install Jenkins on Ubuntu 20.04  
https://www.digitalocean.com/community/tutorials/how-to-install-jenkins-on-ubuntu-20-04

wget -q -O - https://pkg.jenkins.io/debian-stable/jenkins.io.key | sudo apt-key add -

sudo sh -c 'echo deb http://pkg.jenkins.io/debian-stable binary/ > /etc/apt/sources.list.d/jenkins.list' 
sudo apt update
sudo apt install jenkins

sudo systemctl start jenkins
sudo systemctl status jenkins
sudo ufw allow 8080
sudo ufw allow OpenSSH
sudo ufw enable
sudo ufw status

root@anji:~# cat /var/lib/jenkins/secrets/initialAdminPassword
608116a35ffc4daf8fc77202a899ec5c

================+++++++++++++++++++]][[]]

https://www.digitalocean.com/community/tutorials/how-to-install-jenkins-on-ubuntu-22-04
https://www.digitalocean.com/community/tutorials/how-to-install-jenkins-on-ubuntu-22-04

wget -q -O - https://pkg.jenkins.io/debian-stable/jenkins.io.key |sudo gpg --dearmor -o /usr/share/keyrings/jenkins.gpg

sudo sh -c 'echo deb [signed-by=/usr/share/keyrings/jenkins.gpg] http://pkg.jenkins.io/debian-stable binary/ > /etc/apt/sources.list.d/jenkins.list'

sudo apt update

sudo apt install jenkins

sudo systemctl start jenkins.service

sudo systemctl status jenkins

sudo ufw allow 8080

sudo ufw allow OpenSSH
sudo ufw enable

sudo ufw status

sudo cat /var/lib/jenkins/secrets/initialAdminPassword 

========================================================================++++++++++++++++++++++++++++++
Install Docker Engine on Ubuntu  Ubuntu Focal 20.04 (LTS)
https://docs.docker.com/engine/install/ubuntu/

sudo apt-get remove docker docker-engine docker.io containerd runc
sudo apt-get update

sudo apt-get install \
    ca-certificates \
    curl \
    gnupg \
    lsb-release

sudo mkdir -p /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg

echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
  $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

sudo apt-get update

sudo chmod a+r /etc/apt/keyrings/docker.gpg
sudo apt-get update

sudo apt-get install docker-ce docker-ce-cli containerd.io docker-compose-plugin
=============++++++++++++++++++++++++++++
anji@anji:~$ docker pull nginx
Using default tag: latest
Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post 
"http://%2Fvar%2Frun%2Fdocker.sock/v1.24/images/create?fromImage=nginx&tag=latest": dial unix /var/run/docker.sock: connect: permission denied


visudo  add super user permistions
root    ALL=(ALL:ALL) ALL
anji  ALL=(ALL:ALL)     ALL
     or 
anji   ALL=(ALL)  NOPASSWORD:ALL

root@anji:/var/run# ll | grep -i docker.sock 
srw-rw----  1 root              docker    0 Jan 26 14:32 docker.sock=
root@anji:/var/run# chown -R anji docker.sock 
root@anji:/var/run# ll | grep -i docker.sock 
srw-rw----  1 anji              docker    0 Jan 26 14:32 docker.sock=
root@anji:/var/run# 




=================+++++++++++++++++++++++++++++++++++++++##################################################3
How to Deploy To Kubernetes with Jenkins GitOps GitHub Pipeline
Jenkins Kubernetes Integration - Mithun Technologies - 9980923226
https://www.youtube.com/watch?v=IluhOk86prA&t=3s
https://faun.pub/ci-cd-pipeline-using-jenkins-to-deploy-on-kubernetes-cf2fd5e185b8


pipeline {
agent any
stages{
  stage(" checkout from gitgub") {
    steps{
      checkout scmGit(branches: [[name: '*/master']], extensions: [], userRemoteConfigs: [[url: 'https://github.com/rritsoft/jenkins-with-kuberntes.git']])
      }

  }
      
    stage('Deploy App') {
      steps {
        script {
          kubernetesDeploy(configs: "pod.yaml", kubeconfigId: "k8oldplugin")
        }
      }
    }  



}

}
================="   "mutliple manifest files"
pipeline {
agent any
stages{
  stage(" checkout from gitgub") {
    steps{
      checkout scmGit(branches: [[name: '*/master']], extensions: [], userRemoteConfigs: [[url: 'https://github.com/rritsoft/jenkins-with-kuberntes.git']])
      }

  }
      
    stage('Deploy App') {
      steps {
        script {
          kubernetesDeploy(configs: "pod.yaml,nginx1.yaml",  kubeconfigId: "k8oldplugin")
     
        }
      }
    }  


}

}

==========================++++++++++++++++++++==============================

pipeline {
agent any
stages{
  stage(" checkout from gitgub") {
    steps{
      checkout scmGit(branches: [[name: '*/master']], extensions: [], userRemoteConfigs: [[url: 'https://github.com/rritsoft/jenkins-with-kuberntes.git']])
      }

  }
      
//     stage('Deploy App') {
//       steps {
//         script {
//           kubernetesDeploy(configs: "pod.yaml,nginx1.yaml,dp3.yaml",    kubeconfigId: "k8oldplugin")
           
//         }
//       }
//     }  

  stage( " cli commands through deployment" ) {
    steps{
      sh 'kubectl apply -f dp3.yaml'
    
    }  
  }

}

}
====================+++++++++++++++++++++++++++++++++++++++++============ "
https://www.youtube.com/watch?v=adG0vq5boL8&t=9336s
https://github.com/DeekshithSN/CICD_Java_gradle_application/blob/main/Jenkinsfile

pipeline{
    agent any 
    environment{
        VERSION = "${env.BUILD_ID}"
    }
    stages{
        stage("sonar quality check"){
            agent {
                docker {
                    image 'openjdk:11'
                }
            }
            steps{
                script{
                    withSonarQubeEnv(credentialsId: 'sonar-token') {
                            sh 'chmod +x gradlew'
                            sh './gradlew sonarqube'
                    }

                    timeout(time: 1, unit: 'HOURS') {
                      def qg = waitForQualityGate()
                      if (qg.status != 'OK') {
                           error "Pipeline aborted due to quality gate failure: ${qg.status}"
                      }
                    }

                }  
            }
        }
        stage("docker build & docker push"){
            steps{
                script{
                    withCredentials([string(credentialsId: 'docker_pass', variable: 'docker_password')]) {
                             sh '''
                                docker build -t 34.125.214.226:8083/springapp:${VERSION} .
                                docker login -u admin -p $docker_password 34.125.214.226:8083 
                                docker push  34.125.214.226:8083/springapp:${VERSION}
                                docker rmi 34.125.214.226:8083/springapp:${VERSION}
                            '''
                    }
                }
            }
        }
        stage('indentifying misconfigs using datree in helm charts'){
            steps{
                script{

                    dir('kubernetes/') {
                        withEnv(['DATREE_TOKEN=GJdx2cP2TCDyUY3EhQKgTc']) {
                              sh 'helm datree test myapp/'
                        }
                    }
                }
            }
        }
        stage("pushing the helm charts to nexus"){
            steps{
                script{
                    withCredentials([string(credentialsId: 'docker_pass', variable: 'docker_password')]) {
                          dir('kubernetes/') {
                             sh '''
                                 helmversion=$( helm show chart myapp | grep version | cut -d: -f 2 | tr -d ' ')
                                 tar -czvf  myapp-${helmversion}.tgz myapp/
                                 curl -u admin:$docker_password http://34.125.214.226:8081/repository/helm-hosted/ --upload-file myapp-${helmversion}.tgz -v
                            '''
                          }
                    }
                }
            }
        }

        stage('manual approval'){
            steps{
                script{
                    timeout(10) {
                        mail bcc: '', body: "<br>Project: ${env.JOB_NAME} <br>Build Number: ${env.BUILD_NUMBER} <br> Go to build url and approve the deployment request <br> URL de build: ${env.BUILD_URL}", cc: '', charset: 'UTF-8', from: '', mimeType: 'text/html', replyTo: '', subject: "${currentBuild.result} CI: Project name -> ${env.JOB_NAME}", to: "deekshith.snsep@gmail.com";  
                        input(id: "Deploy Gate", message: "Deploy ${params.project_name}?", ok: 'Deploy')
                    }
                }
            }
        }

        stage('Deploying application on k8s cluster') {
            steps {
               script{
                   withCredentials([kubeconfigFile(credentialsId: 'kubernetes-config', variable: 'KUBECONFIG')]) {
                        dir('kubernetes/') {
                          sh 'helm upgrade --install --set image.repository="34.125.214.226:8083/springapp" --set image.tag="${VERSION}" myjavaapp myapp/ ' 
                        }
                    }
               }
            }
        }

        stage('verifying app deployment'){
            steps{
                script{
                     withCredentials([kubeconfigFile(credentialsId: 'kubernetes-config', variable: 'KUBECONFIG')]) {
                         sh 'kubectl run curl --image=curlimages/curl -i --rm --restart=Never -- curl myjavaapp-myapp:8080'

                     }
                }
            }
        }
    }

    post {
		always {
			mail bcc: '', body: "<br>Project: ${env.JOB_NAME} <br>Build Number: ${env.BUILD_NUMBER} <br> URL de build: ${env.BUILD_URL}", cc: '', charset: 'UTF-8', from: '', mimeType: 'text/html', replyTo: '', subject: "${currentBuild.result} CI: Project name -> ${env.JOB_NAME}", to: "deekshith.snsep@gmail.com";  
		 }
	   }
}
Footer

==================================================
https://www.youtube.com/watch?v=o4QG_kqYvHk&t=1112s
https://github.com/saha-rajdeep/kubernetescode/blob/main/Jenkinsfile

node {
    def app

    stage('Clone repository') {
      

        checkout scm
    }

    stage('Build image') {
  
       app = docker.build("raj80dockerid/test")
    }

    stage('Test image') {
  

        app.inside {
            sh 'echo "Tests passed"'
        }
    }

    stage('Push image') {
        
        docker.withRegistry('https://registry.hub.docker.com', 'dockerhub') {
            app.push("${env.BUILD_NUMBER}")
        }
    }
    
    stage('Trigger ManifestUpdate') {
                echo "triggering updatemanifestjob"
                build job: 'updatemanifest', parameters: [string(name: 'DOCKERTAG', value: env.BUILD_NUMBER)]
        }
}

=====================++++++++++++++++++++++====
https://github.com/ankit630/unixcloudfusion/tree/main/jenkins-dynamic-slaves-kubernetes      ===+  different jobs eecucted  freestyle jobs , one by one
https://www.youtube.com/watch?v=ksz0Eqyttgo     
==========================================================================================================================================================
https://github.com/DeekshithSN/CICD_Java_gradle_application/blob/main/Jenkinsfile
https://www.youtube.com/watch?v=adG0vq5boL8&t=9460s

pipeline{
    agent any 
    environment{
        VERSION = "${env.BUILD_ID}"
    }
    stages{
        stage("sonar quality check"){
            agent {
                docker {
                    image 'openjdk:11'
                }
            }
            steps{
                script{
                    withSonarQubeEnv(credentialsId: 'sonar-token') {
                            sh 'chmod +x gradlew'
                            sh './gradlew sonarqube'
                    }

                    timeout(time: 1, unit: 'HOURS') {
                      def qg = waitForQualityGate()
                      if (qg.status != 'OK') {
                           error "Pipeline aborted due to quality gate failure: ${qg.status}"
                      }
                    }

                }  
            }
        }
        stage("docker build & docker push"){
            steps{
                script{
                    withCredentials([string(credentialsId: 'docker_pass', variable: 'docker_password')]) {
                             sh '''
                                docker build -t 34.125.214.226:8083/springapp:${VERSION} .
                                docker login -u admin -p $docker_password 34.125.214.226:8083 
                                docker push  34.125.214.226:8083/springapp:${VERSION}
                                docker rmi 34.125.214.226:8083/springapp:${VERSION}
                            '''
                    }
                }
            }
        }
        stage('indentifying misconfigs using datree in helm charts'){
            steps{
                script{

                    dir('kubernetes/') {
                        withEnv(['DATREE_TOKEN=GJdx2cP2TCDyUY3EhQKgTc']) {
                              sh 'helm datree test myapp/'
                        }
                    }
                }
            }
        }
        stage("pushing the helm charts to nexus"){
            steps{
                script{
                    withCredentials([string(credentialsId: 'docker_pass', variable: 'docker_password')]) {
                          dir('kubernetes/') {
                             sh '''
                                 helmversion=$( helm show chart myapp | grep version | cut -d: -f 2 | tr -d ' ')
                                 tar -czvf  myapp-${helmversion}.tgz myapp/
                                 curl -u admin:$docker_password http://34.125.214.226:8081/repository/helm-hosted/ --upload-file myapp-${helmversion}.tgz -v
                            '''
                          }
                    }
                }
            }
        }

        stage('manual approval'){
            steps{
                script{
                    timeout(10) {
                        mail bcc: '', body: "<br>Project: ${env.JOB_NAME} <br>Build Number: ${env.BUILD_NUMBER} <br> Go to build url and approve the deployment request <br> URL de build: ${env.BUILD_URL}", cc: '', charset: 'UTF-8', from: '', mimeType: 'text/html', replyTo: '', subject: "${currentBuild.result} CI: Project name -> ${env.JOB_NAME}", to: "deekshith.snsep@gmail.com";  
                        input(id: "Deploy Gate", message: "Deploy ${params.project_name}?", ok: 'Deploy')
                    }
                }
            }
        }

        stage('Deploying application on k8s cluster') {
            steps {
               script{
                   withCredentials([kubeconfigFile(credentialsId: 'kubernetes-config', variable: 'KUBECONFIG')]) {
                        dir('kubernetes/') {
                          sh 'helm upgrade --install --set image.repository="34.125.214.226:8083/springapp" --set image.tag="${VERSION}" myjavaapp myapp/ ' 
                        }
                    }
               }
            }
        }

        stage('verifying app deployment'){
            steps{
                script{
                     withCredentials([kubeconfigFile(credentialsId: 'kubernetes-config', variable: 'KUBECONFIG')]) {
                         sh 'kubectl run curl --image=curlimages/curl -i --rm --restart=Never -- curl myjavaapp-myapp:8080'

                     }
                }
            }
        }
    }

    post {
		always {
			mail bcc: '', body: "<br>Project: ${env.JOB_NAME} <br>Build Number: ${env.BUILD_NUMBER} <br> URL de build: ${env.BUILD_URL}", cc: '', charset: 'UTF-8', from: '', mimeType: 'text/html', replyTo: '', subject: "${currentBuild.result} CI: Project name -> ${env.JOB_NAME}", to: "deekshith.snsep@gmail.com";  
		 }
	   }
}

====================++++++++++++@@@@@@@@@@\\\///////\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
error  error  {}{{}{[]{}{{}{{{}}}}}}{{{{{{{{{}{{{{{{{{{{{{}{{{{[]{}|}{}}}}}}}}}}}}}}}}}}}}}}}

root@master:~# kubectl exec -it nginx --  /bin/bash
Error from server: error dialing backend: dial tcp 192.168.122.122:10250: connect: no route to host

root@master:~# ss -tnpl |grep 10250
LISTEN 0      4096                 *:10250            *:*    users:(("kubelet",pid=835,fd=15))        
root@master:~# systemctl disable firewalld && systemctl stop firewalld

  Removed /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service. 
  Removed /etc/systemd/system/multi-user.target.wants/firewalld.service.

root@master:~# ss -tnpl | grep 10250
LISTEN 0      4096                 *:10250            *:*    users:(("kubelet",pid=835,fd=15))        
root@master:~# 

root@master:~# kubectl get pods -n kube-system -o wide
NAME                             READY   STATUS    RESTARTS        AGE    IP                NODE     NOMINATED NODE   READINESS GATES
coredns-787d4945fb-6whjd         1/1     Running   5 (137m ago)    3d7h   10.32.0.2         master   <none>           <none>
coredns-787d4945fb-9f8fs         1/1     Running   5 (137m ago)    3d7h   10.32.0.3         master   <none>           <none>
etcd-master                      1/1     Running   5 (137m ago)    3d7h   192.168.122.103   master   <none>           <none>
kube-apiserver-master            1/1     Running   33 (137m ago)   3d7h   192.168.122.103   master   <none>           <none>
kube-controller-manager-master   1/1     Running   9 (137m ago)    3d7h   192.168.122.103   master   <none>           <none>
kube-proxy-2mh4b                 1/1     Running   5 (137m ago)    3d7h   192.168.122.103   master   <none>           <none>
kube-proxy-x64fm                 1/1     Running   5 (137m ago)    3d7h   192.168.122.122   dev      <none>           <none>
kube-scheduler-master            1/1     Running   9 (137m ago)    3d7h   192.168.122.103   master   <none>           <none>
weave-net-2v8tq                  2/2     Running   11 (137m ago)   3d7h   192.168.122.103   master   <none>           <none>
weave-net-8pnpm                  2/2     Running   10 (137m ago)   3d7h   192.168.122.122   dev      <none>           <none>
=================="
systemctl daemon-reload
systemctl restart kube-apiserver
    results look see "   TO  CHECK  THE NODE/WORKER  
dev@dev:~$ systemctl status firewalld 
\u25cf firewalld.service - firewalld - dynamic firewall daemon
     Loaded: loaded (/lib/systemd/system/firewalld.service; enabled; vendor pre>
     Active: active (running) since Thu 2023-01-26 20:33:40 IST; 3min 50s ago
       Docs: man:firewalld(1)               ####  stop firewalld  in      WORKER OR NODE 
   Main PID: 607 (firewalld)
      Tasks: 2 (limit: 4626)
     Memory: 31.6M
        CPU: 401ms
     CGroup: /system.slice/firewalld.service
             \u2514\u2500607 /usr/bin/python3 /usr/sbin/firewalld --nofork --nopid

Jan 26 20:33:39 dev systemd[1]: Starting firewalld - dynamic firewall daemon...
Jan 26 20:33:40 dev systemd[1]: Started firewalld - dynamic firewall daemon.

dev@dev:~$ systemctl stop firewalld 
dev@dev:~$ lsof -i | grep :10250

root@master:~# kubectl exec -it  nginx -- /bin/bash                 ####  SUCCESS RESULTS OK 
root@nginx:/# ls 
bin   dev		   docker-entrypoint.sh  home  lib64  mnt  proc  run   srv  tmp  var
boot  docker-entrypoint.d  etc			 lib   media  opt  root  sbin  sys  usr
root@nginx:/# cd /usr/share/nginx/html/
root@nginx:/usr/share/nginx/html# ls
50x.html  index.html
root@nginx:/usr/share/nginx/html# 
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  type: NodePort
  selector:
    app: myapp
  ports:
  - port: 80
    targetPort: 80
    nodePort: 30008

curl <node-ip>:<node-port>        # curl <node-ip>:31000
curl <service-ip>:<service-port>  # curl <svc-ip>:8090
curl <pod-ip>:<target-port>       # curl <pod-ip>:80

https://komodor.com/learn/kubernetes-service-examples-basic-usage-and-troubleshooting/
apiVersion: v1
kind: Service
metadata:
  name: my-clusterip-service
spec:
  type: ClusterIP
  clusterIP: 10.10.5.10
  ports:
 â€”name: http
    protocol: TCP
    port: 80
    targetPort: 8080
-----=========
apiVersion: v1
kind: Service
metadata:
  name: my-nodeport-service
spec:
  type: NodePort
  selector:
    app: nginx
  ports:
 â€”name: http
    protocol: TCP
    port: 80
    targetPort: 8080
    nodePort: 30000
----========
apiVersion: v1
kind: Service
metadata:
  name: my-loadbalancer-service
spec:
  type: LoadBalancer
  clusterIP: 10.0.160.135
  loadBalancerIP: 168.196.90.10
  selector:
    app: nginx
  ports:
 â€”name: http
    protocol: TCP
    port: 80
    targetPort: 8080
----================
 apiVersion: v1
kind: Service
metadata:
  name: my-externalname-service
spec:
  type: ExternalName
  externalName: my.database.domain.com
======-----------------


######################################\\\\//\/\/////////////////////////\/\\\\\\\\\\\\\\\\\\/\/\\\/\\/\\\\\\\\\\//////\\\\\\\\\
PROJECT ONE =   =   =  PROJECT -1  ########@@@@@@@@@@@@@@============
pipeline {
agent any
stages{
  stage("checkout from git "){
    steps {
        checkout scmGit(branches: [[name: '*/master']], extensions: [], userRemoteConfigs: [[url: 'https://github.com/rritsoft/jenkins-with-kuberntes.git']])
    }
  }       
 stage("docker build image") {
    steps{
        sh 'docker build -t  anjireddy3993/nginx:latest . '
        sh 'docker images'
        sh 'docker login -u anjireddy3993  -p ASDasd123$'
        sh 'docker push  anjireddy3993/nginx:latest'
        echo " all is success ok  "
    }
 }
 stage (" kube manifest files"){
  steps{
    script {
      kubernetesDeploy(configs: "pod.yaml,nginx1.yaml,dp3.yaml", kubeconfigId: "k8oldplugin")
     }
  }
 }  
    }
}
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
FROM  nginx:latest
COPY  default.conf /etc/nginx/conf.d/  
COPY   index.html /usr/share/nginx/html/
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
## default.conf
server {
    listen       80;
    server_name  localhost;
    location / {
        root   /usr/share/nginx/html;
        index  index.html index.htm;
    }
    error_page   500 502 503 504  /50x.html;
    location = /50x.html {
        root   /usr/share/nginx/html;
    }
    ssi on;
}
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
---
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: nginx-dp
  labels: 
    app: nginx-dp
spec: 
  replicas: 3
  selector:
    matchLabels: 
       app: nginx-dp
  template:    
      metadata: 
        labels: 
         app: nginx-dp
      spec: 
       containers: 
         - name: nginx-dp
           image: anjireddy3993/nginx:latest
           ports: 
            - containerPort: 80        
---
apiVersion: v1
kind: Service
metadata:
  name: outside
spec: 
  selector: 
    name: nginx-dp
  type: NodePort
  ports: 
  - protocol: TCP
    port: 80  
    targetPort: 80
    nodePort: 30008
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
<!doctype html>
<html>
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <h>  ANJIREDDY VELPULS XXXXXXtyyXyyyyyX  </h>
    <title>Simple Transactional Email</title>
    <style>

<html>             
++++++++++++++++++++++++++++++++++++++++++++++++
anji@anji:~$ kubectl get pod,deployment,service  -o wide 
NAME                            READY   STATUS    RESTARTS   AGE   IP          NODE   NOMINATED NODE   READINESS GATES
pod/nginx                       1/1     Running   0          45s   10.44.0.4   dev    <none>           <none>
pod/nginx-dp-56d55b9676-6f2h5   1/1     Running   0          45s   10.44.0.1   dev    <none>           <none>
pod/nginx-dp-56d55b9676-kckdh   1/1     Running   0          45s   10.44.0.3   dev    <none>           <none>
pod/nginx-dp-56d55b9676-mjh4x   1/1     Running   0          45s   10.44.0.2   dev    <none>           <none>
pod/nginx1                      1/1     Running   0          45s   10.44.0.5   dev    <none>           <none>

NAME                       READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES                       SELECTOR
deployment.apps/nginx-dp   3/3     3            3           45s   nginx-dp     anjireddy3993/nginx:latest   app=nginx-dp

NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE     SELECTOR
service/kubernetes   ClusterIP   10.96.0.1        <none>        443/TCP        3m22s   <none>
service/outside      NodePort    10.104.205.255   <none>        80:30008/TCP   45s     name=nginx-dp
anji@anji:~$ 

=================================================================================================================
https://blog.tekspace.io/exposing-pod-as-a-nodeport-service/
https://katharharshal1.medium.com/steps-to-access-the-pod-from-outside-the-cluster-using-nodeport-e9c194b986df


W: http://apt.kubernetes.io/dists/kubernetes-xenial/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.

dev@dev:~$ apt-key list gazebo
Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).

dev@dev:~$ apt-key export gazebo | sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/gazebo-key.gpg
Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).
gpg: WARNING: nothing exported
gpg: no valid OpenPGP data found.

dev@dev:~$ cd /etc/apt

dev@dev:/etc/apt$ sudo cp trusted.gpg trusted.gpg.d

dev@dev:/etc/apt$ apt update 

===================================================


dev@master:~$ kubectl run ubuntu --rm -i --tty --restart=Never --image=ubuntu -- /bin/sh 

If you don't see a command prompt, try pressing enter.

# ls
bin   dev  home  lib32	libx32	mnt  proc  run	 srv  tmp  var
boot  etc  lib	 lib64	media	opt  root  sbin  sys  usr
# apt update -y
Ign:1 http://security.ubuntu.com/ubuntu jammy-security InRelease         
      
Err:1 http://security.ubuntu.com/ubuntu jammy-security InRelease         
  Temporary failure resolving 'security.ubuntu.com'
Err:2 http://archive.ubuntu.com/ubuntu jammy InRelease
  Temporary failure resolving 'archive.ubuntu.com'
Err:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease
  Temporary failure resolving 'archive.ubuntu.com'
Err:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease
All packages are up to date.
W: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/jammy/InRelease  Temporary failure resolving 'archive.ubuntu.com'
W: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/jammy-updates/InRelease  Temporary failure resolving 'archive.ubuntu.com'
W: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/jammy-backports/InRelease  Temporary failure resolving 'archive.ubuntu.com'
W: Failed to fetch http://security.ubuntu.com/ubuntu/dists/jammy-security/InRelease  Temporary failure resolving 'security.ubuntu.com'
W: Some index files failed to download. They have been ignored, or old ones used instead.


root@master:~# sudo service docker restart
Failed to restart docker.service: Unit docker.service not found.

root@master:~# sudo /etc/init.d/docker restart
sudo: /etc/init.d/docker: command not found

https://get.docker.com/
curl -fsSL https://get.docker.com -o get-docker.sh
sh get-docker.sh

results : 

root@master:~#  sudo service docker restart

root@master:~# sudo /etc/init.d/docker restart
Restarting docker (via systemctl): docker.service.

=================++++++++++++++++++++++++++++++++++++++++++
errors  [[][][][][][][[][][[[[][[]]]]]]]]]]]]]]]][[]][[[[[[]]]]]]]]]][][]  errors ERRORS  

W: http://apt.kubernetes.io/dists/kubernetes-xenial/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.

dev@dev:~$ apt-key list gazebo
Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).

dev@dev:~$ apt-key export gazebo | sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/gazebo-key.gpg
Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).
gpg: WARNING: nothing exported
gpg: no valid OpenPGP data found.

dev@dev:~$ cd /etc/apt

dev@dev:/etc/apt$ sudo cp trusted.gpg trusted.gpg.d

dev@dev:/etc/apt$ apt update 

===================================================


dev@master:~$ kubectl run ubuntu --rm -i --tty --restart=Never --image=ubuntu -- /bin/sh 

If you don't see a command prompt, try pressing enter.

# ls
bin   dev  home  lib32	libx32	mnt  proc  run	 srv  tmp  var
boot  etc  lib	 lib64	media	opt  root  sbin  sys  usr
# apt update -y
Ign:1 http://security.ubuntu.com/ubuntu jammy-security InRelease         
      
Err:1 http://security.ubuntu.com/ubuntu jammy-security InRelease         
  Temporary failure resolving 'security.ubuntu.com'
Err:2 http://archive.ubuntu.com/ubuntu jammy InRelease
  Temporary failure resolving 'archive.ubuntu.com'
Err:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease
  Temporary failure resolving 'archive.ubuntu.com'
Err:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease
All packages are up to date.
W: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/jammy/InRelease  Temporary failure resolving 'archive.ubuntu.com'
W: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/jammy-updates/InRelease  Temporary failure resolving 'archive.ubuntu.com'
W: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/jammy-backports/InRelease  Temporary failure resolving 'archive.ubuntu.com'
W: Failed to fetch http://security.ubuntu.com/ubuntu/dists/jammy-security/InRelease  Temporary failure resolving 'security.ubuntu.com'
W: Some index files failed to download. They have been ignored, or old ones used instead.


root@master:~# sudo service docker restart
Failed to restart docker.service: Unit docker.service not found.

root@master:~# sudo /etc/init.d/docker restart
sudo: /etc/init.d/docker: command not found

https://get.docker.com/
curl -fsSL https://get.docker.com -o get-docker.sh
sh get-docker.sh

results : 

root@master:~#  sudo service docker restart

root@master:~# sudo /etc/init.d/docker restart
Restarting docker (via systemctl): docker.service.

=================++++++++++++++++++++++++++++++++++++++++++

systemctl stop firewalld 

sudo rm  -rf /etc/containerd/config.toml

sudo  systemctl restart containerd

sudo kubeadm init --pod-network-cidr=10.244.0.0/16

---------------------
https://stackoverflow.com/questions/65954590/want-to-running-nodeport-on-localhost-kube-services

dell@dell:~$ kubectl get pods 
NAME                        READY   STATUS    RESTARTS   AGE
nginx-dp-56d55b9676-h5jwg   1/1     Running   0          62m
nginx-dp-56d55b9676-qc2l7   1/1     Running   0          62m
nginx-dp-56d55b9676-qd4bm   1/1     Running   0          62m
dell@dell:~$    kubectl port-forward     nginx-dp-56d55b9676-h5jwg   8089:80  --address 0.0.0.0
Forwarding from 0.0.0.0:8089 -> 80
Handling connection for 8089

kubectl expose deployment  deploymentname --type=NodePort  --port  80  --target-port 8086

kubectl expose deployment   nginx-dp   --type=NodePort  --port  80  --target-port 8086


  kubectl port-forward    webserver-744d6b7964-pqnws    8089:80  --address 0.0.0.0

   kubectl port-forward    <podname>    8089:80  --address 0.0.0.0

   kubectl port-forward     nginx-dp-56d55b9676-h5jwg   8089:80  --address 0.0.0.0

   dell@master:~/.kube$  kubectl port-forward nginx-dp-56d55b9676-2rnsw  8089:80 --address 0.0.0.0     ==  important only run in  master ok see 
Forwarding from 0.0.0.0:8089 -> 80                    run in only master 
Handling connection for 8089
^Cdell@master:~/.kube$ 

  ==============================================+++++++++++-------------[][{[}{}{}{{{][}[[[]]]]]]]]}}}]}]

https://www.jenkins.io/doc/book/pipeline/docker/

node {
    checkout scm

    docker.withServer('tcp://swarm.example.com:2376', 'swarm-certs') {
        docker.image('mysql:5').withRun('-p 3306:3306') {
            /* do things */
        }
    }
}



node {
    checkout scm

    docker.withRegistry('https://registry.example.com', 'credentials-id') {

        def customImage = docker.build("my-image:${env.BUILD_ID}")

        /* Push the container to the custom Registry */
        customImage.push()
    }
}

---------------------
https://morioh.com/p/0d410225b84e
   script {
                 withCredentials([string(credentialsId: 'dockerhub-pwd', variable: 'dockerhubpwd')]) {
                    sh 'docker login -u devopshint -p ${dockerhubpwd}'
                 }  
-----
https://medium.com/@dibaekhanal101/jenkins-to-push-docker-image-to-docker-hub-5aefeb48a6c2
withCredentials([string(credentialsId: â€˜json101â€™, variable: â€˜dockerhubpwdâ€™)]) {
sh â€˜docker login -u json101 -p ${dockerhubpwd}â€™
sh â€˜docker push json101/javappâ€™
}
----------------------------
https://www.liatrio.com/blog/building-with-docker-using-jenkins-pipelines
  steps {
      	withCredentials([usernamePassword(credentialsId: 'dockerHub', passwordVariable: 'dockerHubPassword', usernameVariable: 'dockerHubUser')]) {
        	sh "docker login -u ${env.dockerHubUser} -p ${env.dockerHubPassword}"
          sh 'docker push shanem/spring-petclinic:latest'
        }
      }

----------------------
https://iot4beginners.com/how-to-push-a-docker-image-to-the-docker-hub-using-jenkins-pipeline-2022-ci-cd/
        withDockerRegistry([ credentialsId: "dockerhubaccount", url: "" ]) {
        dockerImage.push()
        }
    }   
https://stackoverflow.com/questions/59254492/how-to-pass-credentials-for-jenkins-to-push-a-docker-image-to-my-own-registry
=============================================================================================================================
   PYTHON PLASK APP BUILD IMAGE DOCKER PUSH 
https://runnable.com/docker/python/dockerize-your-flask-application
https://www.digitalocean.com/community/tutorials/how-to-build-and-deploy-a-flask-application-using-docker-on-ubuntu-20-04
https://stuartmccoll.github.io/posts/add-a-flask-application-to-a-docker-container/
https://blog.logrocket.com/build-deploy-flask-app-using-docker/
https://synchronizing.medium.com/running-a-simple-flask-application-inside-a-docker-container-b83bf3e07dd5
https://www.freecodecamp.org/news/how-to-dockerize-a-flask-app/
https://stackoverflow.com/questions/62908052/exposing-flask-app-within-docker-to-internet

=============================================================================
https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/
kubectl port-forward pods/mongo-75f59d57f4-4nd6q 28015:27017
or
kubectl port-forward deployment/mongo 28015:27017
or
kubectl port-forward replicaset/mongo-75f59d57f4 28015:27017
or
kubectl port-forward service/mongo 28015:27017
==================
https://github.com/rritsoft/cicd-docker-kubernetes-project
python flask project  cicd jenkins 
app_service.py
--------------
import json

class AppService:
    
    tasks = [
        {
            'id': 1,
            'name': "task1",
            "description": "This is task 1"
        },
        {
            "id": 2,
            "name": "task2",
            "description": "This is task 2"
        },
        {
            "id": 3,
            "name": "task3",
            "description": "This is task 3"
        }
    ]

    def __init__(self):
        self.tasksJSON = json.dumps(self.tasks)

    def get_tasks(self):
        return self.tasksJSON

    def create_task(self,task):
        tasksData = json.loads(self.tasksJSON)
        tasksData.append(task)
        self.tasksJSON = json.dumps(tasksData)
        return self.tasksJSON

    def update_task(self, request_task):
        tasksData = json.loads(self.tasksJSON)
        for task in tasksData:
            if task["id"] == request_task['id']:
                task.update(request_task)
                return json.dumps(tasksData);
        return json.dumps({'message': 'task id not found'});

    def delete_task(self, request_task_id):
        tasksData = json.loads(self.tasksJSON)
        for task in tasksData:
            if task["id"] == request_task_id:
                tasksData.remove(task)
                return json.dumps(tasksData);
        return json.dumps({'message': 'task id not found'});

------ 
app.py

from flask import Flask, request
from app_service import AppService
import json

app = Flask(__name__)
appService = AppService();


@app.route('/')
def home():
    return "App Works!!!"


@app.route('/api/tasks')
def tasks():
    return appService.get_tasks()

@app.route('/api/task', methods=['POST'])
def create_task():
    request_data = request.get_json()
    task = request_data['task']
    return appService.create_task(task)


@app.route('/api/task', methods=['PUT'])
def update_task():
    request_data = request.get_json()
    return appService.update_task(request_data['task'])


@app.route('/api/task/<int:id>', methods=['DELETE'])
def delete_task(id):
    return appService.delete_task(id)

-----------------------
dockerfile
FROM python:3.7

WORKDIR /opt/app

COPY . .

RUN pip install --no-cache-dir -r requirements-prod.txt

EXPOSE 5000

CMD ["python3", "-m", "flask", "run", "--host=0.0.0.0"]
-------------------------
requirements.txt
flask
python-dotenv
--------
requirements-prod.txt

 -r requirements.txt
-------------------------------
.flaskenv
FLASK_APP=app.py
FLASK_ENV=development
--------
pipeline {
agent any 
stages {
   stage("checkout github"){
     steps{
       checkout scmGit(branches: [[name: '*/master']], extensions: [], userRemoteConfigs: [[url: 'https://github.com/rritsoft/cicd-docker-kubernetes-project.git']])

     }
   }
  stage(" docker build and push image") {
    steps{

      withCredentials([usernamePassword(credentialsId: 'dockerhub', passwordVariable: 'dockerhubPassword', usernameVariable: 'dockerhubUser')]) {
          sh 'docker build -t  anjireddy3993/flaskpro:latest   .'
          sh "docker login -u ${env.dockerhubUser} -p ${env.dockerhubPassword}"
          // sh 'docker run -d -p 5000:5000 anjireddy3993/flaskpro'
          sh 'docker push  anjireddy3993/flaskpro:latest '
      }
   
    }
  }
  stage(" kubectl run commands   "){
    steps{
       withKubeConfig(caCertificate: '', clusterName: 'kubernetes', contextName: 'kubernetes-admin@kubernetes', credentialsId: 'k8file', namespace: '', restrictKubeConfigAccess: false, serverUrl: ' https://192.168.122.31:6443') {
          sh 'kubectl get nodes'
          sh ' kubectl get pods'
          // sh 'kubectl apply -f dp3.yaml'
          // sh 'kubectl apply -f pod.yaml '
          sh 'kubectl apply -f nginx1.yaml'
          sh 'kubectl port-forward  nginx1  5001:5000 '
    }



    }
  }  


}    
}
--------------------------------------

running multiple steps - jenkins pipeline paused stage 3 minutes

https://www.jenkins.io/doc/pipeline/tour/running-multiple-steps/

Jenkinsfile (Declarative Pipeline)

pipeline {
    agent any
    stages {
        stage('Deploy') {
            steps {
                timeout(time: 3, unit: 'MINUTES') {
                    retry(5) {
                        sh './flakey-deploy.sh'
                    }
                }
            }
        }
    }
}
---------
https://stackoverflow.com/questions/63498513/jenkins-pipeline-start-a-stage-2-hours-after-the-1st-one


You can use "sleep" within a stage to pause its execution.

stage("B") {
    steps {
        echo "Pausing stage B"
        sleep(time: 2, unit: "HOURS")
    }
}

----------------------
https://e.printstacktrace.blog/how-to-time-out-jenkins-pipeline-stage-and-keep-the-pipeline-running/
pipeline {
    agent any

    stages {
        stage("A") {
            options {
                timeout(time: 3, unit: "SECONDS")
            }

            steps {
                script {
                    Exception caughtException = null

                    catchError(buildResult: 'SUCCESS', stageResult: 'ABORTED') { 
                        try { 
                            echo "Started stage A"
                            sleep(time: 5, unit: "SECONDS")
                        } catch (org.jenkinsci.plugins.workflow.steps.FlowInterruptedException e) {
                            error "Caught ${e.toString()}" 
                        } catch (Throwable e) {
                            caughtException = e
                        }
                    }

                    if (caughtException) {
                        error caughtException.message
                    }
                }
            }
        }

        stage("B") {
            steps {
                echo "Started stage B"
            }
        }
    }
}
========================================================================================


Manage Docker as a non-root userðŸ”—

sudo groupadd docker
sudo usermod -aG docker $USER

newgrp docker
docker run hello-world

WARNING: Error loading config file: /home/user/.docker/config.json -
stat /home/user/.docker/config.json: permission denied

sudo chown "$USER":"$USER" /home/"$USER"/.docker -R
sudo chmod g+rwx "$HOME/.docker" -R

sudo systemctl enable docker.service
sudo systemctl enable containerd.service

sudo systemctl disable docker.service
sudo systemctl disable containerd.service
--------------------------------------------------
Docker: Temporary failure resolving 'deb.debian.org'
https://docs.docker.com/engine/install/linux-postinstall/#specify-dns-servers-for-docker


root@nginx:/# apt-get update 
Err:1 http://deb.debian.org/debian bullseye InRelease
  Temporary failure resolving 'deb.debian.org'
Err:2 http://deb.debian.org/debian-security bullseye-security InRelease
  Temporary failure resolving 'deb.debian.org'
Err:3 http://deb.debian.org/debian bullseye-updates InRelease
  Temporary failure resolving 'deb.debian.org'
Reading package lists... Done    
W: Failed to fetch http://deb.debian.org/debian/dists/bullseye/InRelease  Temporary failure resolving 'deb.debian.org'
W: Failed to fetch http://deb.debian.org/debian-security/dists/bullseye-security/InRelease  Temporary failure resolving 'deb.debian.org'
W: Failed to fetch http://deb.debian.org/debian/dists/bullseye-updates/InRelease  Temporary failure resolving 'deb.debian.org'
W: Some index files failed to download. They have been ignored, or old ones used instead.
root@nginx:/# 

https://stackoverflow.com/questions/61567404/docker-temporary-failure-resolving-deb-debian-org
https://stackoverflow.com/questions/61567404/docker-temporary-failure-resolving-deb-debian-org

docker run -i -t nginx:latest /bin/bash

apt-get update
apt-get install nano
export TERM=xterm

sudo service docker restart or sudo /etc/init.d/docker restart




Specifying a DNS server for docker containers helped me.

Create a /etc/docker/daemon.json file with this content:

{
  "dns": ["8.8.8.8", "8.8.4.4"]
}

sudo service docker restart




Perhaps the network on the VM is not communicating with the default network created by docker during the build (bridge), so try "host" network :

docker build --network host -t [image_name]

https://www.digitalocean.com/community/questions/apt-error-temporary-failure-resolving-deb-debian-org

echo -e "nameserver 8.8.8.8\nnameserver 8.8.4.4" |sudo tee -a /etc/resolv.conf



I easily resolved it via:

- docker exec -it nginx bash (Go inside container)
- ping google.com (if not working)
- exit (Exit from container)
- sudo service docker restart

Please also confirms /etc/sysctl.conf

- net.ipv4.ip_forward = 1

sudo sysctl -p /etc/sysctl.conf
/etc/init.d/docker restart




I was having the wrong DNS IP address in my /etc/docker/daemon.json. In my case, it was my home router DNS IP address and I was trying from the office network. I found out my office DNS and updated with that.

{ 
  "dns": ["192.168.1.1"] 
}

============================================================+++++++++++++++
https://phoenixnap.com/kb/temporary-failure-in-name-resolution
ping  google.com

Method 1: Badly Configured resolv.conf File

resolv.conf is a file for configuring DNS servers on Linux systems.

To start, open the file in a text editor such as nano.

sudo nano /etc/resolv.conf

sudo systemctl restart systemd-resolved.service

sudo chown root:root /etc/resolv.conf

sudo chmod 777 /etc/resolv.conf

Method 2: Firewall Restrictions:::---

sudo ufw allow 43/tcp
sudo ufw allow 53/tcp

root@master:~# sudo ufw reload
Firewall not enabled (skipping reload)
root@master:~# ufw enable 

root@master:~# sudo ufw reload
Firewall reloaded

root@master:~# sudo firewall-cmd --add-port=43/tcp --permanent
FirewallD is not running
root@master:~# systemctl start firewalld 
root@master:~# sudo firewall-cmd --add-port=43/tcp --permanent
success
root@master:~# sudo firewall-cmd --add-port=53/tcp --permanent

sudo firewall-cmd --reload

-----------------------------------------
https://forum.linuxfoundation.org/discussion/855667/network-does-not-works-on-a-bash-shell-inside-the-new-pod

root@master:~# kubectl exec nginx -it -- /bin/bash 
root@nginx:/# cat /etc/resolv.conf 
search default.svc.cluster.local svc.cluster.local cluster.local
nameserver 10.96.0.10
options ndots:5
root@nginx:/# ping 4.2.2.2 
bash: ping: command not found

root@nginx:/# cat /etc/resolv.conf
search default.svc.cluster.local svc.cluster.local cluster.local
nameserver 10.96.0.10
options ndots:5
root@nginx:/# ping 4.2.2.2
bash: ping: command not found
root@nginx:/# "echo nameserver 8.8.8.8 > /etc/resolv.conf"
bash: echo nameserver 8.8.8.8 > /etc/resolv.conf: No such file or directory
root@nginx:/# echo nameserver 8.8.8.8 > /etc/resolv.conf
root@nginx:/# apt update -y
Err:1 http://deb.debian.org/debian bullseye InRelease
Temporary failure resolving 'deb.debian.org'
0% [Connecting to deb.debian.org]
----------------------------------------
root@master:~# kubectl run ubuntu -it --image=ubuntu  /bin/bash
If you don't see a command prompt, try pressing enter.
root@ubuntu:/# ls
bin  boot  dev  etc  home  lib  lib32  lib64  libx32  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var
root@ubuntu:/# apt update -t
root@ubuntu:/# cat /etc/lsb-release 
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=22.04
DISTRIB_CODENAME=jammy
DISTRIB_DESCRIPTION="Ubuntu 22.04.1 LTS"
root@ubuntu:/# exit
exit
Session ended, resume using 'kubectl attach ubuntu -c ubuntu -i -t' command when the pod is running
root@master:~# kubectl attach ubuntu -c ubuntu -i -t     =  best help this command exit
If you don't see a command prompt, try pressing enter.

oot@master:~# kubectl run ubuntu1  -it --image=ubuntu:18.04
If you don't see a command prompt, try pressing enter.
root@ubuntu1:/# apt update -y
Err:1 http://archive.ubuntu.com/ubuntu bionic InRelease                  
  Temporary failure resolving 'archive.ubuntu.com'
Err:2 http://security.ubuntu.com/ubuntu bionic-security InRelease        
  Temporary failure resolving 'security.ubuntu.com'
Err:3 http://archive.ubuntu.com/ubuntu bionic-updates InRelease          
  Temporary failure resolving 'archive.ubuntu.com'
0% [Connecting to archive.ubuntu.com]^C
root@ubuntu1:/# cat /etc/lsb-release 
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=18.04
DISTRIB_CODENAME=bionic
DISTRIB_DESCRIPTION="Ubuntu 18.04.6 LTS"
root@ubuntu1:/# 
=======================================================================
root@master:~# ufw status 
Status: inactive
root@master:~# ufw enable
Firewall is active and enabled on system startup
root@master:~# systemctl status ufw 
â— ufw.service - Uncomplicated firewall
     Loaded: loaded (/lib/systemd/system/ufw.service; enabled; vendor preset: enabled)
     Active: active (exited) since Sat 2023-01-28 09:12:20 IST; 10h ago
       Docs: man:ufw(8)
   Main PID: 267 (code=exited, status=0/SUCCESS)
      Tasks: 0 (limit: 7644)
     Memory: 0B
     CGroup: /system.slice/ufw.service

Warning: journal has been rotated since unit was started, output may be incomplete.
root@master:~# systemctl status firewalld 
â— firewalld.service - firewalld - dynamic firewall daemon
     Loaded: loaded (/lib/systemd/system/firewalld.service; enabled; vendor preset: enabled)
     Active: inactive (dead) since Sat 2023-01-28 19:42:06 IST; 23min ago
       Docs: man:firewalld(1)
    Process: 51001 ExecStart=/usr/sbin/firewalld --nofork --nopid (code=exited, status=0/SUCCESS)
   Main PID: 51001 (code=exited, status=0/SUCCESS)

Jan 28 18:42:58 master systemd[1]: Starting firewalld - dynamic firewall daemon...
Jan 28 18:42:58 master systemd[1]: Started firewalld - dynamic firewall daemon.
Jan 28 19:42:05 master systemd[1]: Stopping firewalld - dynamic firewall daemon...
Jan 28 19:42:06 master systemd[1]: firewalld.service: Succeeded.
Jan 28 19:42:06 master systemd[1]: Stopped firewalld - dynamic firewall daemon.
root@master:~# systemctl start  firewalld 
root@master:~# systemctl status firewalld 
â— firewalld.service - firewalld - dynamic firewall daemon
     Loaded: loaded (/lib/systemd/system/firewalld.service; enabled; vendor preset: enabled)
     Active: active (running) since Sat 2023-01-28 20:05:36 IST; 7s ago
       Docs: man:firewalld(1)
   Main PID: 53508 (firewalld)
      Tasks: 2 (limit: 7644)
     Memory: 20.2M
     CGroup: /system.slice/firewalld.service
             â””â”€53508 /usr/bin/python3 /usr/sbin/firewalld --nofork --nopid

Jan 28 20:05:35 master systemd[1]: Starting firewalld - dynamic firewall daemon...
Jan 28 20:05:36 master systemd[1]: Started firewalld - dynamic firewall daemon.
root@master:~# ufw status 
Status: inactive
root@master:~# ufw enable
Firewall is active and enabled on system startup
root@master:~# ufw status 
Status: active

To                         Action      From
--                         ------      ----
43/tcp                     ALLOW       Anywhere                  
53/tcp                     ALLOW       Anywhere                  
43/tcp (v6)                ALLOW       Anywhere (v6)             
53/tcp (v6)                ALLOW       Anywhere (v6)             

root@master:~# 
=======================================================================
https://stackoverflow.com/questions/24991136/docker-build-could-not-resolve-archive-ubuntu-com-apt-get-fails-to-install-a 
https://robinwinslow.uk/fix-docker-networking-dns

root@master:~#  docker run --rm busybox nslookup google.com 
Unable to find image 'busybox:latest' locally
latest: Pulling from library/busybox
205dae5015e7: Pull complete 
Digest: sha256:7b3ccabffc97de872a30dfd234fd972a66d247c8cfc69b0550f276481852627c
Status: Downloaded newer image for busybox:latest
;; connection timed out; no servers could be reached

nslookup: write to '8.8.8.8': No route to host
nslookup: write to '8.8.4.4': No route to host
root@master:~#  nmcli dev show | grep 'IP4.DNS'
IP4.DNS[1]:                             192.168.122.1
root@master:~#  cd /etc/docker
root@master:/etc/docker# touch daemon.json

Put this in /etc/docker/daemon.json:

{                                                                          
    "dns": ["10.0.0.2", "10.0.0.3"]                                                                           
}     
     before ######"

     root@master:/etc/docker# cat daemon.json 
{
  "dns": ["8.8.8.8", "8.8.4.4"]
}
root@master:/etc/docker# 

--   after 


{                                                                          
    "dns": ["192.168.122.1", "192.168.122.1"]                                                                           
}  

root@master:/etc/docker# cat daemon.json 

{                                                                          
    "dns": ["192.168.122.1", "192.168.122.1"]                                                                           
}  

root@master:/etc/docker# 
root@master:/etc/docker# sudo service docker restart        main importent one step all solutions

     after   after   
 root@master:~# docker run --rm busybox nslookup google.com     success  
Server:		192.168.122.1  
Address:	192.168.122.1:53

Non-authoritative answer:
Name:	google.com
Address: 172.217.167.174

Non-authoritative answer:
Name:	google.com
Address: 2404:6800:4007:828::200e

--------------
$ vim /etc/default/docker # (uncomment the DOCKER_OPTS and add DNS IP)
DOCKER_OPTS="--dns 172.168.7.2 --dns 8.8.8.8 --dns 8.8.4.4"

$ rm `docker ps --no-trunc -aq` # (remove all the containers to avoid DNS cache)

$ docker rmi $(docker images -q) # (remove all the images)

$ service docker restart #(restart the docker to pick up dns setting)


               %%%%%######XXXXXXX  success   success ==   


 root@master:~# docker run  -it  ubuntu:16.04 /bin/bash 
Unable to find image 'ubuntu:16.04' locally
16.04: Pulling from library/ubuntu
58690f9b18fc: Pull complete 
b51569e7c507: Pull complete 
da8ef40b9eca: Pull complete 
fb15d46c38dc: Pull complete 
Digest: sha256:1f1a2d56de1d604801a9671f301190704c25d604a416f59e03c04f5c6ffee0d6
Status: Downloaded newer image for ubuntu:16.04
root@3672a327b550:/# ls
bin  boot  dev  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var
root@3672a327b550:/# cat /etc/lsb-release 
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=16.04
DISTRIB_CODENAME=xenial
DISTRIB_DESCRIPTION="Ubuntu 16.04.7 LTS"
root@3672a327b550:/# apt update -y
Err:1 http://archive.ubuntu.com/ubuntu xenial InRelease                  
  Temporary failure resolving 'archive.ubuntu.com'
Err:2 http://security.ubuntu.com/ubuntu xenial-security InRelease        
  Temporary failure resolving 'security.ubuntu.com'
Err:3 http://archive.ubuntu.com/ubuntu xenial-updates InRelease          
  Temporary failure resolving 'archive.ubuntu.com'
Err:4 http://archive.ubuntu.com/ubuntu xenial-backports InRelease
  Temporary failure resolving 'archive.ubuntu.com'
Reading package lists... Done        
Building dependency tree       
Reading state information... Done
All packages are up to date.
W: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/xenial/InRelease  Temporary failure resolving 'archive.ubuntu.com'
W: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/xenial-updates/InRelease  Temporary failure resolving 'archive.ubuntu.com'
W: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/xenial-backports/InRelease  Temporary failure resolving 'archive.ubuntu.com'
W: Failed to fetch http://security.ubuntu.com/ubuntu/dists/xenial-security/InRelease  Temporary failure resolving 'security.ubuntu.com'
W: Some index files failed to download. They have been ignored, or old ones used instead.
root@3672a327b550:/# ip a    
bash: ip: command not found
root@3672a327b550:/# cat /etc/host
cat: /etc/host: No such file or directory
root@3672a327b550:/# cat /etc/hosts
127.0.0.1	localhost
::1	localhost ip6-localhost ip6-loopback
fe00::0	ip6-localnet
ff00::0	ip6-mcastprefix
ff02::1	ip6-allnodes
ff02::2	ip6-allrouters
172.17.0.2	3672a327b550
root@3672a327b550:/# ping 172.17.0.2
bash: ping: command not found
root@3672a327b550:/# apt install ping
Reading package lists... Done
Building dependency tree       
Reading state information... Done
E: Unable to locate package ping
root@3672a327b550:/# exit
root@master:~# docker ps 
CONTAINER ID   IMAGE          COMMAND       CREATED         STATUS         PORTS     NAMES
b6a7424c3c18   b6f507652425   "/bin/bash"   4 minutes ago   Up 4 minutes             gifted_sutherland
root@master:~# docker images 
REPOSITORY   TAG       IMAGE ID       CREATED         SIZE
busybox      latest    66ba00ad3de8   3 weeks ago     4.87MB
ubuntu       latest    6b7dfa7e8fdb   7 weeks ago     77.8MB
ubuntu       16.04     b6f507652425   17 months ago   135MB
root@master:~# docker attach b6a7424c3c18  /bin/bash 
"docker attach" requires exactly 1 argument.
See 'docker attach --help'.

Usage:  docker attach [OPTIONS] CONTAINER

Attach local standard input, output, and error streams to a running container
root@master:~# docker attach b6a7424c3c18  
^C
root@b6a7424c3c18:/# 
root@b6a7424c3c18:/# 
root@b6a7424c3c18:/# ls 
bin  boot  dev  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var
root@b6a7424c3c18:/# apt update -y 

0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.
root@b6a7424c3c18:/# ip a 
bash: ip: command not found
root@b6a7424c3c18:/# apt install net-tools 
Reading package lists... Done

Setting up net-tools (1.60-26ubuntu1) ...
root@b6a7424c3c18:/# ip a 
bash: ip: command not found
root@b6a7424c3c18:/# apt install iproute2
Reading package lists... Done
Building dependency tree       

Fetched 586 kB in 1s (316 kB/s)    
debconf: delaying package configuration, since apt-utils is not installed
Selecting previously unselected package libatm1:amd64.
(Reading database ... 4833 files and directories currently installed.)

Processing triggers for libc-bin (2.23-0ubuntu11.3) ...
root@b6a7424c3c18:/# ip  a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
29: eth0@if30: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default 
    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0
       valid_lft forever               

==============================================================================================
https://www.techrepublic.com/article/how-to-deploy-and-use-a-mysql-docker-container/

=================================================================================
https://stackoverflow.com/questions/62072977/whats-default-password-in-docker-container-mysql-server-when-you-dont-set-one


root@anji:~# docker run --name mysql  mysql/mysql-server:latest 
Unable to find image 'mysql/mysql-server:latest' locally
latest: Pulling from mysql/mysql-server
6a4a3ef82cdc: Pull complete 
5518b09b1089: Pull complete 
b6b576315b62: Pull complete 
349b52643cc3: Pull complete 
abe8d2406c31: Pull complete 
c7668948e14a: Pull complete 
c7e93886e496: Pull complete 
Digest: sha256:d6c8301b7834c5b9c2b733b10b7e630f441af7bc917c74dba379f24eeeb6a313
Status: Downloaded newer image for mysql/mysql-server:latest
[Entrypoint] MySQL Docker Image 8.0.32-1.2.11-server
[Entrypoint] Initializing database
[Entrypoint] No password option specified for new database.
[Entrypoint]   A random onetime password will be generated.
2023-01-30T14:22:42.404089Z 0 [Warning] [MY-011068] [Server] The syntax '--skip-host-cache' is deprecated and will be removed in a future release. Please use SET GLOBAL host_cache_size=0 instead.
2023-01-30T14:22:42.404182Z 0 [System] [MY-013169] [Server] /usr/sbin/mysqld (mysqld 8.0.32) initializing of server in progress as process 17
2023-01-30T14:22:42.410396Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.
2023-01-30T14:22:42.789815Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.
2023-01-30T14:22:44.102987Z 6 [Warning] [MY-010453] [Server] root@localhost is created with an empty password ! Please consider switching off the --initialize-insecure option.
[Entrypoint] Database initialized
2023-01-30T14:22:47.511339Z 0 [Warning] [MY-011068] [Server] The syntax '--skip-host-cache' is deprecated and will be removed in a future release. Please use SET GLOBAL host_cache_size=0 instead.
2023-01-30T14:22:47.512792Z 0 [System] [MY-010116] [Server] /usr/sbin/mysqld (mysqld 8.0.32) starting as process 60
2023-01-30T14:22:47.527652Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.
2023-01-30T14:22:47.645254Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.
2023-01-30T14:22:47.876279Z 0 [Warning] [MY-010068] [Server] CA certificate ca.pem is self signed.
2023-01-30T14:22:47.876367Z 0 [System] [MY-013602] [Server] Channel mysql_main configured to support TLS. Encrypted connections are now supported for this channel.
2023-01-30T14:22:47.929408Z 0 [System] [MY-011323] [Server] X Plugin ready for connections. Socket: /var/run/mysqld/mysqlx.sock
2023-01-30T14:22:47.929457Z 0 [System] [MY-010931] [Server] /usr/sbin/mysqld: ready for connections. Version: '8.0.32'  socket: '/var/lib/mysql/mysql.sock'  port: 0  MySQL Community Server - GPL.
Warning: Unable to load '/usr/share/zoneinfo/iso3166.tab' as time zone. Skipping it.
Warning: Unable to load '/usr/share/zoneinfo/leapseconds' as time zone. Skipping it.
Warning: Unable to load '/usr/share/zoneinfo/tzdata.zi' as time zone. Skipping it.
Warning: Unable to load '/usr/share/zoneinfo/zone.tab' as time zone. Skipping it.
Warning: Unable to load '/usr/share/zoneinfo/zone1970.tab' as time zone. Skipping it.
[Entrypoint] GENERATED ROOT PASSWORD: OOMH2O6B+2@;Ko_x95F53&XON=K2.L;v                       ### this is random password 

Entrypoint] ignoring /docker-entrypoint-initdb.d/

2023-01-30T14:22:48.994168Z 11 [System] [MY-013172] [Server] Received SHUTDOWN from user root. Shutting down mysqld (Version: 8.0.32).
2023-01-30T14:22:50.586052Z 0 [System] [MY-010910] [Server] /usr/sbin/mysqld: Shutdown complete (mysqld 8.0.32)  MySQL Community Server - GPL.
[Entrypoint] Server shut down
[Entrypoint] Setting root user as expired. Password will need to be changed before database can be used.

[Entrypoint] MySQL init process done. Ready for start up.

[Entrypoint] Starting MySQL 8.0.32-1.2.11-server
2023-01-30T14:22:51.291152Z 0 [Warning] [MY-011068] [Server] The syntax '--skip-host-cache' is deprecated and will be removed in a future release. Please use SET GLOBAL host_cache_size=0 instead.
2023-01-30T14:22:51.292674Z 0 [System] [MY-010116] [Server] /usr/sbin/mysqld (mysqld 8.0.32) starting as process 1
2023-01-30T14:22:51.298884Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.
2023-01-30T14:22:51.426700Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.
2023-01-30T14:22:51.555423Z 0 [Warning] [MY-010068] [Server] CA certificate ca.pem is self signed.
2023-01-30T14:22:51.555446Z 0 [System] [MY-013602] [Server] Channel mysql_main configured to support TLS. Encrypted connections are now supported for this channel.
2023-01-30T14:22:51.574382Z 0 [System] [MY-011323] [Server] X Plugin ready for connections. Bind-address: '::' port: 33060, socket: /var/run/mysqld/mysqlx.sock
2023-01-30T14:22:51.574399Z 0 [System] [MY-010931] [Server] /usr/sbin/mysqld: ready for connections. Version: '8.0.32'  socket: '/var/lib/mysql/mysql.sock'  port: 3306  MySQL Community Server - GPL.
----------
root@anji:~# docker logs mysql 2>&1 | grep -i  GENERATED 
[Entrypoint]   A random onetime password will be generated.
[Entrypoint] GENERATED ROOT PASSWORD: OOMH2O6B+2@;Ko_x95F53&XON=K2.L;v
root@anji:~# 

----
root@anji:~# docker exec -it mysql  mysql -uroot -p
Enter password: 
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 40
Server version: 8.0.32

Copyright (c) 2000, 2023, Oracle and/or its affiliates.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> 

-----------
Because the MYSQL_ONETIME_PASSWORD option is true by default,

 ALTER USER 'root'@'localhost' IDENTIFIED BY 'password';
  ALTER USER 'root'@'localhost' IDENTIFIED BY 'asd123';

mysql>  ALTER USER 'root'@'localhost' IDENTIFIED BY 'asd123';
Query OK, 0 rows affected (0.01 sec)

mysql> 
===============================================================================
                 second method 
https://dbschema.com/2020/03/31/how-to-run-mysql-in-docker/

docker run --name=mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 -d mysql/mysql-server:8.0

    run - will run a new command in a new Docker container
    â€“name - will give a name to the new container created
    -p - will make the internal docker port visible outside docker
    -e - will change the root password. Here you can insert whatever password you want
    mysql/mysql-server:8.0 - will specify what image to run in the newly created container

root@anji:~# docker exec -it mysql mysql -uroot -p
Enter password: 
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 14
Server version: 8.0.32 MySQL Community Server - GPL

Copyright (c) 2000, 2023, Oracle and/or its affiliates.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> 

https://www.tecmint.com/gliding-through-database-mysql-in-a-nutshell-part-i/

mysql> create database anji;
Query OK, 1 row affected (0.01 sec)

mysql> show databases;
+--------------------+
| Database           |
+--------------------+
| anji               |
| information_schema |
| mysql              |
| performance_schema |
| sys                |
+--------------------+
5 rows in set (0.01 sec)


mysql> 
mysql> use anji;
Database changed
mysql> CREATE TABLE anji (
    -> id Int(3),
    -> first_name  Varchar (15),
    -> email Varchar(20)
    -> );
Query OK, 0 rows affected, 1 warning (0.02 sec)

mysql> show tables;
+----------------+
| Tables_in_anji |
+----------------+
| anji           |
+----------------+
1 row in set (0.01 sec)

mysql> show columns from  anji;
+------------+-------------+------+-----+---------+-------+
| Field      | Type        | Null | Key | Default | Extra |
+------------+-------------+------+-----+---------+-------+
| id         | int         | YES  |     | NULL    |       |
| first_name | varchar(15) | YES  |     | NULL    |       |
| email      | varchar(20) | YES  |     | NULL    |       |
+------------+-------------+------+-----+---------+-------+
3 rows in set (0.01 sec)



    Int is Integer
    Varchar is char having variable length as defined. The value after Type is the length of field up-to which it can store data.

mysql> ALTER TABLE anji ADD  last_name varchar (20)
    -> AFTER first_name ;
Query OK, 0 rows affected (0.04 sec)
Records: 0  Duplicates: 0  Warnings: 0

mysql> show columns from anji;
+------------+-------------+------+-----+---------+-------+
| Field      | Type        | Null | Key | Default | Extra |
+------------+-------------+------+-----+---------+-------+
| id         | int         | YES  |     | NULL    |       |
| first_name | varchar(15) | YES  |     | NULL    |       |
| last_name  | varchar(20) | YES  |     | NULL    |       |
| email      | varchar(20) | YES  |     | NULL    |       |
+------------+-------------+------+-----+---------+-------+
4 rows in set (0.01 sec)

mysql> ALTER TABLE anji ADD country varchar (15)
    -> AFTER email;
Query OK, 0 rows affected (0.02 sec)
Records: 0  Duplicates: 0  Warnings: 0

mysql> show columns from anji;
+------------+-------------+------+-----+---------+-------+
| Field      | Type        | Null | Key | Default | Extra |
+------------+-------------+------+-----+---------+-------+
| id         | int         | YES  |     | NULL    |       |
| first_name | varchar(15) | YES  |     | NULL    |       |
| last_name  | varchar(20) | YES  |     | NULL    |       |
| email      | varchar(20) | YES  |     | NULL    |       |
| country    | varchar(15) | YES  |     | NULL    |       |
+------------+-------------+------+-----+---------+-------+
5 rows in set (0.00 sec)

mysql> INSERT INTO anji VALUES ('1' , 'reddy' , 'venkata' , 'sdx@gmail.com',
    -> 'india' );
Query OK, 1 row affected (0.01 sec)

mysql> INSERT INTO anji VALUES ('2', 'ashok', 'kamatam' , 'as@gmail.com' , 'usa'),
    ->     ('3', 'hanu', 'velpula' , 'v@gmail.com', 'japan');
Query OK, 2 rows affected (0.00 sec)
Records: 2  Duplicates: 0  Warnings: 0

mysql> select * from anji;
+------+------------+-----------+---------------+---------+
| id   | first_name | last_name | email         | country |
+------+------------+-----------+---------------+---------+
|    1 | reddy      | venkata   | sdx@gmail.com | india   |
|    2 | ashok      | kamatam   | as@gmail.com  | usa     |
|    3 | hanu       | velpula   | v@gmail.com   | japan   |
+------+------------+-----------+---------------+---------+
3 rows in set (0.00 sec)

mysql> DELETE FROM anji WHERE id = 3;
Query OK, 1 row affected (0.01 sec)

mysql> select * from anji;
+------+------------+-----------+---------------+---------+
| id   | first_name | last_name | email         | country |
+------+------------+-----------+---------------+---------+
|    1 | reddy      | venkata   | sdx@gmail.com | india   |
|    2 | ashok      | kamatam   | as@gmail.com  | usa     |
+------+------------+-----------+---------------+---------+
2 rows in set (0.00 sec)


mysql> UPDATE anji SET id = 200 WHERE first_name = 'ashok';
Query OK, 1 row affected (0.00 sec)
Rows matched: 1  Changed: 1  Warnings: 0

mysql> select * from anji;
+------+------------+-----------+---------------+---------+
| id   | first_name | last_name | email         | country |
+------+------------+-----------+---------------+---------+
|    1 | reddy      | venkata   | sdx@gmail.com | india   |
|  200 | ashok      | kamatam   | as@gmail.com  | usa     |
+------+------------+-----------+---------------+---------+
2 rows in set (0.00 sec)

ysql> ALTER TABLE anji drop country; 
Query OK, 0 rows affected (0.02 sec)
Records: 0  Duplicates: 0  Warnings: 0

mysql> select  * from anji;
+------+------------+-----------+---------------+
| id   | first_name | last_name | email         |
+------+------------+-----------+---------------+
|    1 | reddy      | venkata   | sdx@gmail.com |
|  200 | ashok      | kamatam   | as@gmail.com  |
+------+------------+-----------+---------------+
2 rows in set (0.00 sec)

mysql> RENAME TABLE  anji  TO reddy;
Query OK, 0 rows affected (0.02 sec)

mysql> show tables;
+----------------+
| Tables_in_anji |
+----------------+
| reddy          |
+----------------+
1 row in set (0.01 sec)

mysql> mysqldump -u root -p 123456 > new.sql


mysql> drop database anji;
Query OK, 1 row affected (0.03 sec)


mysql> show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| mysql              |
| performance_schema |
| sys                |
+--------------------+
4 rows in set (0.00 sec)

mysql> 

mysql> show databases; 

+--------------------+ 
| Database           | 
+--------------------+ 
| information_schema | 
| my_database        | 
| mysql              | 
| performance_schema | 
| phpmyadmin         | 
| sisso              | 
| test               | 
+--------------------+

7 rows in set (0.00 sec) 

# mysql -u root -p tecmint < tecmint.sql
Enter password:
ERROR 1049 (42000): Unknown database 'tecmint'

mysql> create database tecmint; 
Query OK, 1 row affected (0.00 sec) 

# mysql -u root -p tecmint < tecmint.sql 
Enter password:

mysql> show databases; 

+--------------------+ 
| Database           | 
+--------------------+ 
| information_schema | 
| mysql              | 
| performance_schema | 
| tecmint            | 
| test               | 
+--------------------+ 
8 rows in set (0.00 sec)


mysql> show tables from tecmint;

+-------------------+ 
| Tables_in_tecmint | 
+-------------------+ 
| tecmint_table     | 
+-------------------+ 
1 row in set (0.00 sec)

mysql> select * from tecmint_table; 

+------+------------+-----------+-------------------+ 
| id   | first_name | last_name | email             | 
+------+------------+-----------+-------------------+ 
|    1 | Ravi       | Saive     | raivsaive@xyz.com | 
|    2 | Narad      | Shrestha  | narad@xyz.com     | 
|    6 | tecmint    | [dot]com  | tecmint@gmail.com | 
+------+------------+-----------+-------------------+

3 rows in set (0.00 sec)


============================================================================="
anji@anji:~/docker$ docker run -it mysql1  /bin/bash
bash-4.4# mysql -u root -p 
Enter password: 
ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/var/run/mysqld/mysqld.sock' (2)
bash-4.4# 

https://medium.com/@alef.duarte/cant-connect-to-local-mysql-server-through-socket-var-run-mysqld-mysqld-sock-155d580f3a06    = onpermise is ok 

https://stackoverflow.com/questions/25920029/setting-up-mysql-and-importing-dump-within-dockerfile


docker compose run [OPTIONS] SERVICE [COMMAND] [ARGS...]
docker compose run web bash
https://docs.docker.com/engine/reference/commandline/compose_run/
docker compose run --service-ports web python manage.py shell
docker compose run db psql -h db -U docker

#docker-compose.yml

data:
  build: docker/data/.
mysql:
  image: mysql
  ports:
    - "3307:3306"
  environment:
    MYSQL_ROOT_PASSWORD: 1234
  volumes:
    - ./docker/data:/docker-entrypoint-initdb.d
  volumes_from:
    - data

----------
Solution 1: use a one-line RUN

RUN /bin/bash -c "/usr/bin/mysqld_safe --skip-grant-tables &" && \
  sleep 5 && \
  mysql -u root -e "CREATE DATABASE mydb" && \
  mysql -u root mydb < /tmp/dump.sql

Solution 2: use a script
Create an executable script init_db.sh:

#!/bin/bash
/usr/bin/mysqld_safe --skip-grant-tables &
sleep 5
mysql -u root -e "CREATE DATABASE mydb"
mysql -u root mydb < /tmp/dump.sql


Add these lines to your Dockerfile:
ADD init_db.sh /tmp/init_db.sh
RUN /tmp/init_db.sh

mysql:
 image: mysql:5.6
 environment:
   MYSQL_ROOT_PASSWORD: pass
 ports:
   - 3306:3306
 volumes:
   - ./db-dump:/docker-entrypoint-initdb.d

----------
mysql:
 image: mysql:5.6
 environment:
   MYSQL_ROOT_PASSWORD: pass
 ports:
   - 3306:3306
 volumes:
   - ./db-dump:/docker-entrypoint-initdb.d

When I run docker-compose up for the first time, the dump is restored in the db.

Here is a working version using v3 of docker-compose.yml. The key is the volumes directive:

mysql:
  image: mysql:5.6
  ports:
    - "3306:3306"
  environment:
    MYSQL_ROOT_PASSWORD: root
    MYSQL_USER: theusername
    MYSQL_PASSWORD: thepw
    MYSQL_DATABASE: mydb
  volumes:
    - ./data:/docker-entrypoint-initdb.d



I used docker-entrypoint-initdb.d approach (Thanks to @Kuhess) But in my case I want to create my DB based on some parameters I defined in .env file so I did these

1) First I define .env file something like this in my docker root project directory

MYSQL_DATABASE=my_db_name
MYSQL_USER=user_test
MYSQL_PASSWORD=test
MYSQL_ROOT_PASSWORD=test
MYSQL_PORT=3306

2) Then I define my docker-compose.yml file. So I used the args directive to define my environment variables and I set them from .env file

version: '2'
services:
### MySQL Container
    mysql:
        build:
            context: ./mysql
            args:
                - MYSQL_DATABASE=${MYSQL_DATABASE}
                - MYSQL_USER=${MYSQL_USER}
                - MYSQL_PASSWORD=${MYSQL_PASSWORD}
                - MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD}
        ports:
            - "${MYSQL_PORT}:3306"

3) Then I define a mysql folder that includes a Dockerfile. So the Dockerfile is this

FROM mysql:5.7
RUN chown -R mysql:root /var/lib/mysql/

ARG MYSQL_DATABASE
ARG MYSQL_USER
ARG MYSQL_PASSWORD
ARG MYSQL_ROOT_PASSWORD

ENV MYSQL_DATABASE=$MYSQL_DATABASE
ENV MYSQL_USER=$MYSQL_USER
ENV MYSQL_PASSWORD=$MYSQL_PASSWORD
ENV MYSQL_ROOT_PASSWORD=$MYSQL_ROOT_PASSWORD

ADD data.sql /etc/mysql/data.sql
RUN sed -i 's/MYSQL_DATABASE/'$MYSQL_DATABASE'/g' /etc/mysql/data.sql
RUN cp /etc/mysql/data.sql /docker-entrypoint-initdb.d

EXPOSE 3306

4) Now I use mysqldump to dump my db and put the data.sql inside mysql folder

mysqldump -h <server name> -u<user> -p <db name> > data.sql

The file is just a normal sql dump file but I add 2 lines at the beginning so the file would look like this

--
-- Create a database using `MYSQL_DATABASE` placeholder
--
CREATE DATABASE IF NOT EXISTS `MYSQL_DATABASE`;
USE `MYSQL_DATABASE`;

-- Rest of queries
DROP TABLE IF EXISTS `x`;
CREATE TABLE `x` (..)
LOCK TABLES `x` WRITE;
INSERT INTO `x` VALUES ...;
...
...
...

So what happening is that I used "RUN sed -i 's/MYSQL_DATABASE/'$MYSQL_DATABASE'/g' /etc/mysql/data.sql" command to replace the MYSQL_DATABASE placeholder with the name of my DB that I have set it in .env file.

|- docker-compose.yml
|- .env
|- mysql
     |- Dockerfile
     |- data.sql


anji@anji:~/docker/compo$ docker-compose up -d 
ERROR: 
        Can't find a suitable configuration file in this directory or any
        parent. Are you in the right directory?

        Supported filenames: docker-compose.yml, docker-compose.yaml
        
anji@anji:~/docker/compo$ mv a.yaml docker-compose.yaml
anji@anji:~/docker/compo$ docker-compose up -d 

anji@anji:~/docker$ tree -a
.
â”œâ”€â”€ data.sql
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ Dockerfile
â””â”€â”€ .env

0 directories, 4 files

====================================================================================================================
                3 rd method 
https://www.percona.com/blog/2019/11/19/installing-mysql-with-docker/
anji@anji:~/docker$ docker run --name mysql-latest  \
> -p 3306:3306 -p 33060:33060  \
> -e MYSQL_ROOT_HOST='%' -e MYSQL_ROOT_PASSWORD='strongpassword'   \
> -d mysql/mysql-server:latest

anji@anji:~/docker$ docker images 
REPOSITORY           TAG       IMAGE ID       CREATED       SIZE
mysql/mysql-server   latest    1d9c2219ff69   13 days ago   496MB 

anji@anji:~/docker$ docker ps 
CONTAINER ID   IMAGE                       COMMAND                  CREATED          STATUS                    PORTS                                                                                                 NAMES
f7d80c678608   mysql/mysql-server:latest   "/entrypoint.sh mysqâ€¦"   58 seconds ago   Up 57 seconds (healthy)   0.0.0.0:3306->3306/tcp, :::3306->3306/tcp, 0.0.0.0:33060->33060/tcp, :::33060->33060/tcp, 33061/tcp   mysql-latest
anji@anji:~/docker$ 

anji@anji:~/docker$  docker logs mysql-latest
[Entrypoint] MySQL Docker Image 8.0.32-1.2.11-server
[Entrypoint] Initializing database
ssword ! Please consider switching off the --initialize-insecure option.
[Entrypoint] Database initialized
2023-01-31T10:32:26.287068Z 0 [Warning] [MY-011068] [Server] The syntax '--skip-host-cache' is deprecated and will be removed in a future release. Please use SET GLOBAL host_cache_size=0 instead.
rsion: '8.0.32'  socket: '/var/lib/mysql/mysql.sock'  port: 0  MySQL Community Server - GPL.
Warning: Unable to load '/usr/share/zoneinfo/iso3166.tab' as time zone. Skipping it.
[Entrypoint] ignoring /docker-entrypoint-initdb.d/*

-----
anji@anji:~/docker$ docker exec -it mysql-latest mysql -uroot -pstrongpassword
mysql: [Warning] Using a password on the command line interface can be insecure.
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 18
Server version: 8.0.32 MySQL Community Server - GPL

Copyright (c) 2000, 2023, Oracle and/or its affiliates.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

=============================================================================================
https://geshan.com.np/blog/2022/02/mysql-docker-compose/
--rm 
The --rm flag is there to tell the Docker Daemon to clean up the container and remove the file system after the container exits.
 This helps you save disk space after running short-lived containers like this one, that we only started to print "Hello, World!

anji@anji:~/docker$ mkdir /tmp/mysql-data
mkdir: cannot create directory â€˜/tmp/mysql-dataâ€™: File exists

anji@anji:~/docker$ docker run --name basic-mysql --rm -v /tmp/mysql-data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=ANSKk08aPEDbFjDO -e MYSQL_DATABASE=testing -p 3306:3306 -it mysql:8.0

root@anji:/home/anji/docker# docker exec -it basic-mysql /bin/bash
bash-4.4# 
bash-4.4# mysql -u root  -p 
Enter password: 
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 8
Server version: 8.0.32 MySQL Community Server - GPL
Copyright (c) 2000, 2023, Oracle and/or its affiliates.
Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owner.
Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
mysql> show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| mysql              |
| performance_schema |
| sys                |
| testing            |
+--------------------+
5 rows in set (0.01 sec)

====================================================================================
https://ostechnix.com/setup-mysql-with-docker-in-linux/

root@anji:~# docker run --name mysql -p 3306:3306 -v mysql_volume:/var/lib/mysql/ -d -e "MYSQL_ROOT_PASSWORD=temp123" mysql
Unable to find image 'mysql:latest' locally
latest: Pulling from library/mysql

root@anji:~# netstat -plant | grep -i 3306
tcp        0      0 0.0.0.0:3306            0.0.0.0:*               LISTEN      97432/docker-proxy  
tcp6       0      0 :::3306                 :::*                    LISTEN      97438/docker-proxy  
root@anji:~# 
v â†’ Attach a volume to the container. The default behavior of docker is it will not persist the data once the container is removed,so you will lose all your data.

To create persistent storage, I have created volume named "mysql_volume". MySQL stores the data in /var/lib/mysql/ inside the container and here it is mapped to localhost directory /var/lib/docker/volumes/mysql_volume1/_data, so your data will be persistent.

If you wish to know more about docker volumes take a look at our detailed article on same.

-d â†’ Will start and run the container in detached mode. If you omit the -d flag, then you will see the container startup logs in the terminal and you have to open a new terminal session to connect to the container.

-e â†’ Environmental variables. You have to set up mysql root user password using any one of the below parameters.

    MYSQL_ROOT_PASSWORD â†’ Setup your own password using this environment variable.
    MYSQL_ALLOW_EMPTY_PASSWORD â†’ Blank or Null password will be set. You have to set MYSQL_ALLOW_EMPTY_PASSWORD=1.
    MYSQL_RANDOM_ROOT_PASSWORD â†’ random password will be generated when the container is started. You have to set MYSQL_RANDOM_ROOT_PASSWORD=1 to generate the random password.


root@anji:~# docker exec -it mysql bash
bash-4.4# mysql -u root -p
Enter password:   temp123
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 10
Server version: 8.0.32 MySQL Community Server - GPL

Copyright (c) 2000, 2023, Oracle and/or its affiliates.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| mysql              |
| performance_schema |
| sys                |
+--------------------+
4 rows in set (0.00 sec)

anji@anji:~/docker$ docker logs mysql 

mysql> ALTER USER 'root'@'localhost' IDENTIFIED BY 'anji123';
Query OK, 0 rows affected (0.00 sec)

mysql> CREATE DATABASE IF NOT EXISTS football;
Query OK, 1 row affected (0.00 sec)

mysql> 
mysql> USE football;
Database changed
mysql> 
mysql> CREATE TABLE IF NOT EXISTS players (
    ->     player_name     VARCHAR(16)     NOT NULL,
    ->     player_age      INT             NOT NULL,
    ->     player_club     VARCHAR(16)     NOT NULL,
    ->     player_country  VARCHAR(16)     NOT NULL
    -> );
Query OK, 0 rows affected (0.02 sec)

mysql> 
mysql> INSERT INTO players VALUES ("Messi",34,"PSG","Argentina");
Query OK, 1 row affected (0.01 sec)

mysql> INSERT INTO players VALUES ("Ronaldo",36,"MANU","Portugal");
Query OK, 1 row affected (0.00 sec)

mysql> INSERT INTO players VALUES ("Neymar",29,"PSG","Brazil");
Query OK, 1 row affected (0.00 sec)

mysql> INSERT INTO players VALUES ("Kane",28,"SPURS","England");
Query OK, 1 row affected (0.01 sec)

mysql> INSERT INTO players VALUES ("E Hazard",30,"MADRID","Belgium");
Query OK, 1 row affected (0.00 sec)

OR  OR  OR                                   -------------------\\\\\\///
anji@anji:$ docker cp data.sql mysql:/tmp

$ docker exec -it mysql bash
bash-4.4# ls -l /tmp/
total 548
-rw-r--r-- 1 1000 1000 558791 Dec 31 23:05 data.sql
bash-4.4# 

mysql> source /tmp/data.sql
Query OK, 0 rows affected (0.00 sec)

Query OK, 0 rows affected (0.00 sec)

or  or   or  
bash-4.4# mysql -u root -p  <  /tmp/data.sql 
Enter password: 
bash-4.4# 

bash-4.4# mysql -u root -p 
Enter password: 
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 13
Server version: 8.0.32 MySQL Community Server - GPL

Copyright (c) 2000, 2023, Oracle and/or its affiliates.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> use football;
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Database changed
mysql> 

mysql> show databases;
+--------------------+
| Database           |
+--------------------+
| football           |
| information_schema |
| mysql              |
| performance_schema |
| sys                |
| world_x            |
+--------------------+
6 rows in set (0.00 sec)

mysql> 

mysql> use football;
Database changed
mysql> 

mysql> show tables;
+--------------------+
| Tables_in_football |
+--------------------+
| players            |
+--------------------+
1 row in set (0.01 sec)

mysql> 

mysql> select * from players;
+-------------+------------+-------------+----------------+
| player_name | player_age | player_club | player_country |
+-------------+------------+-------------+----------------+
| Messi       |         34 | PSG         | Argentina      |
| Ronaldo     |         36 | MANU        | Portugal       |
| Neymar      |         29 | PSG         | Brazil         |
| Kane        |         28 | SPURS       | England        |
| E Hazard    |         30 | MADRID      | Belgium        |
+-------------+------------+-------------+----------------+
5 rows in set (0.00 sec)

secoond method    === = =  ------====== second method`"
`
root@anji:~# docker exec -i mysql  mysql -u root -p < /home/anji/Downloads/world_x-db/data.sql 
Enter password: ERROR 1045 (28000): Access denied for user 'root'@'localhost' (using password: YES)

     https://dba.stackexchange.com/questions/315142/error-1045-28000-access-denied-for-user-rootlocalhost
     https://askubuntu.com/questions/401449/error-104528000-access-denied-for-user-rootlocalhost-using-password-no
       https://www.javamadesoeasy.com/2015/11/how-to-resolve-error-1045-28000-access.html
      https://www.stechies.com/error-1045-28000-access-denied-user-root-localhost/
       https://stackoverflow.com/questions/21944936/error-1045-28000-access-denied-for-user-rootlocalhost-using-password-y
-----==

Setup MySQL Container Using Docker-Compose  

version: '3.8'
services:
  database:
    image: mysql:latest
    container_name: mysql
    environment:
      MYSQL_ROOT_PASSWORD: temp1234
    ports:
      - "3306:3306"
    volumes:
      - mysql_volume:/var/lib/mysql
volumes:
  mysql_compose_volume:

                
root@anji:~# docker-compose up
ERROR: Version in "./docker-compose.yaml" is unsupported. You might be seeing this error because you're using the wrong Compose file version. Either specify a supported version (e.g "2.2" or "3.3") and place your service definitions under the `services` key, or omit the `version` key and place your service definitions at the root of the file to use version 1.
For more on the Compose file format versions, see https://docs.docker.com/compose/compose-file/
root@anji:~#  sudo apt-get remove docker-compose

https://docs.docker.com/compose/install/other/

curl -SL https://github.com/docker/compose/releases/download/v2.15.1/docker-compose-linux-x86_64 -o /usr/local/bin/docker-compose

sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose

root@anji:~# chmod 777 /usr/bin/docker-compose 

root@anji:~# docker-compose -v
Docker Compose version v2.15.1

root@anji:~# docker-compose up
service "database" refers to undefined volume mysql_volume: invalid compose project
root@anji:~# 

   volumes:
            - "./data/db/mysql:/var/lib/mysql"
                volumes:
        - ./symfony/docker/mysql:/var/mysql/data:ro
        - db-data:/var/lib/mysql:rw
$ docker-compose ps

Name              Command             State                          Ports                       
-------------------------------------------------------------------------------------------------
mysql   docker-entrypoint.sh mysqld   Up      0.0.0.0:3306->3306/tcp,:::3306->3306/tcp, 33060/tcp

-=============================================================================
https://earthly.dev/blog/docker-mysql/

docker run --name mysql -d \
    -p 3306:3306 \
    -e MYSQL_ROOT_PASSWORD=change-me \
    --restart unless-stopped \
    mysql:8


anji@anji:~$ docker exec -it mysql  mysql  -p 
Enter password:  change-me

docker stop mysql
docker rm mysql
---

anji@anji:~$ docker run --name mysql -d \
>     -p 3306:3306 \
>     -e MYSQL_ROOT_PASSWORD=change-me \
>     -v mysql:/var/lib/mysql \
>     mysql:8
695b2e47615a85924f520d3d0a22d70b1c5f066209a4802edc8460aa5a88c1ec
anji@anji:~$ 

anji@anji:~$ docker stop mysql
mysql
anji@anji:~$ docker rm mysql
mysql
anji@anji:~$ 
----

anji@anji:~$ docker network create anji
62f813904d43a711e6224cf306d5a7ceea9a0caae1c79b3df363a171565dde0e
anji@anji:~$ 
anji@anji:~$ docker network ls 
NETWORK ID     NAME      DRIVER    SCOPE
62f813904d43   anji      bridge    local
3de69a1fdebc   bridge    bridge    local
94661259418a   host      host      local
e7293d9ceb94   none      null      local
anji@anji:~$ 

---
docker run --name mysql -d \
    -e MYSQL_ROOT_PASSWORD=change-me \
    -v mysql:/var/lib/mysql \
    --network anji \
    mysql:8



docker run --name api-server -d \
    -p 80:80 \
    --network anji \
    example-api-server:latest
-------------------
root@anji:~# mkdir secrets 
root@anji:~# echo "P@$$w0rd" > secrets/mysql-root-password
root@anji:~# ls -a secrets/
.  ..  mysql-root-password
root@anji:~# ll secrets/
total 12
drwxr-xr-x  2 root root 4096 Jan 31 18:51 ./
drwx------ 16 root root 4096 Jan 31 18:50 ../
-rw-r--r--  1 root root   12 Jan 31 18:51 mysql-root-password
root@anji:~# cat secrets/mysql-root-password 
P@93018w0rd
root@anji:~# 

docker run --name mysql -d \
    -p 3306:3306 \
    -e MYSQL_ROOT_PASSWORD_FILE=/run/secrets/mysql-root-password \
    -v ./secrets:/run/secrets \
    --restart unless-stopped \
    mysql:8

Digest: sha256:19b05df6eb4b7ed6f274c0552f053ff0c00842a40dcf05941225c429a716683d
Status: Downloaded newer image for mysql:8
docker: Error response from daemon: create ./secrets: "./secrets" includes invalid characters for a local volume name, only "[a-zA-Z0-9][a-zA-Z0-9_.-]" are allowed. If you intended to pass a host directory, use absolute path.
See 'docker run --help'.
root@anji:~# 

============================================================================================="
https://www.appsdeveloperblog.com/how-to-start-mysql-in-docker-container/

docker run -d -p 3306:3306 --name mysql -e MYSQL_ROOT_PASSWORD=anji123  -e MYSQL_DATABASE=python  -e MYSQL_USER=anji -e MYSQL_PASSWORD=anji123  mysql/mysql-server:latest

root@anji:~# docker exec -it mysql bash
bash-4.4# ls -a 
.  ..  .dockerenv  bin	boot  dev  docker-entrypoint-initdb.d  entrypoint.sh  etc  healthcheck.sh  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var
bash-4.4# 

--
anji@anji:~$ watch docker logs mysql 
Every 2.0s: docker logs mysql                                                                                                                                                               anji: Tue Jan 31 19:45:54 2023
[Entrypoint] MySQL Docker Image 8.0.32-1.2.11-server
[Entrypoint] Initializing database
2023-01-31T14:11:17.656170Z 0 [Warning] [MY-011068] [Server] The syntax '--skip-host-cache' is deprecated and will be removed in a future release. Please use SET GLOBAL host_cache_size=0 instead.
[Entrypoint] Database initialized
2023-01-31T14:11:22.775834Z 0 [Warning] [MY-011068] [Server] The syntax '--skip-host-cache' is deprecated and will be removed in a future release. Please use SET GLOBAL host_cache_size=0 instead.
2023-01-31T14:11:23.111628Z 0 [System] [MY-010931] [Server] /usr/sbin/mysqld: ready for connections. Version: '8.0.32'  socket: '/var/lib/mysql/mysql.sock'  port: 0  MySQL Community Server - GPL.
Warning: Unable to load '/usr/share/zoneinfo/iso3166.tab' as time zone. Skipping it.
[Entrypoint] ignoring /docker-entrypoint-initdb.d/
2023-01-31T14:11:24.208412Z 14 [System] [MY-013172] [Server] Received SHUTDOWN from user root. Shutting down mysqld (Version: 8.0.32).
2023-01-31T14:11:25.836441Z 0 [System] [MY-010910] [Server] /usr/sbin/mysqld: Shutdown complete (mysqld 8.0.32)  MySQL Community Server - GPL.
[Entrypoint] Server shut down
[Entrypoint] MySQL init process done. Ready for start up.
[Entrypoint] Starting MySQL 8.0.32-1.2.11-server
2023-01-31T14:11:26.455847Z 0 [Warning] [MY-011068] [Server] The syntax '--skip-host-cache' is deprecated and will be removed in a future release. Please use SET GLOBAL host_cache_size=0 instead.
================
mysql> SELECT user FROM mysql.user;
+------------------+
| user             |
+------------------+
| anji             |
| healthchecker    |
| mysql.infoschema |
| mysql.session    |
| mysql.sys        |
| root             |
+------------------+
6 rows in set (0.00 sec)

mysql> show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| mysql              |
| performance_schema |
| python             |
| sys                |
+--------------------+
5 rows in set (0.00 sec)

mysql> create user 'new_user'@'localhost' identified by 'welcome';
Query OK, 0 rows affected (0.01 sec)

mysql> grant all privileges on *.* to 'new_user'@'localhost';
Query OK, 0 rows affected, 1 warning (0.01 sec)

mysql> flush privileges;
Query OK, 0 rows affected (0.00 sec)

mysql> select user from mysql.user;
+------------------+
| user             |
+------------------+
| anji             |
| healthchecker    |
| mysql.infoschema |
| mysql.session    |
| mysql.sys        |
| new_user         |
| root             |
+------------------+
7 rows in set (0.01 sec)
======================================================================================="
https://dev.to/musolemasu/deploy-a-mysql-database-server-in-kubernetes-static-dpc     success 

---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mysql-pv
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-pv-claim
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql
spec:
  selector:
    matchLabels:
      app: mysql
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
        - image: mysql:8.0
          name: mysql
          env:
            - name: MYSQL_ROOT_PASSWORD
              value: password
          ports:
            - containerPort: 3306
              name: mysql
          volumeMounts:
            - name: mysql-persistent-storage
              mountPath: /var/lib/mysql
      volumes:
        - name: mysql-persistent-storage
          persistentVolumeClaim:
            claimName: mysql-pv-claim
---
apiVersion: v1
kind: Service
metadata:
  name: mysql
spec:
  ports:
    - port: 3306
  selector:
    app: mysql
  clusterIP: None

anji@anji:~/deployment$ kubectl exec mysql-55dd4c6c84-4dzf5  -it -- /bin/bash
bash-4.4# ls 
bin   dev			  entrypoint.sh  home  lib64  mnt  proc  run   srv  tmp  var
boot  docker-entrypoint-initdb.d  etc		 lib   media  opt  root  sbin  sys  usr
bash-4.4# 

anji@anji:~$ kubectl cp  /home/anji/Downloads/world_x-db/data.sql  mysql-55dd4c6c84-4dzf5:/var/lib/mysql

bash-4.4# pwd
/var/lib/mysql
bash-4.4# ls -al  | grep -i data.sql 
-rw-r--r-- 1  1000  1000   558791 Feb  1 09:38 data.sql
bash-4.4# 

mysql> source /var/lib/mysql/data.sql 

mysql> show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| mysql              |
| performance_schema |
| sys                |
| world_x            |
+--------------------+
5 rows in set (0.00 sec)


mysql> show tables;
+-------------------+
| Tables_in_world_x |
+-------------------+
| city              |
| country           |
| countryinfo       |
| countrylanguage   |
+-------------------+
4 rows in set (0.01 sec)

mysql> use world_x;
Database changed
mysql> show tables;
+-------------------+
| Tables_in_world_x |
+-------------------+
| city              |
| country           |
| countryinfo       |
| countrylanguage   |
+-------------------+
4 rows in set (0.00 sec)

mysql> select * from city;

mysql> select * from city;
+------+-----------------------------------+-------------+----------------------+--------------------------+
| ID   | Name                              | CountryCode | District             | Info                     |
+------+-----------------------------------+-------------+----------------------+--------------------------+
|    1 | Kabul                             | AFG         | Kabol                | {"Population": 1780000}  |
|    2 | Qandahar                          | AFG         | Qandahar             | {"Population": 237500}   |
|    3 | Herat                             | AFG         | Herat                | {"Population": 186800}   |
|    4 | Mazar-e-Sharif                    | AFG         | Balkh                | {"Population": 127800}   |
|    5 | Amsterdam                         | NLD         | Noord-Holland        | {"Population": 731200}   |
|    6 | Rotterdam                         | NLD         | Zuid-Holland         | {"Population": 593321}   |
|    7 | Haag                              | NLD         | Zuid-Holland         | {"Population": 440900}   |
|    8 | Utrecht                           | NLD         | Utrecht              | {"Population": 234323}   |
|    9 | Eindhoven                         | NLD         | Noord-Brabant        | {"Population": 201843}   |
|   10 | Tilburg                           | NLD         | Noord-Brabant        | {"Population": 193238}   |
|   11 | Groningen                         | NLD         | Groningen            | {"Population": 172701}   |
|   12 | Breda                             | NLD         | Noord-Brabant        | {"Population": 160398}   |
|   13 | Apeldoorn                         | NLD         | Gelderland           | {"Population": 153491}   |
|   14 | Nijmegen                          | NLD         | Gelderland           | {"Population": 152463}   |
|   15 | Enschede                          | NLD         | Overijssel           | {"Population": 149544}   |
|   16 | Haarlem                           | NLD         | Noord-Holland        | {"Population": 14

mysql> select * from city;
+------+-----------------------------------+-------------+----------------------+--------------------------+
| ID   | Name                              | CountryCode | District             | Info                     |
+------+-----------------------------------+-------------+----------------------+--------------------------+
|    1 | Kabul                             | AFG         | Kabol                | {"Population": 1780000}  |
|    2 | Qandahar                          | AFG         | Qandahar             | {"Population": 237500}   |
|    3 | Herat                             | AFG         | Herat                | {"Population": 186800}   |
|    4 | Mazar-e-Sharif                    | AFG         | Balkh                | {"Population": 127800}   |
|    5 | Amsterdam                         | NLD         | Noord-Holland        | {"Population": 731200}   |
|    6 | Rotterdam                         | NLD         | Zuid-Holland         | {"Population": 593321}   |
|    7 | Haag                              | NLD         | Zuid-Holland         | {"Population": 440900}   |
|    8 | Utrecht                           | NLD         | Utrecht              | {"Population": 234323}   |
|    9 | Eindhoven                         | NLD         | Noord-Brabant        | {"Population": 201843}   |
|   10 | Tilburg                           | NLD         | Noord-Brabant        | {"Population": 193238}   |
|   11 | Groningen                         | NLD         | Groningen            | {"Population": 172701}   |
|   12 | Breda                             | NLD         | Noord-Brabant        | {"Population": 160398}   |
|   13 | Apeldoorn                         | NLD         | Gelderland           | {"Population": 153491}   |
|   14 | Nijmegen                          | NLD         | Gelderland           | {"Population": 152463}   |
|   15 | Enschede                          | NLD         | Overijssel           | {"Population": 149544}   |
|   16 | Haarlem                           | NLD         | Noord-Holland        | {"Population": 148772}   |
|   17 | Almere                            | NLD         | Flevoland            | {"Population": 142465}   |
|   18 | Arnhem                            | NLD         | Gelderland           | {"Population": 138020}   |
|   19 | Zaanstad                          | NLD         | Noord-Holland        | {"Population": 135621}   |

-----"
mysql> select * from countryinfo;
+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------+--------------------+
| doc                                                                                                                                                                                                                                                                                                                                                                                                           | _id                                                        | _json_schema       |
+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------+--------------------+
| {"GNP": 828, "_id": "00005de917d80000000000000000", "Code": "ABW", "Name": "Aruba", "IndepYear": null, "geography": {"Region": "Caribbean", "Continent": "North America", "SurfaceArea": 193}, "government": {"HeadOfState": "Beatrix", "GovernmentForm": "Nonmetropolitan Territory of The Netherlands"}, "demographics": {"Population": 103000, "LifeExpectancy": 78.4000015258789}}                        | 0x30303030356465393137643830303030303030303030303030303030 | {"type": "object"} |
| {"GNP": 5976, "_id": "00005de917d80000000000000001", "Code": "AFG", "Name": "Afghanistan", "IndepYear": 1919, "geography": {"Region": "Southern and Central Asia", "Continent": "Asia", "SurfaceArea": 652090}, "government": {"HeadOfState": "Mohammad Omar", "GovernmentForm": "Islamic Emirate"}, "demographics": {"Population": 22720000, "LifeExpectancy": 45.900001525878906}}                          | 0x30303030356465393137643830303030303030303030303030303031 | {"type": "object"} |
| {"GNP": 6648, "_id": "00005de917d80000000000000002", "Code": "AGO", "Name": "Angola", "IndepYear": 1975, "geography": {"Region": "Central Africa", "Continent": "Africa", "SurfaceArea": 1246700}, "government": {"HeadOfState": "Josï¿½ Eduardo dos Santos", "GovernmentForm": "Republic"}, "demographics": {"Population": 12878000, "LifeExpectancy": 38.29999923706055}}                                     | 0x30303030356465393137643830303030303030303030303030303032 | {"type": "object"} |
| {"GNP": 63.20000076293945, "_id": "00005de917d80000000000000003", "Code": "AIA", "Name": "Anguilla", "IndepYear": null, "geography": {"Region": "Caribbean", "Continent": "North America", "SurfaceArea": 96}, "government": {"HeadOfState": "Elizabeth II", "GovernmentForm": "Dependent Territory of the UK"}, "demographics": {"Population": 8000, "LifeExpectancy": 76.0999984741211}}                    | 0x30303030356465393137643830303030303030303030303030303033 | {"type": "object"} |
| {"GNP": 3205, "_id": "00005de917d80000000000000004", "Code": "ALB", "Name": "Albania", "IndepYear": 1912, "geography": {"Region": "Southern Europe", "Continent": "Europe", "SurfaceArea": 28748}, "government": {"HeadOfState": "Rexhep Mejdani", "GovernmentForm": "Republic"}, "demographics": {"Population": 3401200, "LifeExpectancy": 71.5999984741211}}                                                | 0x30303030356465393137643830303030303030303030303030303034 | {"type": "object"} |
| {"GNP": 1630, "_id": "00005de917d80000000000000005", "Code": "AND", "Name": "Andorra", "IndepYear": 1278, "geography": {"Region": "Southern Europe", "Continent": "Europe", "SurfaceArea": 468}, "government": {"HeadOfState": "", "GovernmentForm": "Parliamentary Coprincipality"}, "demographics": {"Population": 78000, "LifeExpectancy": 83.5}}                                                          | 0x30303030356465393137643830303030303030303030303030303035 | {"type": "object"} |
| {"GNP": 1941, "_id": "00005de917d80000000000000006", "Code": "ANT", "Name": "Netherlands Antilles", "IndepYear": null, "geography": {"Region": "Caribbean", "Continent": "North America", "SurfaceArea": 800}, "government": {"HeadOfState": "Beatrix", "GovernmentForm": "Nonmetropolitan Territory of The Netherlands"}, "demographics": {"Population": 217000, "LifeExpectancy": 74.69999694824219}}       | 0x30303030356465393137643830303030303030303030303030303036 | {"type": "object"} |
| {"GNP": 37966, "_id": "00005de917d80000000000000007", "Code": "ARE", "Name": "United Arab Emirates", "IndepYear": 1971, "geography": {"Region": "Middle East", "Continent": "Asia", "SurfaceArea": 83600}, "government": {"HeadOfState": "Zayid bin Sultan al-Nahayan", "GovernmentForm": "Emirate Federation"}, "demographics": {"Population": 2441000, "LifeExpectancy": 74.0999984741211}}                 | 0x30303030356465393137643830303030303030303030303030303037 | {"type": "object"} |
| {"GNP": 340238, "_id": "00005de917d80000000000000008", "Code": "ARG", "Name": "Argentina", "IndepYear": 1816, "geography": {"Region": "South America", "Continent": "South America", "SurfaceArea": 2780400}, "government": {"HeadOfState": "Fernando de la Rï¿½a", "GovernmentForm": "Federal Republic"}, "demographics": {"Population": 37032000, "LifeExpectancy": 75.0999984741211}}                        | 0x30303030356465393137643830303030303030303030303030303038 | {"type": "object"} |
| {"GNP": 1813, "_id": "00005de917d80000000000000009", "Code": "ARM", "Name": "Armenia", "IndepYear": 1991, "geography": {"Region": "Middle East", "Continent": "Asia", "SurfaceArea": 29800}, "government": {"HeadOfState": "Robert Kotï¿½arjan", "GovernmentForm": "Republic"}, "demographics": {"Population": 3520000, "LifeExpectancy": 66.4000015258789}}                                                    | 0x30303030356465393137643830303030303030303030303030303039 | {"type": "object"} |
| {"GNP": 334, "_id": "00005de917d8000000000000000a", "Code": "ASM", "Name": "American Samoa", "IndepYear": null, "geography": {"Region": "Polynesia", "Continent": "Oceania", "SurfaceArea": 199}, "government": {"HeadOfState": "George W. Bush", "GovernmentForm": "US Territory"}, "demographics": {"Population": 68000, "LifeExpectancy": 75.0999984741211}}                                               | 0x30303030356465393137643830303030303030303030303030303061 | {"type": "object"} |
| {"GNP": 0, "_id": "00005de917d8000000000000000b", "Code": "ATA", "Name": "Antarctica", "IndepYear": null, "geography": {"Region": "Antarctica", "Continent": "Antarctica", "SurfaceArea": 13120000}, "government": {"HeadOfState": "", "GovernmentForm": "Co-administrated"}, "demographics": {"Population": 0, "LifeExpectancy": null}}                                                                      | 0x30303030356465393137643830303030303030303030303030303062 | {"type": "object"} |
| {"GNP": 0, "_id": "00005de917d8000000000000000c", "Code": "ATF", "Name": "French Southern territories", "IndepYear": null, "geography": {"Region": "Antarctica", "Continent": "Antarctica", "SurfaceArea": 7780}, "government": {"HeadOfState": "Jacques Chirac", "GovernmentForm": "Nonmetropolitan Territory of France"}, "demographics": {"Population": 0, "LifeExpectancy": null}}                        | 0x30303030356465393137643830303030303030303030303030303063 | {"type": "object"} |
| {"GNP": 612, "_id": "00005de917d8000000000000000d", "Code": "ATG", "Name": "Antigua and Barbuda", "IndepYear": 1981, "geography": {"Region": "Caribbean", "Continent": "North America", "SurfaceArea": 442}, "government": {"HeadOfState": "Elizabeth II", "GovernmentForm": "Constitutional Monarchy"}, "demographics": {"Population": 68000, "LifeExpectancy": 70.5}}                                       | 0x30303030356465393137643830303030303030303030303030303064 | {"type": "object"} |
| {"GNP": 351182, "_id": "00005de917d8000000000000000e", "Code": "AUS", "Name": "Australia", "IndepYear": 1901, "geography": {"Region": "Australia and New Zealand", "Continent": "Oceania", "SurfaceArea": 7741220}, "government": {"HeadOfState": "Elizabeth II", "GovernmentForm": "Constitutional Monarchy, Federation"}, "demographics": {"Population": 18886000, "LifeExpectancy": 79.80000305175781}}    | 0x30303030356465393137643830303030303030303030303030303065 | {"type": "object"} |
| {"GNP": 211860, "_id": "00005de917d8000000000000000f", "Code": "AUT", "Name": "Austria", "IndepYear": 1918, "geography": {"Region": "Western Europe", "Continent": "Europe", "SurfaceArea": 83859}, "government": {"HeadOfState": "Thomas Klestil", "GovernmentForm": "Federal Republic"}, "demographics": {"Population": 8091800, "LifeExpectancy": 77.69999694824219}}                                      | 0x30303030356465393137643830303030303030303030303030303066 | {"type": "object"} |
| {"GNP": 4127, "_id": "00005de917d80000000000000010", "Code": "AZE", "Name": "Azerbaijan", "IndepYear": 1991, "geography": {"Region": "Middle East", "Continent": "Asia", "SurfaceArea": 86600}, "government": {"HeadOfState": "Heydï¿½r ï¿½liyev", "GovernmentForm": "Federal Republic"}, "demographics": {"Population": 7734000, "LifeExpectancy": 62.900001525878906}}                                          | 0x30303030356465393137643830303030303030303030303030303130 | {"type": "object"} |
| {"GNP": 903, "_id": "00005de917d80000000000000011", "Code": "BDI", "Name": "Burundi", "IndepYear": 1962, "geography": {"Region": "Eastern Africa", "Continent": "Africa", "SurfaceArea": 27834}, "government": {"HeadOfState": "Pierre Buyoya", "GovernmentForm": "Republic"}, "demographics": {"Population": 6695000, "LifeExpectancy": 46.20000076293945}}                                                  | 0x30303030356465393137643830303030303030303030303030303131 | {"type": "object"} |
| {"GNP": 249704, "_id": "00005de917d80000000000000012", "Code": "BEL", "Name": "Belgium", "IndepYear": 1830, "geography": {"Region": "Western Europe", "Continent": "Europe", "SurfaceArea": 30518}, "government": {"HeadOfState": "Albert II", "GovernmentForm": "Constitutional Monarchy, Federation"}, "demographics": {"Population": 10239000, "LifeExpectancy": 77.80000305175781}}                       | 0x30303030356465393137643830303030303030303030303030303132 | {"type": "object"} |
| {"GNP": 2357, "_id": "00005de917d80000000000000013", "Code": "BEN", "Name": "Benin", "IndepYear": 1960, "geography": {"Region": "Western Africa", "Continent": "Africa", "SurfaceArea": 112622}, "government": {"HeadOfState": "Mathieu Kï¿½rï¿½kou", "GovernmentForm": "Republic"}, "demographics": {"Population": 6097000, "LifeExpectancy": 50.20000076293945}}                                                | 0x30303030356465393137643830303030303030303030303030303133 | {"type": "object"} |
| {"GNP": 2425, "_id": "00005de917d80000000000000014", "Code": "BFA", "Name": "Burkina Faso", "IndepYear": 1960, "geography": {"Region": "Western Africa", "Continent": "Africa", "SurfaceArea": 274000}, "government": {"HeadOfState": "Blaise Compaorï¿½", "GovernmentForm": "Republic"}, "demographics": {"Population": 11937000, "LifeExpectancy": 46.70000076293945}}                                        | 0x30303030356465393137643830303030303030303030303030303134 | {"type": "object"} |
| {"GNP": 32852, "_id": "00005de917d80000000000000015", "Code": "BGD", "Name": "Bangladesh", "IndepYear": 1971, "geography": {"Region": "Southern and Central Asia", "Continent": "Asia", "SurfaceArea": 143998}, "government": {"HeadOfState": "Shahabuddin Ahmad", "GovernmentForm": "Republic"}, "demographics": {"Population": 129155000, "LifeExpectancy": 60.20000076293945}}                             | 0x30303030356465393137643830303030303030303030303030303135 | {"type": "object"} |
| {"GNP": 12178, "_id": "00005de917d80000000000000016", "Code": "BGR", "Name": "Bulgaria", "IndepYear": 1908, "geography": {"Region": "Eastern Europe", "Continent": "Europe", "SurfaceArea": 110994}, "government": {"HeadOfState": "Petar Stojanov", "GovernmentForm": "Republic"}, "demographics": {"Population": 8190900, "LifeExpectancy": 70.9000015258789}}                                              | 0x30303030356465393137643830303030303030303030303030303136 | {"type": "object"} |
| {"GNP": 6366, "_id": "00005de917d80000000000000017", "Code": "BHR", "Name": "Bahrain", "IndepYear": 1971, "geography": {"Region": "Middle East", "Continent": "Asia", "SurfaceArea": 694}, "government": {"HeadOfState": "Hamad ibn Isa al-Khalifa", "GovernmentForm": "Monarchy (Emirate)"}, "demographics": {"Population": 617000, "LifeExpectancy": 73}}                                                   | 0x30303030356465393137643830303030303030303030303030303137 | {"type": "object"} |
| {"GNP": 3527, "_id": "00005de917d80000000000000018", "Code": "BHS", "Name": "Bahamas", "IndepYear": 1973, "geography": {"Region": "Caribbean", "Continent": "North America", "SurfaceArea": 13878}, "government": {"HeadOfState": "Elizabeth II", "GovernmentForm": "Constitutional Monarchy"}, "demographics": {"Population": 307000, "LifeExpectancy": 71.0999984741211}}                                   | 0x30303030356465393137643830303030303030303030303030303138 | {"type": "object"} |
| {"GNP": 2841, "_id": "00005de917d80000000000000019", "Code": "BIH", "Name": "Bosnia and Herzegovina", "IndepYear": 1992, "geography": {"Region": "Southern Europe", "Continent": "Europe", "SurfaceArea": 51197}, "government": {"HeadOfState": "Ante Jelavic", "GovernmentForm": "Federal Republic"}, "demographics": {"Population": 3972000, "LifeExpectancy": 71.5}}                                       | 0x30303030356465393137643830303030303030303030303030303139 | {"type": "object"} |
| {"GNP": 13714, "_id": "00005de917d8000000000000001a", "Code": "BLR", "Name": "Belarus", "IndepYear": 1991, "geography": {"Region": "Eastern Europe", "Continent": "Europe", "SurfaceArea": 207600}, "government": {"HeadOfState": "Aljaksandr Lukaï¿½enka", "GovernmentForm": "Republic"}, "demographics": {"Population": 10236000, "LifeExpectancy": 68}}                                                      | 0x30303030356465393137643830303030303030303030303030303161 | {"type": "object"} |
| {"GNP": 630, "_id": "00005de917d8000000000000001b", "Code": "BLZ", "Name": "Belize", "IndepYear": 1981, "geography": {"Region": "Central America", "Continent": "North America", "SurfaceArea": 22696}, "government": {"HeadOfState": "Elizabeth II", "GovernmentForm": "Constitutional Monarchy"}, "demographics": {"Population": 241000, "LifeExpectancy": 70.9000015258789}}                               | 0x30303030356465393137643830303030303030303030303030303162 | {"type": "object"} |
| {"GNP": 2328, "_id": "00005de917d8000000000000001c", "Code": "BMU", "Name": "Bermuda", "IndepYear": null, "geography": {"Region": "Nor
------"

mysql> select * from countrylanguage;
+-------------+---------------------------+------------+------------+
| CountryCode | Language                  | IsOfficial | Percentage |
+-------------+---------------------------+------------+------------+
| AFG         | Dari                      | T          |       32.1 |
| AFG         | Pashto                    | T          |       52.4 |
| AFG         | Turkmenian                | F          |        1.9 |
| AFG         | Uzbek                     | F          |        8.8 |
| AGO         | Ambo                      | F          |        2.4 |
| AGO         | Chokwe                    | F          |        4.2 |
| AGO         | Mbundu                    | F          |       21.6 |
| AGO         | Nyaneka-nkhumbi           | F          |        5.4 |
| AGO         | Ovimbundu                 | F          |       37.2 |
| AIA         | English                   | T          |        0.0 |
| ANT         | Papiamento                | T          |       86.2 |
| ARE         | Arabic                    | T          |       42.0 |
| ARE         | Hindi                     | F          |        0.0 |
| ARG         | Indian Languages          | F          |        0.3 |
| ARG         | Italian                   | F          |        1.7 |
| ARG         | Spanish                   | T          |       96.8 |
| ARM         | Armenian                  | T          |       93.4 |
| ARM         | Azerbaijani               | F          |        2.6 |
| ASM         | English                   | T          |        3.1 |
| ASM         | Samoan                    | T          |       90.6 |
| ASM         | Tongan                    | F          |        3.1 |
| ATG         | Creole English            | F          |       95.7 |
| ATG         | English                   | T          |        0.0 |
| AUS         | Serbo-Croatian            | F          |        0.6 |
| AUS         | Vietnamese                | F          |        0.8 |
| AUT         | Czech                     | F          |        0.2 |


mysql> select user from mysql.user;
+------------------+
| user             |
+------------------+
| root             |
| mysql.infoschema |
| mysql.session    |
| mysql.sys        |
| root             |
+------------------+
5 rows in set (0.00 sec)

root@worker:~# ls -al  /mnt/data/             == \\\\/ data test  mount valumes data refelect data 
total 102112
drwxr-xr-x 10 systemd-coredump root                 4096 Feb  1 15:51  .
drwxr-xr-x  3 root             root                 4096 Feb  1 14:20  ..
-rw-r-----  1 systemd-coredump systemd-coredump       56 Feb  1 14:20  auto.cnf
-rw-r-----  1 systemd-coredump systemd-coredump  3032843 Feb  1 14:20  binlog.000001
-rw-r-----  1 systemd-coredump systemd-coredump  1908824 Feb  1 15:13  binlog.000002
-rw-r-----  1 systemd-coredump systemd-coredump       32 Feb  1 14:20  binlog.index
-rw-------  1 systemd-coredump systemd-coredump     1676 Feb  1 14:20  ca-key.pem
-rw-r--r--  1 systemd-coredump systemd-coredump     1112 Feb  1 14:20  ca.pem
-rw-r--r--  1 systemd-coredump systemd-coredump     1112 Feb  1 14:20  client-cert.pem
-rw-------  1 systemd-coredump systemd-coredump     1676 Feb  1 14:20  client-key.pem
-rw-r--r--  1 dell             dell               558791 Feb  1 15:08  data.sql
drwxr-xr-x  2 root             root                 4096 Feb  1 15:50  I-AM-FROM-MISQL-POD
-rw-r-----  1 systemd-coredump systemd-coredump   196608 Feb  1 15:25 '#ib_16384_0.dblwr'
-rw-r-----  1 systemd-coredump systemd-coredump  8585216 Feb  1 14:20 '#ib_16384_1.dblwr'
-rw-r-----  1 systemd-coredump systemd-coredump     5687 Feb  1 14:20  ib_buffer_pool
-rw-r-----  1 systemd-coredump systemd-coredump 12582912 Feb  1 15:25  ibdata1
-rw-r-----  1 systemd-coredump systemd-coredump 12582912 Feb  1 14:20  ibtmp1
drwxr-x---  2 systemd-coredump systemd-coredump     4096 Feb  1 15:12 '#innodb_redo'
drwxr-x---  2 systemd-coredump systemd-coredump     4096 Feb  1 14:20 '#innodb_temp'
drwxr-x---  2 systemd-coredump systemd-coredump     4096 Feb  1 14:20  mysql
-rw-r-----  1 systemd-coredump systemd-coredump 31457280 Feb  1 15:13  mysql.ibd
lrwxrwxrwx  1 systemd-coredump systemd-coredump       27 Feb  1 14:20  mysql.sock -> /var/run/mysqld/mysqld.sock
drwxr-x---  2 systemd-coredump systemd-coredump     4096 Feb  1 14:20  performance_schema
-rw-------  1 systemd-coredump systemd-coredump     1680 Feb  1 14:20  private_key.pem
-rw-r--r--  1 systemd-coredump systemd-coredump      452 Feb  1 14:20  public_key.pem
-rw-r--r--  1 systemd-coredump systemd-coredump     1112 Feb  1 14:20  server-cert.pem
-rw-------  1 systemd-coredump systemd-coredump     1680 Feb  1 14:20  server-key.pem
drwxr-x---  2 systemd-coredump systemd-coredump     4096 Feb  1 14:20  sys
drwxr-xr-x  2 root             root                 4096 Feb  1 15:47  test-node-to-msqlpod
-rw-r--r--  1 root             root                    0 Feb  1 15:51  TEST-VOLUMEMOUNTS-TEST-VOLUMOUNTPATHS-DATA-REFELECT
-rw-r-----  1 systemd-coredump systemd-coredump 16777216 Feb  1 15:13  undo_001
-rw-r-----  1 systemd-coredump systemd-coredump 16777216 Feb  1 15:13  undo_002
drwxr-x---  2 systemd-coredump systemd-coredump     4096 Feb  1 15:13  world_x
=========================================================================================
https://dev.to/musolemasu/deploy-a-mysql-database-server-in-kubernetes-static-dpc     success code
https://phoenixnap.com/kb/kubernetes-mysql
apiVersion: v1
kind: Service
metadata:
  name: mysql
spec:
  ports:
  - port: 3306
  selector:
    app: mysql

https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/
apiVersion: v1
kind: Service
metadata:
  name: mysql
  labels:
    app: mysql
    app.kubernetes.io/name: mysql
spec:
  ports:
  - name: mysql
    port: 3306
  clusterIP: None
  selector:
    app: mysql

---
# Client service for connecting to any MySQL instance for reads.
# For writes, you must instead connect to the primary: mysql-0.mysql.
apiVersion: v1
kind: Service
metadata:
  name: mysql-read
  labels:
    app: mysql
    app.kubernetes.io/name: mysql
    readonly: "true"
spec:
  ports:
  - name: mysql
    port: 3306
  selector:
    app: mysql
----------------------------      ====   \\\\    success pod deploy in kubernetes 
---
apiVersion: v1
kind: PersistentVolume
metadata: 
  name: pv
spec: 
  storageClassName: ssd
  capacity: 
    storage: 5Gi
  accessModes: 
    - ReadWriteOnce
  hostPath: 
    path: "/tmp/data"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata: 
  name: pvc
spec: 
  storageClassName: ssd
  accessModes: 
    - ReadWriteOnce
  resources:
     requests:
       storage: 4Gi
---
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: mysql
  labels: 
    app: mysql
spec: 
   replicas: 3
   selector: 
      matchLabels: 
        app: mysql
   template: 
     metadata:
       labels:   
         app: mysql
     spec: 
        containers: 
          - name: mysql
            image: mysql
            env: 
             - name: MYSQL_ROOT_PASSWORD
               value: anji123
            volumeMounts:
              - name: mysql1
                mountPath: /var/lib/mysql/                               
            ports: 
              - containerPort: 3306
        volumes: 
          - name: mysql1
            persistentVolumeClaim: 
               claimName: pvc
---
apiVersion: v1
kind: Service
metadata: 
   name: outside
spec: 
  selector: 
    app: mysql
  ports: 
    - port: 3306
  clusterIP: None    
anji@anji:~/deployment$  kubectl delete  pod,deployment,service,pv,pvc --all 
==================================================================================="
https://sesamedisk.com/deploy-wordpress-on-k8s/
              kustomization.yaml           -  importent  ok  
secretGenerator:
- name: mysql-password
  literals:
  - password=Mysql.Root2021@
- name: mysql-user
  literals:
  - username=userwp
- name: mysql-user-password
  literals:
  - passworduser=Mysql.User2021@
- name: mysql-database
  literals:
  - database=multitenant_wp
------
       anji@anji:~/deployment$ nano kustomization.yaml
anji@anji:~/deployment$ kubectl apply -k .
secret/mysql-database-4f74mgddt5 created
secret/mysql-password-f547bhm8mc created
secret/mysql-user-4t5mcf8dkm created
secret/mysql-user-password-9m7k5b4k2m created
anji@anji:~/deployment$ 
=========================================================================="
https://www.rancher.cn/running-highly-available-wordpress-mysql-kubernetes

# storage-class.yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: slow
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-standard
  zone: us-central1-a

---
# mysql-services.yaml
# Headless service for stable DNS entries of StatefulSet members.
apiVersion: v1
kind: Service
metadata:
  name: mysql
  labels:
    app: mysql
spec:
  ports:
  - name: mysql
    port: 3306
  clusterIP: None
  selector:
    app: mysql
---
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: mysql
spec:
  selector:
    matchLabels:
      app: mysql
  serviceName: mysql
  replicas: 3
  template:
    metadata:
      labels:
        app: mysql
    spec:
      initContainers:
      - name: init-mysql
        image: mysql:5.7
        command:
        - bash
        - "-c"
        - |
          set -ex
          # Generate mysql server-id from pod ordinal index.
          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1
          ordinal=${BASH_REMATCH[1]}
          echo [mysqld] > /mnt/conf.d/server-id.cnf
          # Add an offset to avoid reserved server-id=0 value.
          echo server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf
          # Copy appropriate conf.d files from config-map to emptyDir.
          if [[ $ordinal -eq 0 ]]; then
            cp /mnt/config-map/master.cnf /mnt/conf.d/
          else
            cp /mnt/config-map/slave.cnf /mnt/conf.d/
          fi
        volumeMounts:
        - name: conf
          mountPath: /mnt/conf.d
        - name: config-map
          mountPath: /mnt/config-map
      - name: clone-mysql
        image: gcr.io/google-samples/xtrabackup:1.0
        command:
        - bash
        - "-c"
        - |
          set -ex
          # Skip the clone if data already exists.
          [[ -d /var/lib/mysql/mysql ]] && exit 0
          # Skip the clone on master (ordinal index 0).
          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1
          ordinal=${BASH_REMATCH[1]}
          [[ $ordinal -eq 0 ]] && exit 0
          # Clone data from previous peer.
          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql
          # Prepare the backup.
          xtrabackup --prepare --target-dir=/var/lib/mysql
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
          subPath: mysql
        - name: conf
          mountPath: /etc/mysql/conf.d
      containers:
      - name: mysql
        image: mysql:5.7
        env:
        - name: MYSQL_ALLOW_EMPTY_PASSWORD
          value: "1"
        ports:
        - name: mysql
          containerPort: 3306
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
          subPath: mysql
        - name: conf
          mountPath: /etc/mysql/conf.d
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
        livenessProbe:
          exec:
            command: ["mysqladmin", "ping"]
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
        readinessProbe:
          exec:
            # Check we can execute queries over TCP (skip-networking is off).
            command: ["mysql", "-h", "127.0.0.1", "-e", "SELECT 1"]
          initialDelaySeconds: 5
          periodSeconds: 2
          timeoutSeconds: 1
      - name: xtrabackup
        image: gcr.io/google-samples/xtrabackup:1.0
        ports:
        - name: xtrabackup
          containerPort: 3307
        command:
        - bash
        - "-c"
        - |
          set -ex
          cd /var/lib/mysql

          # Determine binlog position of cloned data, if any.
          if [[ -f xtrabackup_slave_info ]]; then
            # XtraBackup already generated a partial "CHANGE MASTER TO" query
            # because we're cloning from an existing slave.
            mv xtrabackup_slave_info change_master_to.sql.in
            # Ignore xtrabackup_binlog_info in this case (it's useless).
            rm -f xtrabackup_binlog_info
          elif [[ -f xtrabackup_binlog_info ]]; then
            # We're cloning directly from master. Parse binlog position.
            [[ `cat xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1
            rm xtrabackup_binlog_info
            echo "CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\
                  MASTER_LOG_POS=${BASH_REMATCH[2]}" > change_master_to.sql.in
          fi

          # Check if we need to complete a clone by starting replication.
          if [[ -f change_master_to.sql.in ]]; then
            echo "Waiting for mysqld to be ready (accepting connections)"
            until mysql -h 127.0.0.1 -e "SELECT 1"; do sleep 1; done

            echo "Initializing replication from clone position"
            # In case of container restart, attempt this at-most-once.
            mv change_master_to.sql.in change_master_to.sql.orig
            mysql -h 127.0.0.1 <<EOF
          $(<change_master_to.sql.orig),
            MASTER_HOST='mysql-0.mysql',
            MASTER_USER='root',
            MASTER_PASSWORD='',
            MASTER_CONNECT_RETRY=10;
          START SLAVE;
          EOF
          fi

          # Start a server to send backups when requested by peers.
          exec ncat --listen --keep-open --send-only --max-conns=1 3307 -c \
            "xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1 --user=root"
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
          subPath: mysql
        - name: conf
          mountPath: /etc/mysql/conf.d
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
      volumes:
      - name: conf
        emptyDir: {}
      - name: config-map
        configMap:
          name: mysql
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 10Gi
---
# nfs.yaml
# Define the persistent volume claim
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs
  labels:
    demo: nfs
  annotations:
    volume.alpha.kubernetes.io/storage-class: any
spec:
  accessModes: [ "ReadWriteOnce" ]
  resources:
    requests:
      storage: 200Gi

---
# Define the Replication Controller
apiVersion: v1
kind: ReplicationController
metadata:
  name: nfs-server
spec:
  replicas: 1
  selector:
    role: nfs-server
  template:
    metadata:
      labels:
        role: nfs-server
    spec:
      containers:
      - name: nfs-server
        image: gcr.io/google_containers/volume-nfs:0.8
        ports:
          - name: nfs
            containerPort: 2049
          - name: mountd
            containerPort: 20048
          - name: rpcbind
            containerPort: 111
        securityContext:
          privileged: true
        volumeMounts:
          - mountPath: /exports
            name: nfs-pvc
      volumes:
        - name: nfs-pvc
          persistentVolumeClaim:
            claimName: nfs

---
# Define the Service
kind: Service
apiVersion: v1
metadata:
  name: nfs-server
spec:
  ports:
    - name: nfs
      port: 2049
    - name: mountd
      port: 20048
    - name: rpcbind
      port: 111
  selector:
    role: nfs-server
---
# wordpress.yaml
apiVersion: v1
kind: Service
metadata:
  name: wordpress
  labels:
    app: wordpress
spec:
  ports:
    - port: 80
  selector:
    app: wordpress
    tier: frontend
  type: LoadBalancer

---

apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs
spec:
  capacity:
    storage: 20G
  accessModes:
    - ReadWriteMany
  nfs:
    # FIXME: use the right IP
    server: <IP of the NFS Service>
    path: "/"

---

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: ""
  resources:
    requests:
      storage: 20G

---

apiVersion: apps/v1beta1 # for versions before 1.8.0 use apps/v1beta1
kind: Deployment
metadata:
  name: wordpress
  labels:
    app: wordpress
spec:
  selector:
    matchLabels:
      app: wordpress
      tier: frontend
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: wordpress
        tier: frontend
    spec:
      containers:
      - image: wordpress:4.9-apache
        name: wordpress
        env:
        - name: WORDPRESS_DB_HOST
          value: mysql
        - name: WORDPRESS_DB_PASSWORD
          value: ""
        ports:
        - containerPort: 80
          name: wordpress
        volumeMounts:
        - name: wordpress-persistent-storage
          mountPath: /var/www/html
      volumes:
      - name: wordpress-persistent-storage
        persistentVolumeClaim:
            claimName: nfs

===========================================================================================

https://gopensource.com/deploy-wordpress-blog-with-mysql-on-a-bare-metal-kubernetes-cluster-8a9323c0f4c9


cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Secret
metadata:
  name: mysql-pass
type: Opaque
data:
  password: TXlQYXNzd29yZCEkJHRyMG5
EOF

---
kind: PersistentVolume
apiVersion: v1
metadata:
  name: wp-pv
  labels:
    type: local
spec:
  capacity:
    storage: 3Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/home/shashank/wordpress"
---
kind: PersistentVolume
apiVersion: v1
metadata:
  name: wp-mysql-pv
  labels:
    type: local
spec:
  capacity:
    storage: 3Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/home/shashank/wordpress-mysql"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: wp-pv-claim
  labels:
    app: wordpress
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-pv-claim
  labels:
    app: wordpress
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
---
apiVersion: v1
kind: Service
metadata:
  name: wordpress
  labels:
    app: wordpress
spec:
  ports:
    - port: 80
  selector:
    app: wordpress
    tier: frontend
  type: NodePort
---
apiVersion: v1
kind: Service
metadata:
  name: wordpress-mysql
  labels:
    app: wordpress
spec:
  ports:
    - port: 3306
  selector:
    app: wordpress
    tier: mysql
  clusterIP: 10.96.0.100
---
apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2
kind: Deployment
metadata:
  name: wordpress
  labels:
    app: wordpress
spec:
  selector:
    matchLabels:
      app: wordpress
      tier: frontend
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: wordpress
        tier: frontend
    spec:
      containers:
      - image: wordpress:5.4.1-apache
        name: wordpressenv:
        - name: WORDPRESS_DB_HOST
          value: 10.96.0.100
        - name: WORDPRESS_DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-pass
              key: password
        ports:
        - containerPort: 80
          name: wordpress
        volumeMounts:
        - name: wordpress-persistent-storage
          mountPath: /var/www/html
      volumes:
      - name: wordpress-persistent-storage
        persistentVolumeClaim:
          claimName: wp-pv-claim
---
apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2
kind: Deployment
metadata:
  name: wordpress-mysql
  labels:
    app: wordpress
spec:
  selector:
    matchLabels:
      app: wordpress
      tier: mysql
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: wordpress
        tier: mysql
    spec:
      containers:
      - image: mysql:5.6
        name: mysql
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-pass
              key: password
        ports:
        - containerPort: 3306
          name: mysql
        readinessProbe:
          tcpSocket:
            port: 3306
          initialDelaySeconds: 5
          periodSeconds: 10
        livenessProbe:
          tcpSocket:
            port: 3306
          initialDelaySeconds: 15
          periodSeconds: 20
        volumeMounts:
        - name: mysql-persistent-storage
          mountPath: /var/lib/mysql
      volumes:
      - name: mysql-persistent-storage
        persistentVolumeClaim:
          claimName: mysql-pv-claim
#################################################################################################33333"
https://blog.litespeedtech.com/2022/09/26/wordpress-woocommerce-and-kubernetes-with-litespeed-ingress-controller/  
https://docs.portworx.com/operations/operate-kubernetes/application-install-with-kubernetes/wordpress/
https://hewlettpackard.github.io/hpe-solutions-hpecp/5.0/Validate%20HPE%20Ezmeral%20Container%20Platform%20deployment/Validate%20HPE%20Ezmeral%20Container%20Platform%20deployment.html#deploying-wordpress-application-with-hpe-ezmeral-data-fabric
https://www.goglides.dev/roshan_thapa/how-to-deploy-wordpress-and-mysql-on-kubernetes-5f4d
https://github.com/kubernetes/examples/blob/master/mysql-wordpress-pd/wordpress-deployment.yaml
https://medium.com/codex/how-to-deploy-wordpress-on-kubernetes-part-2-df1cc9cbaa2e
https://phoenixnap.com/kb/kubernetes-wordpress  ok 
       

apiVersion: v1
kind: PersistentVolume
metadata:
  name: mysql-pv
  namespace: woo
spec:
  storageClassName: do-block-storage
  capacity:
    storage: 20Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/var/lib/mysql"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-pv-claim
  namespace: woo
spec:
  storageClassName: do-block-storage
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: wordpress-pv
  namespace: woo
spec:
  storageClassName: do-block-storage
  capacity: 
    storage: 30Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/var/www"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: wordpress-pv-claim
  namespace: woo
spec:
  storageClassName: do-block-storage
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 30Gi
---
apiVersion: v1
kind: Service
metadata: 
  name: mysql-wp
  namespace: woo
spec:
  ports:
    - port: 3306
  selector:
    app: wordpress
    tier: mysql
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql-wp
  namespace: woo
spec:
  selector:
    matchLabels:
      app: wordpress
      tier: mysql
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: wordpress
        tier: mysql
    spec:
      containers:
      - image: mysql:latest
        name: mysql
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-password
              key: password
        - name: MYSQL_USER
          valueFrom:
            secretKeyRef:
              name: mysql-user
              key: username
        - name: MYSQL_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-user-password
              key: password
        - name: MYSQL_DATABASE
          valueFrom:
            secretKeyRef:
              name: mysql-database
              key: database
        ports:
        - containerPort: 3306
          name: mysql
        volumeMounts:
        - name: persistent-storage
          mountPath: /var/lib/mysql
      volumes:
      - name: persistent-storage
        persistentVolumeClaim:
          claimName: mysql-pv-claim
---
apiVersion: v1
kind: Service
metadata: 
  name: wordpress
  namespace: woo
spec:
  ports:
    - port: 80
  selector:
    app: wordpress
    tier: web
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: wordpress
  namespace: woo
spec:
  selector:
    matchLabels:
      app: wordpress
      tier: web
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: wordpress
        tier: web
    spec:
      containers:
      - image: wordpress:php8.1-apache
        name: wordpress
        env:
        - name: WORDPRESS_DB_HOST
          value: mysql-wp:3306
        - name: WORDPRESS_DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-user-password
              key: password
        - name: WORDPRESS_DB_USER
          valueFrom:
            secretKeyRef:
              name: mysql-user
              key: username
        - name: WORDPRESS_DB_NAME
          valueFrom:
            secretKeyRef:
              name: mysql-database
              key: database
        ports:
        - containerPort: 80
          name: wordpress
        volumeMounts:
        - name: persistent-storage
          mountPath: /var/www/html
      volumes:
      - name: persistent-storage
        persistentVolumeClaim:
          claimName: wordpress-pv-claim
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: wordpress
  namespace: woo
  annotations:
    kubernetes.io/ingress.class: litespeedtech.com/lslbd
    cert-manager.io/cluster-issuer: letsencrypt-production
spec:
  rules:
  - host: YOUR DNS NAME
    http:
     paths:
     - path: "/"
       pathType: Prefix
       backend:
         service:
           name: wordpress
           port:
             number: 80
  tls:
  - hosts:
    - YOUR DNS NAME
    secretName: YOUR DNS NAME
################################################################################################################"
https://www.serverlab.ca/tutorials/containers/kubernetes/deploying-wordpress-on-kubernetes/
---
apiVersion: v1
kind: Pod
metadata:
  name: wordpress-blog
spec:
  containers:
  - name: wordpress-blog
    image: wordpress:5.2
    env:
    - name: WORDPRESS_DB_HOST
      value: database.host.name:3306
    - name: WORDPRESS_DB_NAME
      value: wordpress
    - name: WORDPRESS_DB_PREFIX
      value: myblog_ 
---
---
apiVersion: v1
kind: Service
metadata:
  name: wordpress-blog-service
spec:
  type: LoadBalancer
  selector:
    application: wordpress-blog
  ports:
  - port: 80
  targetPort:
  - port: 80 
####################################################################33
https://matthewdavis.io/highly-available-wordpress-on-kubernetes/
https://github.com/anjilinux/project--mysql        
https://github.com/anjilinux/project-nfs-kubernetes
https://github.com/anjilinux/project-k8-byexamples-cert-manager
https://github.com/anjilinux/project-ingress-controller

apiVersion: apps/v1                                         ==--  success wordpress 
kind: Deployment
metadata: 
  name: wordpress
  labels: 
    app: wordpress
spec: 
   replicas: 1
   selector: 
     matchLabels: 
       app: wordpress
   template: 
     metadata: 
        labels: 
           app: wordpress
     spec: 
       containers: 
           - name: wordpress
             image: wordpress
             env:
               - name: WORDPRESS_DB_HOST
                 value: mysql
               - name: WORDPRESS_DB_USER
                 value: wordpress
               - name: WORDPRESS_DB_PASSWORD
                 value: wordpress
               - name: WORDPRESS_DB_NAME
                 value: wordpress
             ports: 
               - containerPort: 80
======================================================
docker pull wordpress

-e WORDPRESS_DB_HOST=...
-e WORDPRESS_DB_USER=...
-e WORDPRESS_DB_PASSWORD=...
-e WORDPRESS_DB_NAME=...
-e WORDPRESS_TABLE_PREFIX=...

docker run --name wordpress -p 8085:80 -d wordpress

root@anji:~# docker run --name wordpress -p 8085:80 -d wordpress
ad4c8635296a180c1e6e6f9a023af104b354dc8e932dccfd058881587f70d1d1
root@anji:~# docker images 
REPOSITORY           TAG       IMAGE ID       CREATED       SIZE
mysql/mysql-server   latest    1d9c2219ff69   2 weeks ago   496MB
wordpress            latest    fcd4967b9728   3 weeks ago   615MB
root@anji:~# docker ps 
CONTAINER ID   IMAGE       COMMAND                  CREATED          STATUS          PORTS                                   NAMES
ad4c8635296a   wordpress   "docker-entrypoint.sâ€¦"   30 seconds ago   Up 30 seconds   0.0.0.0:8085->80/tcp, :::8085->80/tcp   wordpress

http://localhost:8085/wp-admin/setup-config.php?step=1


          docker run --name wordpress -p 8085:80 -d wordpress
root@anji:~# docker  ps 
CONTAINER ID   IMAGE       COMMAND                  CREATED          STATUS          PORTS                                   NAMES
7a909a002009   wordpress   "docker-entrypoint.sâ€¦"   48 seconds ago   Up 46 seconds   0.0.0.0:8085->80/tcp, :::8085->80/tcp   wordpress "

https://hub.docker.com/_/wordpress
version: '3.1'                            \\/   success full k 

services:

  wordpress:
    image: wordpress
    restart: always
    ports:
      - 8082:80
    environment:
      WORDPRESS_DB_HOST: db
      WORDPRESS_DB_USER: anjireddy
      WORDPRESS_DB_PASSWORD: anjireddy
      WORDPRESS_DB_NAME: anjireddy
    volumes:
      - wordpress:/var/www/html

  db:
    image: mysql:5.7
    restart: always
    environment:
      MYSQL_DATABASE: anjireddy
      MYSQL_USER: anjireddy
      MYSQL_PASSWORD: anjireddy
      MYSQL_RANDOM_ROOT_PASSWORD: '1'
    volumes:
      - db:/var/lib/mysql

volumes:
  wordpress:
  db:

docker-compose -f stack.yaml up 

anji@anji:~/deployment$ docker ps
CONTAINER ID   IMAGE       COMMAND                  CREATED          STATUS          PORTS                                   NAMES
e2e0e2795cf1   wordpress   "docker-entrypoint.sâ€¦"   15 minutes ago   Up 15 minutes   0.0.0.0:8088->80/tcp, :::8088->80/tcp   secrete-wordpress-1
d63a2e6e6ada   mysql:5.7   "docker-entrypoint.sâ€¦"   22 minutes ago   Up 15 minutes   3306/tcp, 33060/tcp                     secrete-db-1
anji@anji:~/deployment$ 


====
[+] Running 2/2
 â ¿ Container secrete-db-1         Stopped                                                                                           1.8s
 â ¿ Container secrete-wordpress-1  Stopped                                                                                           1.4s
canceled



==========
FROM wordpress:apache
WORKDIR /usr/src/wordpress
RUN set -eux; \
	find /etc/apache2 -name '*.conf' -type f -exec sed -ri -e "s!/var/www/html!$PWD!g" -e "s!Directory /var/www/!Directory $PWD!g" '{}' +; \
	cp -s wp-config-docker.php wp-config.php
COPY custom-theme/ ./wp-content/themes/custom-theme/
COPY custom-plugin/ ./wp-content/plugins/custom-plugin/
-----------------
$ docker run ... \
	--read-only \
	--tmpfs /tmp \
	--tmpfs /run \
	--mount type=...,src=...,dst=/usr/src/wordpress/wp-content/uploads \
	... \
	--env WORDPRESS_DB_HOST=... \
	--env WORDPRESS_AUTH_KEY=... \
	--env ... \
	custom-wordpress:tag
  ------------
   docker run -it --rm \
	--volumes-from some-wordpress \
	--network container:some-wordpress \
	-e WORDPRESS_DB_USER=... \
	-e WORDPRESS_DB_PASSWORD=... \
	# [and other used environment variables]
	wordpress:cli user list
----------------------------------------
 docker run --name wordpress -e WORDPRESS_DB_PASSWORD_FILE=/run/secrete/pwd ... -d wordpress

 docker run --name wordpress -e WORDPRESS_DB_PASSWORD_FILE=/run/secrete/pwd  -d wordpress

root@anji:/run/secrete# docker run --name wordpress -p 8086:80 -e WORDPRESS_DB_PASSWORD_FILE=/run/secrete/pwd -d wordpress"
#######################################################################################################################################
https://medium.com/@containerum/how-to-deploy-wordpress-and-mysql-on-kubernetes-bda9a3fdd2d5

---
apiVersion: v1
kind: Secret
metadata:
  name: mysql-pass
type: Opaque
data:
  password: YWRtaW4=
---
# Create PersistentVolume
# change the ip of NFS server
apiVersion: v1
kind: PersistentVolume
metadata:
  name: wordpress-persistent-storage
  labels:
    app: wordpress
    tier: frontend
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteMany
  nfs:
    server: 172.31.39.63
    # Exported path of your NFS server
    path: "/html"

---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mysql-persistent-storage
  labels:
    app: wordpress
    tier: mysql
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteMany
  nfs:
    server: 172.31.39.63
    # Exported path of your NFS server
    path: "/mysql"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: wordpress-persistent-storage
  labels:
    app: wordpress
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 6Gi
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-persistent-storage
  labels:
    app: wordpress
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 6Gi
---
apiVersion: v1
kind: Service
metadata:
  name: wordpress-mysql   # will be used as a value in
  labels:                 # WORDPRESS_DB_HOST in wordpress-deploy.yml
    app: wordpress
spec:
  ports:
    - port: 3306
  selector:
    app: wordpress
    tier: mysql
  clusterIP: None
---
apiVersion:  apps/v1beta2 # for versions before 1.9.0 use apps/v1beta2
kind: Deployment
metadata:
  name: wordpress-mysql
  labels:
    app: wordpress
spec:
  selector:
    matchLabels:
      app: wordpress
      tier: mysql
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: wordpress
        tier: mysql
    spec:
      containers:
      - image: mysql:5.6
        name: mysql
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-pass        # the one generated before in secret.yml
              key: password
        ports:
        - containerPort: 3306
          name: mysql
        volumeMounts:
        - name: mysql-persistent-storage  # which data will be stored
          mountPath: "/var/lib/mysql"
      volumes:
      - name: mysql-persistent-storage    # PVC
        persistentVolumeClaim:
          claimName: mysql-persistent-storage                  
---
# create a service for wordpress
apiVersion: v1
kind: Service
metadata:
  name: wordpress
  labels:
    app: wordpress
spec:
  ports:
    - port: 80
  selector:
    app: wordpress
    tier: frontend
  type: ClusterIP---
apiVersion: apps/v1beta2 # for versions before 1.9.0 use apps/v1beta2
kind: Deployment
metadata:
  name: wordpress
  labels:
    app: wordpress
spec:
  selector:
    matchLabels:
      app: wordpress
      tier: frontend
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: wordpress
        tier: frontend
    spec:
      containers:
      - image: wordpress:4.8-apache
        name: wordpress
        env:
        - name: WORDPRESS_DB_HOST
          value: wordpress-mysql
        - name: WORDPRESS_DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-pass          # generated before in secret.yml
              key: password
        ports:
        - containerPort: 80
          name: wordpress
        volumeMounts:
        - name: wordpress-persistent-storage
          mountPath: "/var/www/html"          # which data will be stored
      volumes:
      - name: wordpress-persistent-storage
        persistentVolumeClaim:
          claimName: wordpress-persistent-storage
======+++
apiVersion: v1
kind: Service
metadata:
  name: wordpress-mysql   # will be used as a value in
  labels:                 # WORDPRESS_DB_HOST in wordpress-deploy.yml
    app: wordpress
spec:
  ports:
    - port: 3306
  selector:
    app: wordpress
    tier: mysql
  clusterIP: None         ##  see diffrent
           
---
# create a service for wordpress
apiVersion: v1
kind: Service
metadata:
  name: wordpress
  labels:
    app: wordpress
spec:
  ports:
    - port: 80
  selector:
    app: wordpress
    tier: frontend
  type: ClusterIP---        ## see  diffrent 

+++++++++=
apiVersion: v1
kind: Service
metadata:
  name: mwithword  # will be used as a value in
  labels:                 # WORDPRESS_DB_HOST in wordpress-deploy.yml
    app: wordpress
spec:
  ports:
    - port: 3306
  selector:
    app: wordpress
    tier: mysql
  clusterIP: None         ##  see diffrent
           
---
# create a service for wordpress
apiVersion: v1
kind: Service
metadata:
  name: wordpress
  labels:
    app: wordpress
spec:
  ports:
    - port: 80
  selector:
    app: wordpress
    tier: frontend
  type: NodePort        ## see  diffrent 
==========

---
apiVersion: v1
kind: Service
metadata:
  name: mwithw   # will be used as a value in
  labels:                 # WORDPRESS_DB_HOST in wordpress-deploy.yml
    app: wordpress
spec:
  ports:
    - port: 3306
  selector:
    app: wordpress
    tier: mysql
  clusterIP: None         ##  see diffrent
           
---
# create a service for wordpress
apiVersion: v1
kind: Service
metadata:
  name: wordpress
  labels:
    app: wordpress
spec:
  ports:
    - port: 80
  selector:
    app: wordpress
    tier: frontend
  type:       ## see  diffrent 
-------------------------------

  selector:
    app: mariadb
  ports:
    - protocol: TCP
      port: 3306
      targetPort: 3306
  clusterIP: None

 spec:
  selector:
    app: wordpress
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP #default
      nodePort: 31000
  type: LoadBalancer 
  -----
https://clearlinux.org/blogs-news/deploy-scalable-wordpress-kubernetes-clear-linux-os-containers    = best helpfull
apiVersion: v1
kind: Service
metadata:
  name: wordpress
  labels:
    app: wordpress
spec:
  ports:
    - port: 80
      nodePort: 30180
  selector:
    app: wordpress
    tier: frontend
#  type: LoadBalancer
  type: NodePort

======================
https://github.com/anjilinux/Scalable-WordPress-deployment-on-Kubernetes/edit/master/wordpress-deployment.yaml
https://github.com/anjilinux/Scalable-WordPress-deployment-on-Kubernetes
---
apiVersion: v1
kind: Service
metadata:
  name: wordpress
  labels:
    app: wordpress
    tier: frontend
spec:
  ports:
    - port: 80
  selector:
    app: wordpress
    tier: frontend
  type: NodePort
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: wp-pv-volume
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 20Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/tmp/wp/data"
  persistentVolumeReclaimPolicy: Recycle
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: wp-pv-claim
  labels:
    app: wordpress
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi
---
apiVersion: apps/v1   # versions before 1.9.0 use apps/v1beta2
kind: Deployment
metadata:
  name: wordpress
  labels:
    app: wordpress
    tier: frontend
spec:
  selector:
    matchLabels:
      app: wordpress
      tier: frontend
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: wordpress
        tier: frontend
    spec:
      containers:
        - image: wordpress:5.8.1-php7.4
          name: wordpress
          env:
            - name: WORDPRESS_DB_HOST
              value: wordpress-mysql
            - name: WORDPRESS_DB_NAME
              value: wordpress
            - name: WORDPRESS_DB_USER
              value: root
            - name: WORDPRESS_DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mysql-pass
                  key: password
          ports:
            - containerPort: 80
              name: wordpress
          volumeMounts:
            - name: wordpress-persistent-storage
              mountPath: /var/www/html
      volumes:
        - name: wordpress-persistent-storage
          persistentVolumeClaim:
            claimName: wp-pv-claim


#########################################################################################################3











